{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basline Model\nThe starting point of our Encoder-Decoder challenge.","metadata":{}},{"cell_type":"code","source":"# Install packages\n!pip install rouge-score","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.19.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.15.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (0.12.0)\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os  # operating system access\nimport sys  # system specific operations\nimport datetime as dt  # datetime handling\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split  # data preperation\n\nimport tensorflow as tf  # tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers, Input, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow_addons as tfa\nimport tensorboard as tb\n\nimport tqdm  # progress bars\nimport json\nimport codecs\nimport pickle\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport matplotlib.pyplot as plt\n\nimport nltk.translate.bleu_score as ntbs  # BLEU scoring basics\nimport rouge_score.rouge_scorer as rouge_scorer  # ROUGE scoring basics\nsys.path.insert(0, '../input/aida-metrics-functions')  # custom metrics implementation\nimport metrics as met  # custom metrics BLUE-1 pecsision and ROUGE-L recall","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"# Runtime config\nEPOCHS = 1\nRUN_TRAINING = True  # Train the model during run\nSAVE_MODEL = True  # Write model to hd5 file\nLOG_MODEL = True  # Write tensorboard and model history logs during training\nRUN_DATA_CHECK = False  # Do a basic check on the input data\n\n# Tensorboard view config\nTENSORBOARD_SHOW_INLINE = False\n# Launch tensorboard\n# True: From that notebook, False: connect to already running tensorboard\nTENSORBOARD_LAUNCH = True\n\n# Directories and filenames\nFP_CAPTIONS = '../input/flickr8k/captions.txt'\nDIR_IMAGES = '../input/flickr8k/Images/'\nDIR_IMAGE_FEATURES = '../input/aida-image-captioning/Images/'\nDIR_MODEL_STORE = './models/'\nDIR_MODEL_LOG = './models/'\nDIR_RESULT_STORE = './results/'\nDIR_TENSORBOARD_LOG = './tensorboard/'\n\n# Create specified directories\n!mkdir results\n!mkdir models\n!mkdir tensorboard","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Init","metadata":{}},{"cell_type":"code","source":"timestamp_str = None\nFN_TIMESTAMP = \"20210428-143443\"\nFN_MODEL2LOAD = f\"Baseline_Train_Test_{FN_TIMESTAMP}.h5\"\nFN_HIST2LOAD = f\"Baseline_Train_Test_{FN_TIMESTAMP}.pickle\"","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def preproc_text(texts):\n    \"\"\"\n    - Adds <start> to the beginning of texts to the inputs\n    - Adds <end> to the end of texts\n    Input: texts: list of sentences\n    Output: - tokenized, padded text with <start> token for using as input in decoder\n            - tokenized, padded text with <end> token for using as target in decoder\n            - tokenizer\n            - max_seq_len\n    \"\"\"\n    \n    print('Creating and fitting tokenizer...')\n    tokenizer_fit_data = ['<start> ' + s + ' <end>' for s in texts]\n\n    # create tokenizer for captions + <start> and <end> tags\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;?@[\\\\]^_`{|}~\\t\\n')\n    tokenizer.fit_on_texts(tokenizer_fit_data)\n    tokenizer.word_index['<pad>'] = 0\n    tokenizer.index_word[0] = '<pad>'\n    print('Creating and fitting tokenizer done')\n    \n    print('Create text input sequences...')\n    # add <start> tag before every caption and tokenize + pad ('post')\n    enc_input_data = ['<start> ' + s for s in texts]\n    enc_input_token_seqs = tokenizer.texts_to_sequences(enc_input_data)\n    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n    enc_input_prep = pad_sequences(enc_input_token_seqs, padding='post')\n    print('Create text input sequences done')\n    \n    print('Create text target sequences...')\n    # add <start> tag before every caption and tokenize + pad ('post')\n    enc_target_data = [s + ' <end>' for s in texts]\n    enc_target_token_seqs = tokenizer.texts_to_sequences(enc_target_data)\n    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n    enc_target_prep = pad_sequences(enc_target_token_seqs, padding='post')\n    print('Create text target sequences done')\n    \n    print('\\n\\n')\n    \n    # show some texts processed\n    for i  in range(2):\n        print(f'               {i} orig: {texts[i]}')\n        print(f'              {i} input: {enc_input_data[i]}')\n        print(f' {i} preprocessed input: {enc_input_prep[i]}')\n        print(f'             {i} target: {enc_target_data[i]}')\n        print(f'{i} preprocessed target: {enc_target_prep[i]}')\n        print()\n\n    print('\\n\\n')\n    print(f' preprocessed input shape: {enc_input_prep.shape}')\n    print(f'preprocessed target shape: {enc_target_prep.shape}')\n    \n    return enc_input_prep, enc_target_prep, tokenizer, enc_input_prep.shape[-1]  # last dim is the length of the sequences        \n\n\nclass History_trained_model(object):\n    \"\"\"Helper class for saving training history\"\"\"\n    def __init__(self, history, epoch, params):\n        self.history = history\n        self.epoch = epoch\n        self.params = params\n        \n\ndef save_hist(path, history):\n    \"\"\"Saves the training history to file\"\"\"\n    with open(path, 'wb') as file:\n        model_history = History_trained_model(history.history, history.epoch, history.params)\n        pickle.dump(model_history, file, pickle.HIGHEST_PROTOCOL)        \n\n        \ndef load_hist(path):\n    \"\"\"Loads the training history from file\"\"\"\n    with open(path, 'rb') as file:\n        history = pickle.load(file)\n    return history","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"# Picture captions\ndf_cap = pd.read_csv(FP_CAPTIONS)\ndisplay(df_cap.head())","metadata":{"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Check Data","metadata":{}},{"cell_type":"code","source":"if RUN_DATA_CHECK :\n    # Number of captions per image\n    print(df_cap.groupby('image').agg(cap_count=('caption','count')).describe())\n\n    # Test if images are there for every entry\n    img_file_not_exist = []\n\n    for image in tqdm(df_cap['image'].unique()):\n        if not os.path.exists(os.path.join(DIR_IMAGES, image)):\n            img_file_not_exist.append(image)\n\n    print(f'Number of image files not exists in \"{DIR_IMAGES}\" directory: {len(img_file_not_exist)}')\n\n    # test if images features are there for every entry\n    img_file_not_exist = []\n    for image in tqdm(df_cap['image'].unique()):\n        if not os.path.exists(os.path.join(DIR_IMAGE_FEATURES, image.replace('.jpg', '.npy'))):\n            img_file_not_exist.append(image)\n\n    print(f'Number of image feature files not exists in \"{DIR_IMAGE_FEATURES}\" directory: {len(img_file_not_exist)}')","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Captions","metadata":{}},{"cell_type":"code","source":"enc_input_prep, enc_target_prep, tokenizer, max_seq_len = preproc_text(df_cap['caption'].values)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Creating and fitting tokenizer...\nCreating and fitting tokenizer done\nCreate text input sequences...\nCreate text input sequences done\nCreate text target sequences...\nCreate text target sequences done\n\n\n\n               0 orig: A child in a pink dress is climbing up a set of stairs in an entry way .\n              0 input: <start> A child in a pink dress is climbing up a set of stairs in an entry way .\n 0 preprocessed input: [   2    1   43    4    1   90  172    7  119   51    1  393   12  394\n    4   28 5159  670    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n             0 target: A child in a pink dress is climbing up a set of stairs in an entry way . <end>\n0 preprocessed target: [   1   43    4    1   90  172    7  119   51    1  393   12  394    4\n   28 5159  670    3    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n\n               1 orig: A girl going into a wooden building .\n              1 input: <start> A girl going into a wooden building .\n 1 preprocessed input: [  2   1  19 316  64   1 196 117   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0]\n             1 target: A girl going into a wooden building . <end>\n1 preprocessed target: [  1  19 316  64   1 196 117   3   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0]\n\n\n\n\n preprocessed input shape: (40455, 38)\npreprocessed target shape: (40455, 38)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create Train-Test Split","metadata":{}},{"cell_type":"code","source":"# Create 80,20 % split on the images\n_, test_img_names = train_test_split(df_cap['image'].unique(), test_size=0.2, random_state=42)\ndf_cap['is_test'] = df_cap['image'].isin(test_img_names)\ndisplay(df_cap.head(2))\n\n# Trainset\ntrain_enc_input_prep = enc_input_prep[df_cap[~df_cap['is_test']].index]\ntrain_enc_target_prep = enc_target_prep[df_cap[~df_cap['is_test']].index]\ntrain_image_feature_names = df_cap[~df_cap['is_test']]['image'].map(lambda x: os.path.join(DIR_IMAGE_FEATURES, x.replace('.jpg', '.npy'))).values\n\n# Testset\ntest_enc_input_prep = enc_input_prep[df_cap[df_cap['is_test']].index]\ntest_enc_target_prep = enc_target_prep[df_cap[df_cap['is_test']].index]\ntest_image_feature_names = df_cap[df_cap['is_test']]['image'].map(lambda x: os.path.join(DIR_IMAGE_FEATURES,x.replace('.jpg', '.npy'))).values\n\n# show train_image_features\ntrain_image_feature_names.shape, train_image_feature_names[:2]","metadata":{"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n\n                                             caption  is_test  \n0  A child in a pink dress is climbing up a set o...    False  \n1              A girl going into a wooden building .    False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>is_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((32360,),\n array(['../input/aida-image-captioning/Images/1000268201_693b08cb0e.npy',\n        '../input/aida-image-captioning/Images/1000268201_693b08cb0e.npy'],\n       dtype=object))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create Train-Test Datasets for Tensorflow\n\n__Currently doesn't work, continue with *Create Data Generator* (next section)__","metadata":{}},{"cell_type":"code","source":"\"\"\"\nBUFFER_SIZE = 1000\nBATCH_SIZE = 64\nEMBEDDING_DIM = 256\nUNITS = 512\nVOCAB_SIZE = len(tokenizer.index_word)\n\nprint(f'vocab size: {VOCAB_SIZE}')\n\n# Load the numpy files\ndef map_func(img_name, cap_inp, cap_targ):\n    #img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n    img_tensor = np.load(img_name)\n    return img_tensor, cap_inp, cap_targ\n\n# train dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_image_feature_names, train_enc_input_prep, train_enc_target_prep))\n#train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": train_image_feature_names, \"input_2\": train_enc_input_prep}, train_enc_target_prep))\n\n# Use map to load the numpy files in parallel\ntrain_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\ndata = next(iter(train_dataset))\n\ntype(data), len(data)\n\n# test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_image_feature_names, test_enc_input_prep, test_enc_target_prep))\n\n# Use map to load the numpy files in parallel\ntest_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Do not shuffle\ntest_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\"\"\"","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\nBUFFER_SIZE = 1000\\nBATCH_SIZE = 64\\nEMBEDDING_DIM = 256\\nUNITS = 512\\nVOCAB_SIZE = len(tokenizer.index_word)\\n\\nprint(f\\'vocab size: {VOCAB_SIZE}\\')\\n\\n# Load the numpy files\\ndef map_func(img_name, cap_inp, cap_targ):\\n    #img_tensor = np.load(img_name.decode(\\'utf-8\\')+\\'.npy\\')\\n    img_tensor = np.load(img_name)\\n    return img_tensor, cap_inp, cap_targ\\n\\n# train dataset\\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_image_feature_names, train_enc_input_prep, train_enc_target_prep))\\n#train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": train_image_feature_names, \"input_2\": train_enc_input_prep}, train_enc_target_prep))\\n\\n# Use map to load the numpy files in parallel\\ntrain_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\\n          num_parallel_calls=tf.data.AUTOTUNE)\\n\\n# Shuffle and batch\\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\\n\\ndata = next(iter(train_dataset))\\n\\ntype(data), len(data)\\n\\n# test dataset\\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_image_feature_names, test_enc_input_prep, test_enc_target_prep))\\n\\n# Use map to load the numpy files in parallel\\ntest_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\\n          num_parallel_calls=tf.data.AUTOTUNE)\\n\\n# Do not shuffle\\ntest_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Setup Data Generator","metadata":{}},{"cell_type":"code","source":"class MyDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Custom class for providing the data to the model\"\"\"\n    def __init__(self, \n                 image_names: np.ndarray, \n                 cap_inputs: np.ndarray, \n                 cap_targets: np.ndarray, \n                 batch_size: int, \n                 shuffle: bool = True):\n        \"\"\"\n        Constructor\n        Arguments:\n            image_name: Name of the imaga file\n            cap_inputs: Word vectors of known captions used for training and test,\n                        containing <start> keyword\n            cap_targets: Word vectors of known captions used for prediction,\n                         containing <end> keyword\n            batch_size: Batch size to use\n            shuffle: True: do shuffeling\n                     False: use order as is\n        \"\"\"\n        self.image_names = image_names\n        self.cap_inputs = cap_inputs\n        self.cap_targets = cap_targets\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        \"\"\"Updates indexes after each epoch\"\"\"\n        self.indexes = np.arange(len(self.image_names))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n        \n    def __len__(self):\n        \"\"\"Returns the number of batch in the sequence.\"\"\"\n        return int(np.floor(len(self.image_names) / self.batch_size))\n    \n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data using the index\"\"\"\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Generate data\n        imgs, cap_ins, cap_outs = [], [], []\n        for i in indexes:\n            imgs.append( np.load(self.image_names[i]) )\n            cap_ins.append( self.cap_inputs[i] )\n            cap_outs.append( self.cap_targets[i] )\n        \n        return (np.array(imgs), np.array(cap_ins)),  np.array(cap_outs)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Config batchsize\nBATCH_SIZE = 64\n\n# Init generators\ntrain_gen = MyDataGenerator(train_image_feature_names, train_enc_input_prep, train_enc_target_prep, batch_size=BATCH_SIZE, shuffle=True)\ntest_gen = MyDataGenerator(test_image_feature_names, test_enc_input_prep, test_enc_target_prep, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Create Encoder-Decoder model for Training/Test","metadata":{}},{"cell_type":"code","source":"# Create own loss function\n# means: mask out padded targets\ndef loss_func(targets, preds):\n    # last layer has softmax activation => from_logits=False\n    #sparse_cat_ce = SparseCategoricalCrossentropy()\n    sparse_cat_ce = SparseCategoricalCrossentropy(from_logits=True)\n    mask = K.cast(K.greater(targets, 0), dtype=tf.int64)    \n    # calc cross entropy masked\n    loss = sparse_cat_ce(targets, preds, sample_weight=mask)\n    return loss\n\n\n# Create own accuracy function\ndef accuracy_func(targets, preds):\n    real_preds = K.cast(K.argmax(preds, axis=-1), dtype=tf.float32)\n    correct = K.cast(K.equal(targets, real_preds), dtype=tf.float32)\n    # mask out 0 padded elements\n    mask = K.cast(K.greater(targets, 0), dtype=tf.float32)\n    n_correct = K.sum(mask * correct)\n    n_total = K.sum(mask)\n    return n_correct/n_total\n\n\nif RUN_TRAINING:    \n    # Set timestamp of training\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n    \n    def create_base_enc_dec_model(\n            enc_input_shape: tuple, \n            output_seq_length: int, \n            embedding_dim: int, \n            vocab_size: int, \n            units: int) -> tf.keras.Model:    \n        \"\"\"\n        Build up the encoder- and decoder-part for training and testing purpose\n\n        Arguments:\n            enc_input_shape: Tuple of input dimensions of the encoder\n            output_seq_length: Max length of caption sequence to predict\n            embedding_dim: Length of used word vectors\n            vocab_size: Number of words within the vocabulary\n            units: Number of CNN and RNN units to use\n        Returns:\n            compiled keras model\n        \"\"\"    \n        # ENCODER\n        enc_input = Input(shape=enc_input_shape, name='enc_input')   \n        enc_output = layers.Flatten()(enc_input) # BSx64x2048 => BSx131072\n        enc_output = layers.Dense(units, activation='relu', name='enc_output')(enc_output)    \n\n        # DECODER\n        dec_input = Input(shape=(None,), name='dec_input')    \n\n        dec_embed_out = layers.Embedding(\n            input_dim=vocab_size, \n            output_dim=embedding_dim, \n            name='dec_embed')(dec_input)\n\n        dec_lstm_out, _, _ = layers.LSTM(\n            units, \n            return_sequences=True, \n            return_state=True,\n            name= 'dec_lstm')(dec_embed_out, initial_state=[enc_output, enc_output])\n\n        dec_out = layers.Dense(vocab_size, activation='softmax', name='dec_dense')(dec_lstm_out)\n\n        # Compile model\n        model = Model([enc_input, dec_input], dec_out, name='Baseline_Train_Test')\n        model.compile(\n            optimizer=Adam(learning_rate=1e-4, clipnorm=5.0),\n            loss=loss_func,\n            metrics=[accuracy_func]        \n        )\n\n        return model\n\n    # Config model dimensions\n    EMBEDDING_DIM = 256\n    UNITS = 512\n    VOCAB_SIZE = len(tokenizer.index_word)\n\n    # Build up model\n    model = create_base_enc_dec_model(\n        enc_input_shape=(64, 2048), \n        output_seq_length=enc_input_prep.shape[-1], \n        embedding_dim=EMBEDDING_DIM,\n        vocab_size=VOCAB_SIZE,\n        units=UNITS)\n\n    # Define callbacks\n    checkpoint_filepath = DIR_MODEL_STORE + 'checkpoint'\n    cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=True,\n        monitor='val_accuracy_func',\n        mode='max',\n        save_best_only=True)\n    cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n    cb_tqdm = tfa.callbacks.TQDMProgressBar()    \n    if LOG_MODEL:\n        cb_tensorboard = tf.keras.callbacks.TensorBoard(\n            log_dir = DIR_TENSORBOARD_LOG + f\"{model.name}_{timestamp_str}\",\n            histogram_freq=0, update_freq='epoch',\n            write_graph=True, write_images=False)\n        callbacks = [cb_early_stopping, cb_tqdm, cb_tensorboard, cb_checkpoint]\n    else:\n        callbacks = [cb_early_stopping, cb_tqdm, cb_checkpoint]\n\n    # Plot model\n    if SAVE_MODEL:\n        plot_model(model, \n               show_shapes=True, \n               to_file=DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".png\")\n    else:\n        plot_model(model, show_shapes=True)\n\n    # Train model\n    if RUN_TRAINING:\n        history = model.fit(\n            train_gen,\n            epochs=EPOCHS, \n            validation_data=test_gen, \n            callbacks=callbacks,\n            verbose=0)\n\n        # Save model and history\n        if SAVE_MODEL:\n            # Save history\n            fp_hist = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".pickle\"\n            save_hist(fp_hist, history)\n            print(f\"Model history for {model.name} saved to: {fp_hist}\")\n            \n            # Save model\n            fp_model = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".h5\"\n            model.save(fp_model, save_format='h5')\n            print(f\"Model {model.name} saved to: {fp_model}\")","metadata":{"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|           0/1 ETA: ?s,  ?epochs/s","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737658a4aff8460cb7c8741b8a312226"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0/505           ETA: ?s - ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fa8ea5b0a94c12811e1cbbfc31b3d5"}},"metadata":{}},{"name":"stdout","text":"Model history for Baseline_Train_Test saved to: ./models/Baseline_Train_Test_20210428-175738.pickle\nModel Baseline_Train_Test saved to: ./models/Baseline_Train_Test_20210428-175738.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display Tensorboard\nif TENSORBOARD_SHOW_INLINE:\n    # Load extension\n    %load_ext tensorboard\n\n    # Start tensorboard\n    if TENSORBOARD_LAUNCH:\n        %tensorboard --logdir $DIR_TENSORBOARD_LOG\n    else:\n        # Show tensorboard inline für monitoring training performance\n        tb.notebook.list() # View open TensorBoard instances\n        tb.notebook.display(port=6006, height=800)  #  Select the hopefully already startet","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Create Encoder-Decoder model for Prediction","metadata":{}},{"cell_type":"code","source":"def creat_prediction_models(model: tf.keras.Model, units: int) -> (tf.keras.Model, tf.keras.Model):\n    \"\"\"\n    Create prediction models from trained model\n    \n    Arguments:\n        model: Former created training model\n        units: Number of CNN and RNN units to use\n    Returns:\n        Tuple containing the compiled models\n        Index: 0: endoder\n               1: decoder\n    \"\"\"    \n\n    # ENCODER\n    enc_input = model.get_layer(name='enc_input').input   \n    enc_output = model.get_layer(name='enc_output').output\n    encoder_model = Model(enc_input, [enc_output, enc_output])  # I take the same for hidden state and cell state\n        \n    # DECODER\n    # 1. define Inputs (LSTM => (hidden_state, cell_state))\n    dec_input = model.get_layer(name='dec_input').input\n    dec_state_input_h = Input(shape=(units,))\n    dec_state_input_c = Input(shape=(units,))\n    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n    \n    # 2. get embedding layer\n    dec_embedding = model.get_layer(name='dec_embed')\n    dec_outputs = dec_embedding(dec_input)\n    \n    # 3. get LSTM \n    # initial_state comes from encoder in first time step after that from last times hidden+cell state\n    dec_lstm = model.get_layer(name='dec_lstm')  \n    dec_outputs, state_h_dec, state_c_dec = dec_lstm(\n        dec_outputs, initial_state=dec_states_inputs\n    )\n  \n    # 4. output FC layer\n    dec_dense = model.get_layer(name='dec_dense')\n    dec_outputs = dec_dense(dec_outputs)\n    \n    # 5. Create Model\n    decoder_model = Model(\n        [dec_input, dec_state_input_h, dec_state_input_c],\n        [dec_outputs, state_h_dec, state_c_dec] # give out hidden state for next iteration\n    )\n    \n    return encoder_model, decoder_model\n\n# Build up model\nif not RUN_TRAINING:\n    # Load training history\n    fp_hist = DIR_MODEL_STORE + FN_HIST2LOAD\n    history = load_hist(fp_hist)\n    print(f\"Training history for model {model.name} loaded from file: {fp_hist}\")\n    \n    # Load model\n    fp_model = DIR_MODEL_STORE + FN_MODEL2LOAD\n    model = tf.keras.models.load_model(\n        fp_model,\n        custom_objects={\"loss_func\": loss_func, \"accuracy_func\" : accuracy_func}\n    )\n    print(f\"Model {model.name} loaded from file: {fp_model}\")\n    \nencoder_model, decoder_model = creat_prediction_models(model, units=UNITS)","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"MAX_DECODER_SEQ_LEN = enc_input_prep.shape[-1]\n\ndef make_prediction(input_img_feature_path, verbose: int = 0):\n    \"\"\"\n    Initialize \n    Arguments:\n        input_img_feature_path: Complete path to image file\n        verbose: 0: don't show any comment\n                 1: show working comments\n    Returns:\n        predicted caption for the input image\n    \"\"\"\n    # input_img_feature_path: path to picture features numpy data\n    # load numpy features from input_img_feature_path\n    img_features = np.expand_dims(np.load(input_img_feature_path), axis=0 )\n    \n    # get encoder states\n    state_h, state_c = encoder_model.predict(img_features)\n    if verbose >= 1:\n        print('state_h, state_c .shape', state_h.shape, state_c.shape)\n    \n    # create start token\n    dec_input = tf.constant([[tokenizer.word_index['<start>']]])\n    if verbose >= 1:\n        print('dec_input.shape', dec_input.shape)\n    \n    caption = []\n    while len(caption) < MAX_DECODER_SEQ_LEN:\n        dec_output, state_h, state_c = decoder_model.predict(\n            [dec_input, state_h, state_c],\n            verbose = 0,\n        )\n        \n        # next input must be the current output (this is already a word index)\n        dec_input = tf.argmax(dec_output, -1)\n\n        word_ndx = dec_input.numpy()[0,0] # detach from tf\n        predicted_word = tokenizer.index_word[word_ndx]\n        \n        # do not append <end> tag to caption\n        if predicted_word == '<end>':\n           break\n        \n        caption.append(predicted_word)\n        \n    return ' '.join(caption)","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Calc ROUGE-L and BLEU-1 metrics","metadata":{}},{"cell_type":"markdown","source":"### Testset","metadata":{}},{"cell_type":"code","source":"# Prepare true captures (references) and predicted captures (candidates)\nc_true_test = dict()\nc_pred_test = dict()\ndf = df_cap.loc[df_cap.is_test == True]\nfor img_fname in tqdm.notebook.tqdm(df.image.unique(), total=len(df.image.unique())):\n    c_true_test[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()\n    c_pred_test[img_fname] = make_prediction(os.path.join(DIR_IMAGE_FEATURES, img_fname.replace('.jpg','.npy')))\n\n# Serialize data into file\nif timestamp_str == None:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\njson.dump(c_true_test, open(DIR_RESULT_STORE + f\"c_true_test_{timestamp_str}.json\", 'w'))\njson.dump(c_pred_test, open(DIR_RESULT_STORE + f\"c_pred_test_{timestamp_str}.json\", 'w'))\n\n# Calculate Metrics\nrouge_l_test, bleu_1_test = met.evaluate(c_true_test, c_pred_test)\nprint(f\"Metrics for Testset:\\nROUGE-L recall:{rouge_l_test:>10.2%}\\nBLEU-1 precision: {bleu_1_test:>7.2%}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainset","metadata":{}},{"cell_type":"code","source":"# Prepare true captures (references) and predicted captures (candidates)\nc_true_train = dict()\nc_pred_train = dict()\ndf = df_cap.loc[df_cap.is_test == False]\nfor img_fname in tqdm.notebook.tqdm(df.image.unique(), total=len(df.image.unique())):\n    c_true_train[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()\n    c_pred_train[img_fname] = make_prediction(os.path.join(DIR_IMAGE_FEATURES, img_fname.replace('.jpg','.npy')))\n\n# Serialize data into file\nif timestamp_str == None:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\njson.dump(c_true_train, open(DIR_RESULT_STORE + f\"c_true_train_{timestamp_str}.json\", 'w'))\njson.dump(c_pred_train, open(DIR_RESULT_STORE + f\"c_pred_train_{timestamp_str}.json\", 'w'))\n\n# Calculate Metrics\nrouge_l_train, bleu_1_train = met.evaluate(c_true_train, c_pred_train)\nprint(f\"Metrics for Trainset:\\nROUGE-L recall:{rouge_l_train:>10.2%}\\nBLEU-1 precision: {bleu_1_train:>7.2%}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save results","metadata":{}},{"cell_type":"code","source":"# Save metrics to csv file\nif timestamp_str == None:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n\ndf = pd.DataFrame({\n    \"Metric\": [\"ROUGE-L recallt\", \"BLEU-1 precission\"],\n    \"Score Test\": [rouge_l_test, bleu_1_test],\n    \"Score Train\": [rouge_l_train, bleu_1_train]}).reset_index(drop=True)\n\ndf.to_csv(DIR_RESULT_STORE + f\"df_result_{timestamp_str}.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict something from test","metadata":{}},{"cell_type":"code","source":"def eval_image_from_test(img_name):\n    img = plt.imread(os.path.join(DIR_IMAGES, img_name))\n    plt.figure(figsize=(8,8))\n    plt.imshow(img)\n    plt.show()\n    \n    print(f'\\nReference captions:\\n{\"-\"*80}')\n    for l in df_cap[df_cap['image']==img_name]['caption'].values:\n        print(f'- {l}')\n    \n    prediction = make_predict(os.path.join(DIR_IMAGE_FEATURES, img_name.replace('.jpg','.npy')))\n    print(f'\\nPrediction:\\n{\"-\"*80}')\n    print(f'- {prediction}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_predictions = 2\nfor img_file_name in df_cap[df_cap['is_test']].sample(num_predictions)['image'].values:\n    eval_image_from_test(img_file_name)\n    print(f'{\"=\"*80}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}