{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n#from tqdm import tqdm\nimport datetime as dt  # datetime handling\nimport json\nimport tensorflow_addons as tfa\n\nimport tensorflow as tf\nimport sys\nimport tqdm  # progress bars\nimport nltk.translate.bleu_score as ntbs  # BLEU scoring basics\n\n# Install packages\n!pip install rouge-score\n\nimport rouge_score.rouge_scorer as rouge_scorer  # ROUGE scoring basics\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.7/site-packages (0.0.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.19.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (0.12.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"MODEL_NAME = 'WordVecs_2_LSTMS_Train_Test'\n\nFP_CAPTIONS = '../input/flickr8k/captions.txt'\nDIR_IMAGES = '../input/flickr8k/Images/'\nDIR_IMAGE_FEATURES = '../input/aida-image-captioning/Images/'\nDIR_MODEL_STORE = './models/'\nDIR_MODEL_LOG = './models/'\nDIR_RESULT_STORE = './results/'\nDIR_TENSORBOARD_LOG = './tensorboard/'\nEARLY_STOPPING_PATIENT = 10\ntimestamp_str = None","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!mkdir results\n!mkdir models    \n!mkdir tensorboard\n!mkdir models/$MODEL_NAME","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load word vectors","metadata":{}},{"cell_type":"code","source":"import pickle\n\nembeddings_index = pickle.load(  open( \"../input/aida-image-captioning-preprocessing-glove/embeddings_index.p\", \"rb\" ) )","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"values_view = embeddings_index.values()\nvalue_iterator = iter(values_view)\nfirst_value = next(value_iterator)\n\nwv_size = first_value.shape[0]","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"df_cap = pd.read_csv('../input/flickr8k/captions.txt')\ndf_cap.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# check number captions per image\ndf_cap.groupby('image').agg(cap_count=('caption','count')).describe()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       cap_count\ncount     8091.0\nmean         5.0\nstd          0.0\nmin          5.0\n25%          5.0\n50%          5.0\n75%          5.0\nmax          5.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cap_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8091.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# test if images are there for every entry\nimg_file_not_exist = []\n\nIMAGES_BASE = '../input/flickr8k/Images'\nfor image in df_cap['image'].unique():\n    if not os.path.exists(os.path.join(IMAGES_BASE, image)):\n        img_file_not_exist.append(image)\n    \nprint(f'Number of image files not exists in \"{IMAGES_BASE}\" directory: {len(img_file_not_exist)}')","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of image files not exists in \"../input/flickr8k/Images\" directory: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# test if images features are there for every entry\nimg_file_not_exist = []\n\nIMAGE_FEATURES_BASE = '../input/aida-image-captioning/Images'\nfor image in df_cap['image'].unique():\n    if not os.path.exists(os.path.join(IMAGE_FEATURES_BASE, image.replace('.jpg', '.npy'))):\n        img_file_not_exist.append(image)\n    \nprint(f'Number of image feature files not exists in \"{IMAGE_FEATURES_BASE}\" directory: {len(img_file_not_exist)}')","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of image feature files not exists in \"../input/aida-image-captioning/Images\" directory: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess Captions","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef preproc_text(texts):\n    \"\"\"\n    - Adds <start> to the beginning of texts to the inputs\n    - Adds <end> to\n    Input: texts: list of sentences\n    Output: - tokenized, padded text with <start> token for using as input in decoder\n            - tokenized, padded text with <end> token for using as target in decoder\n            - tokenizer\n            - max_seq_len\n    \"\"\"\n    \n    print('Creating and fitting tokenizer...')\n    tokenizer_fit_data = ['<start> ' + s + ' <end>' for s in texts]\n\n    # create tokenizer for captions + <start> and <end> tags\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;?@[\\\\]^_`{|}~\\t\\n')\n    tokenizer.fit_on_texts(tokenizer_fit_data)\n    tokenizer.word_index['<pad>'] = 0\n    tokenizer.index_word[0] = '<pad>'\n    print('Creating and fitting tokenizer done')\n    \n    print('Create text input sequences...')\n    # add <start> tag before every caption and tokenize + pad ('post')\n    enc_input_data = ['<start> ' + s for s in texts]\n    enc_input_token_seqs = tokenizer.texts_to_sequences(enc_input_data)\n    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n    enc_input_prep = pad_sequences(enc_input_token_seqs, padding='post')\n    print('Create text input sequences done')\n    \n    print('Create text target sequences...')\n    # add <start> tag before every caption and tokenize + pad ('post')\n    enc_target_data = [s + ' <end>' for s in texts]\n    enc_target_token_seqs = tokenizer.texts_to_sequences(enc_target_data)\n    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n    enc_target_prep = pad_sequences(enc_target_token_seqs, padding='post')\n    print('Create text target sequences done')\n    \n    print('\\n\\n')\n    \n    # show some texts processed\n    for i  in range(2):\n        print(f'               {i} orig: {texts[i]}')\n        print(f'              {i} input: {enc_input_data[i]}')\n        print(f' {i} preprocessed input: {enc_input_prep[i]}')\n        print(f'             {i} target: {enc_target_data[i]}')\n        print(f'{i} preprocessed target: {enc_target_prep[i]}')\n        print()\n\n    print('\\n\\n')\n    print(f' preprocessed input shape: {enc_input_prep.shape}')\n    print(f'preprocessed target shape: {enc_target_prep.shape}')\n    \n    return enc_input_prep, enc_target_prep, tokenizer, enc_input_prep.shape[-1]  # last dim is the length of the sequences        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_input_prep, enc_target_prep, tokenizer, max_seq_len = preproc_text(df_cap['caption'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size =  len(tokenizer.word_index) \n# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, wv_size))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nembedding_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# create 80,20 % split on the images\n_, test_img_names = train_test_split(df_cap['image'].unique(), test_size=0.2, random_state=42)\n\ndf_cap['is_test'] = df_cap['image'].isin(test_img_names)\n\ndf_cap.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_enc_input_prep = enc_input_prep[df_cap[~df_cap['is_test']].index]\ntrain_enc_target_prep = enc_target_prep[df_cap[~df_cap['is_test']].index]\n\ntest_enc_input_prep = enc_input_prep[df_cap[df_cap['is_test']].index]\ntest_enc_target_prep = enc_target_prep[df_cap[df_cap['is_test']].index]\n\ntrain_image_feature_names = df_cap[~df_cap['is_test']]['image'].map(lambda x: os.path.join(IMAGE_FEATURES_BASE, x.replace('.jpg', '.npy'))).values\ntest_image_feature_names = df_cap[df_cap['is_test']]['image'].map(lambda x: os.path.join(IMAGE_FEATURES_BASE,x.replace('.jpg', '.npy'))).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show train_image_features\ntrain_image_feature_names.shape, train_image_feature_names[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Train-Test Datasets for Tensorflow\n\n__Currently doesn't work, continue with *Create Data Generator* (next section)__","metadata":{}},{"cell_type":"code","source":"BUFFER_SIZE = 1000\nBATCH_SIZE = 64\nEMBEDDING_DIM = 256\nUNITS = 512\nVOCAB_SIZE = len(tokenizer.index_word)\n\nprint(f'vocab size: {VOCAB_SIZE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap_inp, cap_targ):\n    #img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n    img_tensor = np.load(img_name)\n    return img_tensor, cap_inp, cap_targ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_image_feature_names, train_enc_input_prep, train_enc_target_prep))\n#train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": train_image_feature_names, \"input_2\": train_enc_input_prep}, train_enc_target_prep))\n\n# Use map to load the numpy files in parallel\ntrain_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(data), len(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_image_feature_names, test_enc_input_prep, test_enc_target_prep))\n\n# Use map to load the numpy files in parallel\ntest_dataset = train_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n          map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Do not shuffle\ntest_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Data Generator","metadata":{}},{"cell_type":"code","source":"class MyDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, image_names, cap_inputs, cap_targets, batch_size, shuffle=True):\n        self.image_names = image_names\n        self.cap_inputs = cap_inputs\n        self.cap_targets = cap_targets\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.image_names))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n        \n    def __len__(self):\n        return int(np.floor(len(self.image_names) / self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Generate data\n        imgs, cap_ins, cap_outs = [], [], []\n        \n        for i in indexes:\n            imgs.append( np.load(self.image_names[i]) )\n            cap_ins.append( self.cap_inputs[i] )\n            cap_outs.append( self.cap_targets[i] )\n        \n        return (np.array(imgs), np.array(cap_ins)),  np.array(cap_outs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nEMBEDDING_DIM = 256\nUNITS = 2048#512\nVOCAB_SIZE = len(tokenizer.index_word)\n\ntrain_gen = MyDataGenerator(train_image_feature_names, train_enc_input_prep, train_enc_target_prep, batch_size=BATCH_SIZE, shuffle=True)\ntest_gen = MyDataGenerator(test_image_feature_names, test_enc_input_prep, test_enc_target_prep, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Enc-Dec Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, Input, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras import backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_base_enc_dec_model(enc_input_shape, output_seq_length, embedding_dim, vocab_size, units):\n    # ENCODER\n    enc_input = Input(shape=enc_input_shape, name='enc_input')   \n    enc_output=layers.Reshape((8,8,2048))(enc_input)\n    enc_output= layers.GlobalAveragePooling2D()(enc_output)\n    #enc_output = layers.Flatten()(enc_input) # BSx64x2048 => BSx131072\n    enc_output = layers.Dense(units, activation='relu', name='enc_output')(enc_output)    \n\n    # DECODER\n    # dec_input = Input(shape=(output_seq_length,), name='dec_input')    \n    dec_input = Input(shape=(None,), name='dec_input')    \n    #dec_embed_out = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='dec_embed')(dec_input)\n    dec_embed_out = layers.Embedding(vocab_size, wv_size, weights=[embedding_matrix], trainable=False, name='dec_embed')(dec_input) #input_length=max_seq_len, \n    #dec_embed_out = layers.Dropout(.2)(dec_embed_out)\n    dec_lstm_out, a, b = layers.LSTM(units, return_sequences=True, return_state=True,name= 'dec_lstm')(dec_embed_out, initial_state=[enc_output, enc_output])\n    \n    dec_lstm_out = layers.LSTM(units, return_sequences=True, return_state=False,name= 'dec_lstm2')(dec_lstm_out)#, initial_state=[a, b])\n    #dec_lstm_out = layers.Dropout(.2)(dec_lstm_out)\n    dec_out = layers.Dense(vocab_size, activation='softmax', name='dec_dense')(dec_lstm_out)\n    \n    # create own loss function\n    # means: mask out padded targets\n    def loss_func(targets, preds):\n        # last layer has softmax activation => from_logits=False\n        #sparse_cat_ce = SparseCategoricalCrossentropy()\n        sparse_cat_ce = SparseCategoricalCrossentropy(from_logits=True)\n        mask = K.cast(K.greater(targets, 0), dtype=tf.int64)    \n        # calc cross entropy masked\n        loss = sparse_cat_ce(targets, preds, sample_weight=mask)\n        return loss\n\n    def accuracy_func(targets, preds):\n        real_preds = K.cast(K.argmax(preds, axis=-1), dtype=tf.float32)\n        correct = K.cast(K.equal(targets, real_preds), dtype=tf.float32)\n        # mask out 0 padded elements\n        mask = K.cast(K.greater(targets, 0), dtype=tf.float32)\n        n_correct = K.sum(mask * correct)\n        n_total = K.sum(mask)\n        return n_correct/n_total\n    \n    model = Model([enc_input, dec_input], dec_out)\n\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4, clipnorm=5.0),\n        loss=loss_func,\n        metrics=[accuracy_func]        \n    )\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_base_enc_dec_model(\n    enc_input_shape=(64, 2048), \n    output_seq_length=enc_input_prep.shape[-1], \n    embedding_dim=EMBEDDING_DIM,\n    vocab_size=VOCAB_SIZE,\n    units=UNITS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n#history = model.fit(train_gen, epochs=100, validation_data=test_gen, callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define callbacks\ncheckpoint_filepath = DIR_MODEL_STORE + 'checkpoint'\ncb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy_func',\n    mode='max',\n    save_best_only=True)\ncb_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=EARLY_STOPPING_PATIENT)\ncb_tqdm = tfa.callbacks.TQDMProgressBar()\nif True: #LOG_MODEL:\n    cb_tensorboard = tf.keras.callbacks.TensorBoard(\n        log_dir = DIR_TENSORBOARD_LOG + f\"{model.name}_{timestamp_str}\",\n        histogram_freq=0, update_freq='epoch',\n        write_graph=True, write_images=False)\n    callbacks = [cb_early_stopping, cb_tqdm, cb_tensorboard, cb_checkpoint]\nelse:\n    callbacks = [cb_early_stopping, cb_tqdm, cb_checkpoint]\n\n# Plot model\nif True: #SAVE_MODEL:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n    plot_model(model, \n           show_shapes=True, \n           to_file=DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".png\")\nelse:\n    plot_model(model, show_shapes=True)\n\nEPOCHS=50\n# Train model\nif True:\n    history = model.fit(\n        train_gen,\n        epochs=EPOCHS, \n        validation_data=test_gen, \n        callbacks=callbacks,\n        verbose=0)\n\n    # Save model and history\n    if True: #SAVE_MODEL:\n        # Save history\n        fp_hist = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".pickle\"\n        save_hist(fp_hist, history)\n        print(f\"Model history for {model.name} saved to: {fp_hist}\")\n\n        # Save model\n        fp_model = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".h5\"\n        model.save(fp_model, save_format='h5')\n        print(f\"Model {model.name} saved to: {fp_model}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_fname = 'trained_model_wv.model'\nmodel.save(trained_model_fname)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create prediction models","metadata":{}},{"cell_type":"code","source":" #create prediction models from trained model\ndef crea_prediction_models(model, units):\n    # ENCODER\n    enc_input = model.get_layer(name='enc_input').input   \n    enc_output = model.get_layer(name='enc_output').output\n    encoder_model = Model(enc_input, [enc_output, enc_output, enc_output, enc_output])  # I take the same for hidden state and cell state\n        \n    # DECODER\n    # 1. define Inputs (LSTM => (hidden_state, cell_state))\n    dec_input = model.get_layer(name='dec_input').input\n    dec_state_input_h = Input(shape=(units,))\n    dec_state_input_c = Input(shape=(units,))\n    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n\n    #dec_state_input_h2 = Input(shape=(units,))\n    #dec_state_input_c2 = Input(shape=(units,))\n    #dec_states_inputs2 = [dec_state_input_h2, dec_state_input_c2]\n\n    # 2. get embedding layer\n    dec_embedding = model.get_layer(name='dec_embed')\n    dec_outputs = dec_embedding(dec_input)\n    \n    # 3. get LSTM \n    # initial_state comes from encoder in first time step after that from last times hidden+cell state\n    dec_lstm = model.get_layer(name='dec_lstm')  \n    dec_outputs, state_h_dec, state_c_dec = dec_lstm(\n        dec_outputs, initial_state=dec_states_inputs\n    )\n    \n    dec_lstm2 = model.get_layer(name='dec_lstm2')  \n    dec_outputs2 = dec_lstm2(#, state_h_dec2, state_c_dec2\n        dec_outputs#, initial_state=dec_states_inputs2\n    )    \n  \n    # 4. output FC layer\n    dec_dense = model.get_layer(name='dec_dense')\n    dec_outputs = dec_dense(dec_outputs2)\n    \n    # 5. Create Model\n    decoder_model = Model(\n        [dec_input, dec_state_input_h, dec_state_input_c],#, dec_state_input_h2, dec_state_input_c2],\n        [dec_outputs, state_h_dec, state_c_dec] #, state_h_dec2, state_c_dec2] # give out hidden state for next iteration\n    )\n    \n    return encoder_model, decoder_model\n\nencoder_model, decoder_model = crea_prediction_models(model, units=UNITS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_DECODER_SEQ_LEN = enc_input_prep.shape[-1]\n\ndef make_predict(input_img_feature_path):\n    #\n    # input_img_feature_path: path to picture features numpy data\n    \n    # load numpy features from input_img_feature_path\n    img_features = np.expand_dims( np.load(input_img_feature_path), axis=0 )\n    \n    # get encoder states\n    state_h, state_c, state_h2, state_c2 = encoder_model.predict(img_features)\n    \n    #print('state_h, state_c .shape', state_h.shape, state_c.shape)\n    \n    # create start token\n    dec_input = tf.constant([[tokenizer.word_index['<start>']]])\n    #print('dec_input.shape', dec_input.shape)\n    \n    caption = []\n    \n    while len(caption) < MAX_DECODER_SEQ_LEN:\n        dec_output, state_h, state_c= decoder_model.predict(\n            [dec_input, state_h, state_c] #, state_h2, state_c2]\n        )\n        \n        # next input must be the current output (this is already a word index)\n        dec_input = tf.argmax(dec_output, -1)\n\n        word_ndx = dec_input.numpy()[0,0] # detach from tf\n        predicted_word = tokenizer.index_word[word_ndx]\n        \n        # do not append <end> tag to caption\n        if predicted_word == '<end>':\n           break\n        \n        caption.append(predicted_word)\n        \n    return ' '.join(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef eval_image_from_test(img_name):\n    img = plt.imread(os.path.join(IMAGES_BASE, img_name))\n    plt.figure(figsize=(8,8))\n    plt.imshow(img)\n    plt.show()\n    \n    print(f'\\nReference captions:\\n{\"-\"*80}')\n    for l in df_cap[df_cap['image']==img_name]['caption'].values:\n        print(f'- {l}')\n    \n    prediction = make_predict(os.path.join(IMAGE_FEATURES_BASE, img_name.replace('.jpg','.npy')))\n    print(f'\\nPrediction:\\n{\"-\"*80}')\n    print(f'- {prediction}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict something from test","metadata":{}},{"cell_type":"code","source":"num_predictions = 10\nfor img_file_name in df_cap[df_cap['is_test']].sample(num_predictions)['image'].values:\n    eval_image_from_test(img_file_name)\n    print(f'{\"=\"*80}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calc ROUGE-L and BLEU-1 metrics","metadata":{}},{"cell_type":"code","source":"sys.path.insert(0, '../input/aida-metrics-functions')  # custom metrics implementation\nimport metrics as met  # custom metrics BLUE-1 pecsision and ROUGE-L recall\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIR_RESULT_STORE = './results/'\n!mkdir $DIR_RESULT_STORE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testset","metadata":{}},{"cell_type":"code","source":"# Prepare true captures (references) and predicted captures (candidates)\nc_true_test = dict()\nc_pred_test = dict()\ndf = df_cap.loc[df_cap.is_test == True]\nfor img_fname in tqdm.tqdm_notebook(df.image.unique(), total=len(df.image.unique())):\n    c_true_test[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()    \n    c_pred_test[img_fname] = make_predict(os.path.join(IMAGE_FEATURES_BASE, img_fname.replace('.jpg','.npy')))\n\n#timestamp_str=None\n# Serialize data into file\nif timestamp_str == None:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\njson.dump(c_true_test, open(DIR_RESULT_STORE + f\"c_true_test_{timestamp_str}.json\", 'w'))\njson.dump(c_pred_test, open(DIR_RESULT_STORE + f\"c_pred_test_{timestamp_str}.json\", 'w'))\n\n# Calculate Metrics\nrouge_l_test, bleu_1_test = met.evaluate(c_true_test, c_pred_test)\nprint(f\"Metrics for Testset:\\nROUGE-L recall:{rouge_l_test:>10.2%}\\nBLEU-1 precision: {bleu_1_test:>7.2%}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainset","metadata":{}},{"cell_type":"code","source":"# Prepare true captures (references) and predicted captures (candidates)\nc_true_train = dict()\nc_pred_train = dict()\ndf = df_cap.loc[df_cap.is_test == False]\nfor img_fname in tqdm.tqdm_notebook(df.image.unique(), total=len(df.image.unique())):\n    c_true_train[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()\n    c_pred_train[img_fname] = make_predict(os.path.join(IMAGE_FEATURES_BASE, img_fname.replace('.jpg','.npy')))\n\n# Serialize data into file\nif timestamp_str == None:\n    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\njson.dump(c_true_train, open(DIR_RESULT_STORE + f\"c_true_train_{timestamp_str}.json\", 'w'))\njson.dump(c_pred_train, open(DIR_RESULT_STORE + f\"c_pred_train_{timestamp_str}.json\", 'w'))\n\n# Calculate Metrics\nrouge_l_train, bleu_1_train = met.evaluate(c_true_train, c_pred_train)\nprint(f\"Metrics for Trainset:\\nROUGE-L recall:{rouge_l_train:>10.2%}\\nBLEU-1 precision: {bleu_1_train:>7.2%}\")","metadata":{},"execution_count":null,"outputs":[]}]}