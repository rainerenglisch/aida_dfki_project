{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJCMwRx9_4rN"
   },
   "source": [
    "# POC using object recognition in Encoder-Part (CNN)\n",
    "Approach:\n",
    "* Transfer captions to entities\n",
    "* Check quality of recognition\n",
    "* Use net as Decoder-Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7yzKetZAK3s"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "iLciMje9AOmB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "#https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "#from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from numba import cuda\n",
    "\n",
    "import sklearn.model_selection as skms\n",
    "from sklearn.utils import class_weight\n",
    "import sklearn.feature_extraction.text as skfet\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.stem as nstem\n",
    "import sklego.meta as sklmet\n",
    "\n",
    "#from wcs.google import google_drive_share\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#from google.colab import drive\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcfsOKfzC2N6"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime config\n",
    "RUN_ON_KAGGLE = False\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"InceptionV3_customized\"\n",
    "DO_TRAIN_VALID_SPLIT = False\n",
    "\n",
    "BATCH_SIZE = 64*4\n",
    "EPOCHS = 10\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE  # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "\n",
    "DO_SHUFFLE = False\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "\n",
    "IMG_DIMS = [299, 299]\n",
    "IMG_CHANNELS = 3  # Keep RGB color channels to match the input format of the model\n",
    "\n",
    "# Data pipeline\n",
    "DP_TFDATA = \"Data pipeline using tf.data\"\n",
    "DP_IMGGEN = \"Data pipeline using tf.keras.ImageGenerator\"\n",
    "DP = DP_TFDATA\n",
    "\n",
    "\n",
    "# Directories and filenames\n",
    "if RUN_ON_KAGGLE:\n",
    "    FP_CAPTIONS = '../input/flickr8k/captions.txt'\n",
    "    DIR_IMAGES = '../input/flickr8k/Images/'\n",
    "    DIR_IMAGE_FEATURES = '../input/aida-image-captioning/Images/'\n",
    "    DIR_MODEL_STORE = './models/'\n",
    "    DIR_MODEL_LOG = './models/'\n",
    "    DIR_RESULT_STORE = './results/'\n",
    "    DIR_TENSORBOARD_LOG = './tensorboard/'\n",
    "    DIR_INTERIM = \"./models/data/interim/\"\n",
    "    DIR_RAW = \"./models/data/raw/\"\n",
    "else:\n",
    "    FP_CAPTIONS = '../data/raw/flickr8k/captions.txt'\n",
    "    DIR_IMAGES = '../data/raw/flickr8k/Images/'\n",
    "    DIR_IMAGE_FEATURES = '../data/interim/aida-image-captioning/Images/'\n",
    "    DIR_MODEL_STORE = f'../models/{MODEL_NAME}/'\n",
    "    DIR_MODEL_LOG = f'../models/logs/{MODEL_NAME}/'\n",
    "    DIR_RESULT_STORE = f'../data/results/{MODEL_NAME}/'\n",
    "    DIR_TENSORBOARD_LOG = './tensorboard_logs/scalars/'\n",
    "    DIR_INTERIM = '../data/interim/'\n",
    "    DIR_RAW = \"../data/raw/\"\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Set the max column width to see the complete caption\n",
    "pd.set_option('display.max_colwidth',-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_text(text: str) -> str:\n",
    "    \"\"\"Get roots of words\"\"\"\n",
    "    # Split text to list\n",
    "    l_text = text.lower().split()\n",
    "\n",
    "    # Lemmatize\n",
    "    lemma = nstem.WordNetLemmatizer()\n",
    "    for i, t in enumerate(l_text):\n",
    "        tl = lemma.lemmatize(t)\n",
    "        if tl != t:\n",
    "            l_text[i] = tl\n",
    "\n",
    "    return \" \".join(l_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: During GPU handling.\n",
      "1 Physical GPU, 2 Logical GPUs\n",
      "\n",
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1'], variable_device = '/device:CPU:0'\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "\n",
      "Runtime Context: 1-GPU\n",
      "Recommended Batch Size: 1024 datasets\n"
     ]
    }
   ],
   "source": [
    "# To get access to a GPU instance you can use the `change runtime type` and set the option to `GPU` from the `Runtime` tab  in the notebook\n",
    "# Checking the GPU availability for the notebook\n",
    "#tf.test.gpu_device_name()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Create virtual GPUs\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK\n",
    "            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],\n",
    "        )\n",
    "\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK \n",
    "            gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],            \n",
    "        )\n",
    "    except:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(\"Warning: During GPU handling.\")\n",
    "        pass\n",
    "    finally:\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\\n\")\n",
    "\n",
    "# Set runtime context and batch size\n",
    "l_rtc_names = [\n",
    "    \"multi-GPU_MirroredStrategy\",\n",
    "    \"multi-GPU_CentralStorageStrategy\",\n",
    "    \"1-GPU\",\n",
    "    \"CPUs\",\n",
    "    \"multi-GPU_MirroredStrategy_NCCL-All-Reduced\",\n",
    "]\n",
    "l_rtc = [\n",
    "    tf.distribute.MirroredStrategy().scope(),\n",
    "    tf.distribute.experimental.CentralStorageStrategy().scope(),\n",
    "    tf.device(\"/GPU:0\"),\n",
    "    tf.device(\"/CPU:0\"),\n",
    "    tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce()).scope(),\n",
    "]\n",
    "if len(gpus) == 0:\n",
    "    rtc_idx = 3\n",
    "    batch_size = 64\n",
    "\n",
    "elif len(gpus) == 1:\n",
    "    rtc_idx = 2\n",
    "    batch_size = 4*256\n",
    "\n",
    "elif len(gpus) > 1:\n",
    "    rtc_idx = 0\n",
    "    batch_size = 8*256\n",
    "\n",
    "runtime_context = l_rtc[rtc_idx]\n",
    "\n",
    "print(f\"\\nRuntime Context: {l_rtc_names[rtc_idx]}\")\n",
    "print(f\"Recommended Batch Size: {batch_size} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapipeline based on tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(DIR_IMAGES + filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=IMG_CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_DIMS[0], IMG_DIMS[1]])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, cache=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if cache == True:\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        dataset = dataset.cache()\n",
    "        # Shuffle the data each buffer size\n",
    "        if DO_SHUFFLE:\n",
    "            dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA1fV-j_KX0L"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "acVfQ8KqKlaI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a wooden cabin .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n",
       "1  A girl going into a wooden building .                                     \n",
       "2  A little girl climbing into a wooden playhouse .                          \n",
       "3  A little girl climbing the stairs to her playhouse .                      \n",
       "4  A little girl in a pink dress going into a wooden cabin .                 "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "# Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples.\n",
    "df = pd.read_csv(FP_CAPTIONS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "acVfQ8KqKlaI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a spotted dog are fighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002674143_1b742ab4b8.jpg</td>\n",
       "      <td>A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003163366_44323f5815.jpg</td>\n",
       "      <td>A man lays on a bench while his dog sits by him .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1007129816_e794419615.jpg</td>\n",
       "      <td>A man in an orange hat starring at something .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1001773457_577c3a7d70.jpg   \n",
       "2  1002674143_1b742ab4b8.jpg   \n",
       "3  1003163366_44323f5815.jpg   \n",
       "4  1007129816_e794419615.jpg   \n",
       "\n",
       "                                                                                        caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .                      \n",
       "1  A black dog and a spotted dog are fighting                                                    \n",
       "2  A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .  \n",
       "3  A man lays on a bench while his dog sits by him .                                             \n",
       "4  A man in an orange hat starring at something .                                                "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregateion by filename\n",
    "df_agg = captions_df.groupby(\"image\").first().reset_index()\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get caption labels\n",
    "sp = spacy.load('en_core_web_sm') # english language model\n",
    "sp_cap = sp(df_agg.caption[1])\n",
    "print(sp_cap)\n",
    "for idx, token in enumerate(sp_cap):\n",
    "    if idx == 0:\n",
    "        print(f\"{'Word pos.':^10} {'Text':<15} {'POS-Tag':^10} {'POS-Tag expl.'}\")\n",
    "    print(f\"{idx+1:^10} {token.text:<15} {token.tag_:^10} {token.pos_:<5} - {spacy.explain(token.tag_)}\")\n",
    "\n",
    "cap_cleaned = \" \".join(set([token.text for token in sp_cap if token.tag_ in [\"ADJ\", \"AUX\", \"JJ\", \"NN\", \"NNS\", \"VB\", \"VBG\", \"VBN\", \"VBZ\"]]))\n",
    "cap_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caption_short\"] = df.caption.map(lambda x: \" \".join(set([token.text for token in sp(x) if token.tag_ in [\"ADJ\", \"AUX\", \"JJ\", \"NN\", \"NNS\", \"VB\", \"VBG\", \"VBN\", \"VBZ\"]])))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_short</th>\n",
       "      <th>caption_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "      <td>set climbing dress child entry pink stairs is way</td>\n",
       "      <td>set climbing dress child entry pink stair is way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>wooden building going girl</td>\n",
       "      <td>wooden building going girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>little climbing wooden playhouse girl</td>\n",
       "      <td>little climbing wooden playhouse girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playhouse .</td>\n",
       "      <td>little climbing playhouse stairs girl</td>\n",
       "      <td>little climbing playhouse stair girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a wooden cabin .</td>\n",
       "      <td>little wooden dress pink girl cabin going</td>\n",
       "      <td>little wooden dress pink girl cabin going</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \\\n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .   \n",
       "1  A girl going into a wooden building .                                      \n",
       "2  A little girl climbing into a wooden playhouse .                           \n",
       "3  A little girl climbing the stairs to her playhouse .                       \n",
       "4  A little girl in a pink dress going into a wooden cabin .                  \n",
       "\n",
       "                                       caption_short  \\\n",
       "0  set climbing dress child entry pink stairs is way   \n",
       "1  wooden building going girl                          \n",
       "2  little climbing wooden playhouse girl               \n",
       "3  little climbing playhouse stairs girl               \n",
       "4  little wooden dress pink girl cabin going           \n",
       "\n",
       "                                       caption_lemm  \n",
       "0  set climbing dress child entry pink stair is way  \n",
       "1  wooden building going girl                        \n",
       "2  little climbing wooden playhouse girl             \n",
       "3  little climbing playhouse stair girl              \n",
       "4  little wooden dress pink girl cabin going         "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"caption_lemm\"] = df.apply(lambda r: get_lemma_text(r[\"caption_short\"]), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "df_train, df_test = skms.train_test_split(df, test_size=.2, random_state=42)\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "# Def feature and target column\n",
    "feat_col = \"caption_lemm\"\n",
    "\n",
    "# Build up feature and target structure\n",
    "X_train = df_train[feat_col]\n",
    "X_test = df_test[feat_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature matrix shape:    (32364, 5925)\n",
      "Test feature matrix shape:     (8091, 5925)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize captions\n",
    "vectorizer = skfet.TfidfVectorizer()\n",
    "\n",
    "# Train it\n",
    "X_train_CountVec = vectorizer.fit_transform(X_train)\n",
    "print(f\"{'Train feature matrix shape:':<30} {X_train_CountVec.shape}\")\n",
    "\n",
    "# Transform test features to tokens\n",
    "X_test_CountVec = vectorizer.transform(X_test)\n",
    "print(f\"{'Test feature matrix shape:':<30} {X_test_CountVec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1ChqgNRK0py"
   },
   "source": [
    " # Create ImageGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline using tf.data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((273876, 33), (1000, 27))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(DP)\n",
    "\n",
    "if DO_TRAIN_VALID_SPLIT:\n",
    "    df_train, df_valid = skms.train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "else:\n",
    "    df_train = df\n",
    "    df_valid = df_test\n",
    "\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function parse_function at 0x000001B19E3EE3A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_function at 0x000001B19E3EE3A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "273876 training datasets, using 19 classes\n",
      "1000 validation datasets, unsing 19 classes\n",
      "1000 training datasets, using 19 classes\n"
     ]
    }
   ],
   "source": [
    "#tf.autograph.set_verbosity(3, True)\n",
    "\n",
    "if DP == DP_IMGGEN:\n",
    "    datagen = ImageDataGenerator(rescale=1 / 255.)#, validation_split=0.1)\n",
    "\n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"genre_ids2_list\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=SEED,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(299, 299),\n",
    "        subset='training',\n",
    "        validate_filenames=True\n",
    "    )\n",
    "\n",
    "    valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_valid,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"genre_ids2_list\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=SEED,\n",
    "        shuffle=False,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(299, 299),\n",
    "        subset='training',\n",
    "        validate_filenames=True\n",
    "    )\n",
    "\n",
    "    test_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"genre_ids2_list\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=SEED,\n",
    "        shuffle=False,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(299, 299),\n",
    "        subset='training',\n",
    "        validate_filenames=True\n",
    "    )\n",
    "else:\n",
    "    X_train = df_train.filename.to_numpy()\n",
    "    y_train = df_train[LABEL_COLS].to_numpy()\n",
    "    X_valid = df_valid.filename.to_numpy()\n",
    "    y_valid = df_valid[LABEL_COLS].to_numpy()\n",
    "    X_test = df_test.filename.to_numpy()\n",
    "    y_test = df_test[LABEL_COLS].to_numpy()\n",
    "\n",
    "    train_generator = create_dataset(X_train, y_train, cache=True)\n",
    "    valid_generator = create_dataset(X_valid, y_valid, cache=True)\n",
    "    test_generator = create_dataset(X_test, y_test, cache=True)\n",
    "\n",
    "    print(f\"{len(X_train)} training datasets, using {y_train.shape[1]} classes\")\n",
    "    print(f\"{len(X_valid)} validation datasets, unsing {y_valid.shape[1]} classes\")\n",
    "    print(f\"{len(X_test)} training datasets, using {y_test.shape[1]} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Label distributions'}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show label distribution\n",
    "df_tmp = df = pd.DataFrame(\n",
    "    {\n",
    "        'train': df_train[LABEL_COLS].sum()/len(df_train), \n",
    "        'valid': df_valid[LABEL_COLS].sum()/len(df_valid), \n",
    "        'test': df_test[LABEL_COLS].sum()/len(df_test)\n",
    "    }, \n",
    "    index=LABEL_COLS\n",
    ")\n",
    "df_tmp.sort_values('train', ascending=False).plot.bar(figsize=(14,6), title='Label distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 273876 entries, 0 to 274854\n",
      "Data columns (total 33 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   id               273876 non-null  int64         \n",
      " 1   original_title   273876 non-null  object        \n",
      " 2   release_date     273876 non-null  datetime64[ns]\n",
      " 3   popularity       273876 non-null  float64       \n",
      " 4   adult            273876 non-null  bool          \n",
      " 5   video            273876 non-null  bool          \n",
      " 6   poster_url       273876 non-null  object        \n",
      " 7   poster_exists    273876 non-null  bool          \n",
      " 8   filename         273876 non-null  object        \n",
      " 9   genre_id         273876 non-null  object        \n",
      " 10  genre_ids2       273876 non-null  object        \n",
      " 11  genre_ids2_list  273876 non-null  object        \n",
      " 12  genre_id_count   273876 non-null  int64         \n",
      " 13  Action           273876 non-null  int64         \n",
      " 14  Adventure        273876 non-null  int64         \n",
      " 15  Animation        273876 non-null  int64         \n",
      " 16  Comedy           273876 non-null  int64         \n",
      " 17  Crime            273876 non-null  int64         \n",
      " 18  Documentary      273876 non-null  int64         \n",
      " 19  Drama            273876 non-null  int64         \n",
      " 20  Family           273876 non-null  int64         \n",
      " 21  Fantasy          273876 non-null  int64         \n",
      " 22  History          273876 non-null  int64         \n",
      " 23  Horror           273876 non-null  int64         \n",
      " 24  Music            273876 non-null  int64         \n",
      " 25  Mystery          273876 non-null  int64         \n",
      " 26  Romance          273876 non-null  int64         \n",
      " 27  Science Fiction  273876 non-null  int64         \n",
      " 28  TV Movie         273876 non-null  int64         \n",
      " 29  Thriller         273876 non-null  int64         \n",
      " 30  War              273876 non-null  int64         \n",
      " 31  Western          273876 non-null  int64         \n",
      " 32  genre_names      273876 non-null  object        \n",
      "dtypes: bool(3), datetime64[ns](1), float64(1), int64(21), object(7)\n",
      "memory usage: 65.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Action</th>\n",
       "      <td>0.978046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adventure</th>\n",
       "      <td>1.821594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation</th>\n",
       "      <td>1.265968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comedy</th>\n",
       "      <td>0.359334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>1.400726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Documentary</th>\n",
       "      <td>0.472618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drama</th>\n",
       "      <td>0.237221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family</th>\n",
       "      <td>1.654197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fantasy</th>\n",
       "      <td>2.417686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>3.348684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horror</th>\n",
       "      <td>1.111147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>1.194060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mystery</th>\n",
       "      <td>2.585711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>0.893817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science Fiction</th>\n",
       "      <td>2.303611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TV Movie</th>\n",
       "      <td>2.510547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thriller</th>\n",
       "      <td>0.999749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>War</th>\n",
       "      <td>4.386322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western</th>\n",
       "      <td>5.261271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   weight\n",
       "Action           0.978046\n",
       "Adventure        1.821594\n",
       "Animation        1.265968\n",
       "Comedy           0.359334\n",
       "Crime            1.400726\n",
       "Documentary      0.472618\n",
       "Drama            0.237221\n",
       "Family           1.654197\n",
       "Fantasy          2.417686\n",
       "History          3.348684\n",
       "Horror           1.111147\n",
       "Music            1.194060\n",
       "Mystery          2.585711\n",
       "Romance          0.893817\n",
       "Science Fiction  2.303611\n",
       "TV Movie         2.510547\n",
       "Thriller         0.999749\n",
       "War              4.386322\n",
       "Western          5.261271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "#In order to calculate the class weight do the following\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  LABEL_COLS, # np.array(list(train_generator.class_indices.keys()),dtype=\"int\"), \n",
    "                                                  np.array(df_train.genre_names.explode()))\n",
    "\n",
    "class_weights = dict(zip(list(range(len(class_weights))), class_weights))\n",
    "number_of_classes = len(LABEL_COLS)\n",
    "pd.DataFrame({'weight': [i[1] for i in class_weights.items()]}, index=[LABEL_COLS[i[0]] for i in class_weights.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create(model_name: str):\n",
    "    \"\"\"Create the customized InceptionV3 model\"\"\"\n",
    "    base_inc_res = tf.keras.applications.InceptionV3(\n",
    "        include_top=False, \n",
    "        weights='imagenet',\n",
    "        input_shape=(299,299,3)\n",
    "    )\n",
    "    base_inc_res.trainable = False\n",
    "\n",
    "    inputs = keras.Input(shape=(299,299,3))\n",
    "    x = base_inc_res(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Dense(19, activation='sigmoid')(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x, name=model_name)\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 6s 0us/step\n",
      "Model: \"InceptionV3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 8, 8, 2048)        8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8, 8, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8, 8, 512)         524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8, 8, 19)          9747      \n",
      "=================================================================\n",
      "Total params: 24,443,699\n",
      "Trainable params: 2,636,819\n",
      "Non-trainable params: 21,806,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_create(MODEL_NAME)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXBrisCLgl2"
   },
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3'], variable_device = '/device:CPU:0'\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Runtime Context: 2-GPU_MirroredStrategy\n",
      "Loading model from file InceptionResNetV2_corrected_20210323T223046Z.hd5...\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "model InceptionResNetV2_Customized loaded in 3m 24s!\n",
      "15\n",
      "stopping at 10\n",
      "create callbacks\n",
      "Model: \"InceptionResNetV2_Customized\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_6 (TFOpLambda)  (None, 299, 299, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_7 (TFOpLambda)  (None, 299, 299, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_6 (TFOpLambda) (None, 299, 299, 3)  0           tf.math.truediv_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_7 (TFOpLambda) (None, 299, 299, 3)  0           tf.math.truediv_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_resnet_v2 (Functional (None, 1536)         54336736    tf.math.subtract_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "xception (Functional)           (None, 2048)         20861480    tf.math.subtract_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_834 (BatchN (None, 1536)         6144        inception_resnet_v2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_835 (BatchN (None, 2048)         8192        xception[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 3584)         0           batch_normalization_834[0][0]    \n",
      "                                                                 batch_normalization_835[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         3671040     tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          524800      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 19)           9747        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 79,418,139\n",
      "Trainable params: 4,205,587\n",
      "Non-trainable params: 75,212,552\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fit\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\A291127E01\\.conda\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "   1/1070 [..............................] - ETA: 0s - loss: 0.4261 - accuracy: 0.0664 - categorical_accuracy: 0.0664 - auc: 0.4568 - f1_micro: 0.1387 - f1_macro: 0.0913 - f1_score_weighted: 0.2409WARNING:tensorflow:From C:\\Users\\A291127E01\\.conda\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  61/1070 [>.............................] - ETA: 1:28:53 - loss: 0.2441 - accuracy: 0.1795 - categorical_accuracy: 0.1795 - auc: 0.5134 - f1_micro: 0.0228 - f1_macro: 0.0146 - f1_score_weighted: 0.0228"
     ]
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "l_rtc_names = [\n",
    "    \"2-GPU_MirroredStrategy\",\n",
    "    \"2-GPU_CentralStorageStrategy\",\n",
    "    \"1-GPU\",\n",
    "    \"56_CPU\",\n",
    "    \"2-GPU_MirroredStrategy_NCCL-All-Reduced\",\n",
    "]\n",
    "l_rtc = [\n",
    "    tf.distribute.MirroredStrategy().scope(),\n",
    "    tf.distribute.experimental.CentralStorageStrategy().scope(),\n",
    "    tf.device(\"/GPU:0\"),\n",
    "    tf.device(\"/CPU:0\"),\n",
    "    tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce()).scope(),\n",
    "]\n",
    "\n",
    "# Load Model\n",
    "i = 0\n",
    "runtime_context = l_rtc[i]\n",
    "######for i, runtime_context in enumerate(l_rtc):\n",
    "print(f\"Runtime Context: {l_rtc_names[i]}\")\n",
    "\n",
    "# Create and train model\n",
    "with runtime_context:\n",
    "    model_name = MODEL_NAME\n",
    "    model = create_model(model_name)\n",
    "\n",
    "    # Start time measurement\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    # Define Tensorflow callback log-entry\n",
    "    model_name_full = f\"{model.name}_{l_rtc_names[i]}_{dt.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tb_logdir = f\"{TENSORBOARD_LOGDIR}{model_name_full}\"\n",
    "    # mark loaded layers as not trainable\n",
    "    # except last layer\n",
    "    leng = len(model.layers)\n",
    "    print(leng)\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if leng-i == 5:\n",
    "          print(\"stopping at\",i)\n",
    "          break\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Def metrics\n",
    "    threshold = 0.5\n",
    "    f1_micro = tfa.metrics.F1Score(num_classes=19, average='micro', name='f1_micro',threshold=threshold), \n",
    "    f1_macro = tfa.metrics.F1Score(num_classes=19, average='macro', name='f1_macro',threshold=threshold)\n",
    "    f1_weighted = tfa.metrics.F1Score(num_classes=19,  average='weighted', name='f1_score_weighted',threshold=threshold)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\", \n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            \"categorical_accuracy\",\n",
    "            tf.keras.metrics.AUC(multi_label = True),#,label_weights=class_weights),\n",
    "            f1_micro,\n",
    "            f1_macro,\n",
    "            f1_weighted]\n",
    "    )\n",
    "\n",
    "    print(\"create callbacks\")\n",
    "    #filepath = \"model_checkpoints/{model_name}_saved-model-{epoch:02d}-{val_f1_score_weighted:.2f}.hdf5\"\n",
    "    #cb_checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score_weighted', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    cb_tensorboard = TensorBoard(\n",
    "        log_dir = tb_logdir,\n",
    "        histogram_freq=0, \n",
    "        update_freq='epoch',\n",
    "        write_graph=True, \n",
    "        write_images=False)\n",
    "    #callbacks_list = [cb_checkpoint, cb_tensorboard]\n",
    "    #callbacks_list = [cb_checkpoint]\n",
    "    callbacks_list = [cb_tensorboard]\n",
    "\n",
    "    # Model summary\n",
    "    print(model.summary())\n",
    "\n",
    "    # Train model\n",
    "    print(\"model fit\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        # reduce steps per epochs for faster epochs\n",
    "        #steps_per_epoch = math.ceil(266957 / BATCH_SIZE /8),\n",
    "        class_weight = class_weights,\n",
    "        callbacks=callbacks_list,\n",
    "        use_multiprocessing=False\n",
    "    )\n",
    "\n",
    "    # Measure time of loop\n",
    "    toc = time.perf_counter()\n",
    "    secs_all = toc - tic\n",
    "    mins = int(secs_all / 60)\n",
    "    secs = int((secs_all - mins*60))\n",
    "    print(f\"Time spend for current run: {secs_all:0.4f} seconds => {mins}m {secs}s\")\n",
    "\n",
    "    # Predict testset\n",
    "    y_pred_test = model.predict(test_generator)\n",
    "\n",
    "    # Store resulting model\n",
    "    try:\n",
    "        fpath = MODEL_DIR + model_name_full\n",
    "        print(f\"Saving final model to file {fpath}\")\n",
    "        model.save(fpath)\n",
    "    except Exception as e:\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(f\"Error during saving of final model\\n{e}\")\n",
    "        print(\"-------------------------------------------\\n\")\n",
    "\n",
    "    try:\n",
    "        fpath = MODEL_DIR + model_name_full + \".ckpt\"\n",
    "        print(f\"Saving final model weights to file {fpath}]\")\n",
    "        model.save_weights(fpath)\n",
    "    except Exception as e:\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(f\"Error during saving of final model weights\\n{e}\")\n",
    "        print(\"-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "threshold = 0.35\n",
    "f1_micro = tfa.metrics.F1Score(num_classes=19, average='micro', name='f1_micro',threshold=threshold), \n",
    "f1_macro = tfa.metrics.F1Score(num_classes=19, average='macro', name='f1_macro',threshold=threshold)\n",
    "f1_weighted = tfa.metrics.F1Score(num_classes=19,  average='weighted', name='f1_score_weighted',threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = [ [1 if i in e else 0 for i in range(19)] for e in test_generator.labels]\n",
    "y_true_test = np.array(y_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ths = np.linspace(0.1, 0.5, 10)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'threshold': ths, \n",
    "    'f1-micro': [f1_score(y_true_test, (y_pred_test > th)*1., average=\"micro\") for th in ths],\n",
    "    'f1-weighted': [f1_score(y_true_test, (y_pred_test > th)*1., average=\"weighted\") for th in ths],\n",
    "    'class' : \"all\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ths = np.linspace(0.1, 0.5, 9)\n",
    "\n",
    "df_ths = pd.DataFrame({'threshold' : ths}\n",
    ")\n",
    "\n",
    "for cl in range(19):\n",
    "    col = pd.DataFrame({f'f1-class_{cl}': [f1_score(y_true_test[:,cl], (y_pred_test[:,cl] > th)*1.) for th in ths] \n",
    "                       })\n",
    "    df_ths=pd.concat([df_ths,col],axis=\"columns\")\n",
    "\n",
    "df_ths.style.highlight_max(color = 'lightgreen', axis = 0)\n",
    "df_ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_index=df_ths.iloc[:,1:].idxmax(axis=0)\n",
    "class_thresholds = df_ths.threshold[argmax_index].values\n",
    "class_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_opt_th = f1_score(y_true_test, (y_pred_test > class_thresholds)*1., average=\"micro\")\n",
    "f1_weighted_opt_th = f1_score(y_true_test, (y_pred_test > class_thresholds)*1., average=\"weighted\")\n",
    "print(\"Class thresholds optimized on test set:\",\n",
    "        f\"f1_micro_opt_th: {f1_micro_opt_th:.3f}, f1_weighted_opt_th: {f1_weighted_opt_th:.3f}\",\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datagen = ImageDataGenerator(rescale=1 / 255.)#, validation_split=0.1)\n",
    "BATCH_SIZE = 64\n",
    "train2_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df.loc[~df.is_holdout].sample(20000),\n",
    "    directory=IMAGES_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"genre_id\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",    \n",
    "    target_size=(299, 299),\n",
    "    subset='training',\n",
    "    validate_filenames=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(train2_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = [ [1 if i in e else 0 for i in range(19)] for e in train2_generator.labels]\n",
    "y_true_train = np.array(y_true_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ths = np.linspace(0.1, 0.5, 9)\n",
    "\n",
    "df_ths = pd.DataFrame({'threshold' : ths}\n",
    ")\n",
    "\n",
    "for cl in range(19):\n",
    "    col = pd.DataFrame({f'f1-class_{cl}': [f1_score(y_true_train[:,cl], (y_pred_train[:,cl] > th)*1.) for th in ths]          \n",
    "                       })\n",
    "    df_ths=pd.concat([df_ths,col],axis=\"columns\")\n",
    "\n",
    "df_ths.style.highlight_max(color = 'lightgreen', axis = 0)\n",
    "df_ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_index=df_ths.iloc[:,1:].idxmax(axis=0)\n",
    "class_thresholds = df_ths.threshold[argmax_index].values\n",
    "class_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_opt_th = f1_score(y_true, (y_pred > class_thresholds)*1., average=\"micro\")\n",
    "f1_weighted_opt_th = f1_score(y_true, (y_pred > class_thresholds)*1., average=\"weighted\")\n",
    "print(\"Class thresholds optimized on training set:\",\n",
    "        f\"f1_micro_opt_th: {f1_micro_opt_th:.3f}, f1_weighted_opt_th: {f1_weighted_opt_th:.3f}\",\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f45937856810>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>popularity</th>\n",
       "      <th>adult</th>\n",
       "      <th>video</th>\n",
       "      <th>poster_url</th>\n",
       "      <th>poster_exists</th>\n",
       "      <th>filename</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>...</th>\n",
       "      <th>History</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Music</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>TV Movie</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30228</th>\n",
       "      <td>752866</td>\n",
       "      <td>Kama Wosi: Music in the Trobriand Islands</td>\n",
       "      <td>1971-03-10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w500//bdmkZk1flJD7IFP5We4eyZiKhGm.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>bdmkZk1flJD7IFP5We4eyZiKhGm.jpg</td>\n",
       "      <td>[99]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170485</th>\n",
       "      <td>479935</td>\n",
       "      <td>The Cumbrian Coast</td>\n",
       "      <td>1997-12-01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w500//9Fzus94L0lSwNWUKdfKPw8K0y3y.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>9Fzus94L0lSwNWUKdfKPw8K0y3y.jpg</td>\n",
       "      <td>[99]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                             original_title release_date  \\\n",
       "30228   752866  Kama Wosi: Music in the Trobriand Islands   1971-03-10   \n",
       "170485  479935                         The Cumbrian Coast   1997-12-01   \n",
       "\n",
       "        popularity  adult  video  \\\n",
       "30228          0.6  False  False   \n",
       "170485         0.6  False  False   \n",
       "\n",
       "                                                                  poster_url  \\\n",
       "30228   https://www.themoviedb.org/t/p/w500//bdmkZk1flJD7IFP5We4eyZiKhGm.jpg   \n",
       "170485  https://www.themoviedb.org/t/p/w500//9Fzus94L0lSwNWUKdfKPw8K0y3y.jpg   \n",
       "\n",
       "        poster_exists                         filename genre_id  ... History  \\\n",
       "30228            True  bdmkZk1flJD7IFP5We4eyZiKhGm.jpg     [99]  ...       0   \n",
       "170485           True  9Fzus94L0lSwNWUKdfKPw8K0y3y.jpg     [99]  ...       0   \n",
       "\n",
       "       Horror  Music  Mystery  Romance  Science Fiction  TV Movie  Thriller  \\\n",
       "30228       0      0        0        0                0         0         0   \n",
       "170485      0      0        0        0                0         0         0   \n",
       "\n",
       "        War  Western  \n",
       "30228     0        0  \n",
       "170485    0        0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.original_title.str.contains(\"brian\")==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
