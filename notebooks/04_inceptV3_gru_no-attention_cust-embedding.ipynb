{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation of the Baseline Model\n",
    "Different to the Baseline are the usage of GRU layers instead of LSTM layers on the Decoder-Part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: nltk in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (3.5)\n",
      "Requirement already satisfied: numpy in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (1.19.2)\n",
      "Requirement already satisfied: absl-py in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (0.12.0)\n",
      "Requirement already satisfied: click in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (2020.7.14)\n",
      "Requirement already satisfied: joblib in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (0.15.1)\n",
      "Requirement already satisfied: tqdm in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os  # operating system access\n",
    "import sys  # system specific operations\n",
    "import datetime as dt  # datetime handling\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split  # data preperation\n",
    "\n",
    "import tensorflow as tf  # tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_addons as tfa\n",
    "import tensorboard as tb\n",
    "\n",
    "import tqdm  # progress bars\n",
    "import json\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk.translate.bleu_score as ntbs  # BLEU scoring basics\n",
    "import rouge_score.rouge_scorer as rouge_scorer  # ROUGE scoring basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: das Verzeichnis »../models/InceptV3_GRU_NoAttention_CustEmbedding_768-units-all“ kann nicht angelegt werden: Die Datei existiert bereits\n",
      "mkdir: das Verzeichnis »../data/results/InceptV3_GRU_NoAttention_CustEmbedding_768-units-all“ kann nicht angelegt werden: Die Datei existiert bereits\n"
     ]
    }
   ],
   "source": [
    "# Runtime config\n",
    "RUN_ON_KAGGLE = False\n",
    "EPOCHS = 50\n",
    "RUN_TRAINING = False  # Train the model during run\n",
    "USE_CONFIG_TIMESTAMP = True  # Use below defined timestamp FN_TIMESTAMP if missing\n",
    "SAVE_MODEL = False  # Write model to hd5 file\n",
    "LOG_MODEL = False  # Write tensorboard and model history logs during training\n",
    "RUN_DATA_CHECK = False  # Do a basic check on the input data\n",
    "\n",
    "# Model config\n",
    "# Config 1\n",
    "# MODEL_NAME = 'InceptV3_GRU_NoAttention_CustEmbedding'\n",
    "# BATCH_SIZE = 64\n",
    "# EARLY_STOPPING_PATIENT = 5\n",
    "# EMBEDDING_DIM = 256\n",
    "# UNITS = 512\n",
    "\n",
    "# Config 2 (20210429-172359)\n",
    "MODEL_NAME = 'InceptV3_GRU_NoAttention_CustEmbedding_768-units-all'\n",
    "BATCH_SIZE = 64\n",
    "EARLY_STOPPING_PATIENT = 5\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512 + 256\n",
    "\n",
    "\n",
    "# GPU/CPU config\n",
    "MEMORY_OF_GPU = 6000  # MBytes\n",
    "\n",
    "# Tensorboard view config\n",
    "TENSORBOARD_SHOW_INLINE = False\n",
    "# Launch tensorboard\n",
    "# True: From that notebook, False: connect to already running tensorboard\n",
    "TENSORBOARD_LAUNCH = False\n",
    "\n",
    "# Directories and filenames\n",
    "if RUN_ON_KAGGLE:\n",
    "    FP_CAPTIONS = '../input/flickr8k/captions.txt'\n",
    "    DIR_IMAGES = '../input/flickr8k/Images/'\n",
    "    DIR_IMAGE_FEATURES = '../input/aida-image-captioning/Images/'\n",
    "    DIR_MODEL_STORE = './models/'\n",
    "    DIR_MODEL_LOG = './models/'\n",
    "    DIR_RESULT_STORE = './results/'\n",
    "    DIR_TENSORBOARD_LOG = './tensorboard/'\n",
    "else:\n",
    "    FP_CAPTIONS = '../data/raw/flickr8k/captions.txt'\n",
    "    DIR_IMAGES = '../data/raw/flickr8k/Images/'\n",
    "    DIR_IMAGE_FEATURES = '../data/interim/aida-image-captioning/Images/'\n",
    "    DIR_MODEL_STORE = f'../models/{MODEL_NAME}/'\n",
    "    DIR_MODEL_LOG = f'../models/logs/{MODEL_NAME}/'\n",
    "    DIR_RESULT_STORE = f'../data/results/{MODEL_NAME}/'\n",
    "    DIR_TENSORBOARD_LOG = './tensorboard_logs/scalars/'\n",
    "\n",
    "# Misc config\n",
    "if RUN_ON_KAGGLE:\n",
    "    # Create specified directories\n",
    "    !mkdir results\n",
    "    !mkdir models    \n",
    "    !mkdir tensorboard\n",
    "    !mkdir models/$MODEL_NAME\n",
    "\n",
    "    sys.path.insert(0, '../input/aida-metrics-functions')  # custom metrics implementation\n",
    "    import metrics as met  # custom metrics BLUE-1 pecsision and ROUGE-L recall\n",
    "else:\n",
    "    !mkdir ../models/$MODEL_NAME\n",
    "    !mkdir ../data/results/$MODEL_NAME\n",
    "    import src.models.metrics as met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "Runtime Context: CPUs\n",
      "Recommended Batch Size: 64 datasets\n"
     ]
    }
   ],
   "source": [
    "timestamp_str = None\n",
    "FN_TIMESTAMP = \"20210429-173000\"\n",
    "FN_MODEL2LOAD = f\"{MODEL_NAME}_{FN_TIMESTAMP}.h5\"\n",
    "FN_HIST2LOAD = f\"{MODEL_NAME}_{FN_TIMESTAMP}.pickle\"\n",
    "\n",
    "# To get access to a GPU instance you can use the `change runtime type` and set the option to `GPU` from the `Runtime` tab  in the notebook\n",
    "# Checking the GPU availability for the notebook\n",
    "#tf.test.gpu_device_name()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Create virtual GPUs\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK\n",
    "            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],\n",
    "        )\n",
    "\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK \n",
    "            gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],            \n",
    "        )\n",
    "    except:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(\"Warning: During GPU handling.\")\n",
    "        pass\n",
    "    finally:\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\\n\")\n",
    "\n",
    "# Set runtime context and batch size\n",
    "l_rtc_names = [\n",
    "    \"multi-GPU_MirroredStrategy\",\n",
    "    \"multi-GPU_CentralStorageStrategy\",\n",
    "    \"1-GPU\",\n",
    "    \"CPUs\",\n",
    "    \"multi-GPU_MirroredStrategy_NCCL-All-Reduced\",\n",
    "]\n",
    "l_rtc = [\n",
    "    tf.distribute.MirroredStrategy().scope(),\n",
    "    tf.distribute.experimental.CentralStorageStrategy().scope(),\n",
    "    tf.device(\"/GPU:0\"),\n",
    "    tf.device(\"/CPU:0\"),\n",
    "    tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce()).scope(),\n",
    "]\n",
    "if len(gpus) == 0:\n",
    "    rtc_idx = 3\n",
    "    batch_size = 64\n",
    "\n",
    "elif len(gpus) == 1:\n",
    "    rtc_idx = 2\n",
    "    batch_size = 4*256\n",
    "\n",
    "elif len(gpus) > 1:\n",
    "    rtc_idx = 0\n",
    "    batch_size = 8*256\n",
    "\n",
    "runtime_context = l_rtc[rtc_idx]\n",
    "\n",
    "print(f\"\\nRuntime Context: {l_rtc_names[rtc_idx]}\")\n",
    "print(f\"Recommended Batch Size: {batch_size} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_text(texts):\n",
    "    \"\"\"\n",
    "    - Adds <start> to the beginning of texts to the inputs\n",
    "    - Adds <end> to the end of texts\n",
    "    Input: texts: list of sentences\n",
    "    Output: - tokenized, padded text with <start> token for using as input in decoder\n",
    "            - tokenized, padded text with <end> token for using as target in decoder\n",
    "            - tokenizer\n",
    "            - max_seq_len\n",
    "    \"\"\"\n",
    "\n",
    "    print('Creating and fitting tokenizer...')\n",
    "    tokenizer_fit_data = ['<start> ' + s + ' <end>' for s in texts]\n",
    "\n",
    "    # create tokenizer for captions + <start> and <end> tags\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(tokenizer_fit_data)\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    print('Creating and fitting tokenizer done')\n",
    "\n",
    "    print('Create text input sequences...')\n",
    "    # add <start> tag before every caption and tokenize + pad ('post')\n",
    "    enc_input_data = ['<start> ' + s for s in texts]\n",
    "    enc_input_token_seqs = tokenizer.texts_to_sequences(enc_input_data)\n",
    "    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n",
    "    enc_input_prep = pad_sequences(enc_input_token_seqs, padding='post')\n",
    "    print('Create text input sequences done')\n",
    "\n",
    "    print('Create text target sequences...')\n",
    "    # add <start> tag before every caption and tokenize + pad ('post')\n",
    "    enc_target_data = [s + ' <end>' for s in texts]\n",
    "    enc_target_token_seqs = tokenizer.texts_to_sequences(enc_target_data)\n",
    "    # if no \"maxlen\" defined then pad_sequences takes the longest sequence as max_len\n",
    "    enc_target_prep = pad_sequences(enc_target_token_seqs, padding='post')\n",
    "    print('Create text target sequences done')\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # show some texts processed\n",
    "    for i  in range(2):\n",
    "        print(f'               {i} orig: {texts[i]}')\n",
    "        print(f'              {i} input: {enc_input_data[i]}')\n",
    "        print(f' {i} preprocessed input: {enc_input_prep[i]}')\n",
    "        print(f'             {i} target: {enc_target_data[i]}')\n",
    "        print(f'{i} preprocessed target: {enc_target_prep[i]}')\n",
    "        print()\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print(f' preprocessed input shape: {enc_input_prep.shape}')\n",
    "    print(f'preprocessed target shape: {enc_target_prep.shape}')\n",
    "\n",
    "    return enc_input_prep, enc_target_prep, tokenizer, enc_input_prep.shape[-1]  # last dim is the length of the sequences        \n",
    "\n",
    "\n",
    "class History_trained_model(object):\n",
    "    \"\"\"Helper class for saving training history\"\"\"\n",
    "    def __init__(self, history, epoch, params):\n",
    "        self.history = history\n",
    "        self.epoch = epoch\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "def save_hist(path, history):\n",
    "    \"\"\"Saves the training history to file\"\"\"\n",
    "    with open(path, 'wb') as file:\n",
    "        model_history = History_trained_model(history.history, history.epoch, history.params)\n",
    "        pickle.dump(model_history, file, pickle.HIGHEST_PROTOCOL)        \n",
    "\n",
    "\n",
    "def load_hist(path):\n",
    "    \"\"\"Loads the training history from file\"\"\"\n",
    "    with open(path, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "    return history\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "  \"\"\"\n",
    "  Learning rate scheduler for keras callbacks\n",
    "  Returns a custom learning rate that decreases as epochs progress.\n",
    "  \"\"\"\n",
    "  tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
    "  return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Picture captions\n",
    "df_cap = pd.read_csv(FP_CAPTIONS)\n",
    "display(df_cap.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DATA_CHECK:\n",
    "    # Number of captions per image\n",
    "    print(df_cap.groupby('image').agg(cap_count=('caption','count')).describe())\n",
    "\n",
    "    # Test if images are there for every entry\n",
    "    img_file_not_exist = []\n",
    "\n",
    "    df = df_cap['image'].unique()\n",
    "    for image in tqdm.tqdm_notebook(df, total=len(df)):\n",
    "        if not os.path.exists(os.path.join(DIR_IMAGES, image)):\n",
    "            img_file_not_exist.append(image)\n",
    "\n",
    "    print(f'Number of image files not exists in \"{DIR_IMAGES}\" directory: {len(img_file_not_exist)}')\n",
    "\n",
    "    # test if images features are there for every entry\n",
    "    img_file_not_exist = []\n",
    "    for image in tqdm.tqdm_notebook(df_cap['image'].unique()):\n",
    "        if not os.path.exists(os.path.join(DIR_IMAGE_FEATURES, image.replace('.jpg', '.npy'))):\n",
    "            img_file_not_exist.append(image)\n",
    "\n",
    "    print(f'Number of image feature files not exists in \"{DIR_IMAGE_FEATURES}\" directory: {len(img_file_not_exist)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and fitting tokenizer...\n",
      "Creating and fitting tokenizer done\n",
      "Create text input sequences...\n",
      "Create text input sequences done\n",
      "Create text target sequences...\n",
      "Create text target sequences done\n",
      "\n",
      "\n",
      "\n",
      "               0 orig: A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "              0 input: <start> A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      " 0 preprocessed input: [   2    1   43    4    1   90  172    7  119   51    1  393   12  394\n",
      "    4   28 5159  670    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "             0 target: A child in a pink dress is climbing up a set of stairs in an entry way . <end>\n",
      "0 preprocessed target: [   1   43    4    1   90  172    7  119   51    1  393   12  394    4\n",
      "   28 5159  670    3    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "               1 orig: A girl going into a wooden building .\n",
      "              1 input: <start> A girl going into a wooden building .\n",
      " 1 preprocessed input: [  2   1  19 316  64   1 196 117   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "             1 target: A girl going into a wooden building . <end>\n",
      "1 preprocessed target: [  1  19 316  64   1 196 117   3   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " preprocessed input shape: (40455, 38)\n",
      "preprocessed target shape: (40455, 38)\n"
     ]
    }
   ],
   "source": [
    "enc_input_prep, enc_target_prep, tokenizer, max_seq_len = preproc_text(df_cap['caption'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>is_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  is_test  \n",
       "0  A child in a pink dress is climbing up a set o...    False  \n",
       "1              A girl going into a wooden building .    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((32360,),\n",
       " array(['../data/interim/aida-image-captioning/Images/1000268201_693b08cb0e.npy',\n",
       "        '../data/interim/aida-image-captioning/Images/1000268201_693b08cb0e.npy'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 80,20 % split on the images\n",
    "_, test_img_names = train_test_split(df_cap['image'].unique(), test_size=0.2, random_state=42)\n",
    "df_cap['is_test'] = df_cap['image'].isin(test_img_names)\n",
    "display(df_cap.head(2))\n",
    "\n",
    "# Trainset\n",
    "train_enc_input_prep = enc_input_prep[df_cap[~df_cap['is_test']].index]\n",
    "train_enc_target_prep = enc_target_prep[df_cap[~df_cap['is_test']].index]\n",
    "train_image_feature_names = df_cap[~df_cap['is_test']]['image'].map(lambda x: os.path.join(DIR_IMAGE_FEATURES, x.replace('.jpg', '.npy'))).values\n",
    "\n",
    "# Testset\n",
    "test_enc_input_prep = enc_input_prep[df_cap[df_cap['is_test']].index]\n",
    "test_enc_target_prep = enc_target_prep[df_cap[df_cap['is_test']].index]\n",
    "test_image_feature_names = df_cap[df_cap['is_test']]['image'].map(lambda x: os.path.join(DIR_IMAGE_FEATURES,x.replace('.jpg', '.npy'))).values\n",
    "\n",
    "# show train_image_features\n",
    "train_image_feature_names.shape, train_image_feature_names[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom class for providing the data to the model\"\"\"\n",
    "    def __init__(self, \n",
    "                 image_names: np.ndarray, \n",
    "                 cap_inputs: np.ndarray, \n",
    "                 cap_targets: np.ndarray, \n",
    "                 batch_size: int, \n",
    "                 shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        Arguments:\n",
    "            image_name: Name of the imaga file\n",
    "            cap_inputs: Word vectors of known captions used for training and test,\n",
    "                        containing <start> keyword\n",
    "            cap_targets: Word vectors of known captions used for prediction,\n",
    "                         containing <end> keyword\n",
    "            batch_size: Batch size to use\n",
    "            shuffle: True: do shuffeling\n",
    "                     False: use order as is\n",
    "        \"\"\"\n",
    "        self.image_names = image_names\n",
    "        self.cap_inputs = cap_inputs\n",
    "        self.cap_targets = cap_targets\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.image_names))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of batch in the sequence.\"\"\"\n",
    "        return int(np.floor(len(self.image_names) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data using the index\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        imgs, cap_ins, cap_outs = [], [], []\n",
    "        for i in indexes:\n",
    "            imgs.append( np.load(self.image_names[i]) )\n",
    "            cap_ins.append( self.cap_inputs[i] )\n",
    "            cap_outs.append( self.cap_targets[i] )\n",
    "        return (np.array(imgs), np.array(cap_ins)),  np.array(cap_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init generators\n",
    "train_gen = MyDataGenerator(train_image_feature_names, train_enc_input_prep, train_enc_target_prep, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_gen = MyDataGenerator(test_image_feature_names, test_enc_input_prep, test_enc_target_prep, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encoder-Decoder model for Training/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config model dimensions\n",
    "VOCAB_SIZE = len(tokenizer.index_word)\n",
    "\n",
    "# Create own loss function\n",
    "# means: mask out padded targets\n",
    "def loss_func(targets, preds):\n",
    "    # last layer has softmax activation => from_logits=False\n",
    "    #sparse_cat_ce = SparseCategoricalCrossentropy()\n",
    "    sparse_cat_ce = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = K.cast(K.greater(targets, 0), dtype=tf.int64)    \n",
    "    # calc cross entropy masked\n",
    "    loss = sparse_cat_ce(targets, preds, sample_weight=mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Create own accuracy function\n",
    "def accuracy_func(targets, preds):\n",
    "    real_preds = K.cast(K.argmax(preds, axis=-1), dtype=tf.float32)\n",
    "    correct = K.cast(K.equal(targets, real_preds), dtype=tf.float32)\n",
    "    # mask out 0 padded elements\n",
    "    mask = K.cast(K.greater(targets, 0), dtype=tf.float32)\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "    return n_correct/n_total\n",
    "\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    # Set timestamp of training\n",
    "    timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n",
    "\n",
    "    def create_base_enc_dec_model(\n",
    "            enc_input_shape: tuple,\n",
    "            output_seq_length: int,\n",
    "            embedding_dim: int,\n",
    "            vocab_size: int,\n",
    "            units: int) -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Build up the encoder- and decoder-part for training and testing purpose\n",
    "\n",
    "        Arguments:\n",
    "            enc_input_shape: Tuple of input dimensions of the encoder\n",
    "            output_seq_length: Max length of caption sequence to predict\n",
    "            embedding_dim: Length of used word vectors\n",
    "            vocab_size: Number of words within the vocabulary\n",
    "            units: Number of CNN and RNN units to use\n",
    "        Returns:\n",
    "            compiled keras model\n",
    "        \"\"\"\n",
    "        # ENCODER\n",
    "        enc_input = Input(shape=enc_input_shape, name='enc_input')   \n",
    "        enc_output = layers.Flatten()(enc_input) # BSx64x2048 => BSx131072\n",
    "        enc_output = layers.Dense(units, activation='relu', name='enc_output')(enc_output)\n",
    "\n",
    "        # DECODER\n",
    "        dec_input = Input(shape=(None,), name='dec_input')\n",
    "\n",
    "        dec_embed_out = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            name='dec_embed')(dec_input)\n",
    "\n",
    "        dec_gru_out, _ = layers.GRU(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            name= 'dec_gru')(dec_embed_out, initial_state=[enc_output])\n",
    "\n",
    "        dec_out = layers.Dense(vocab_size, activation='softmax', name='dec_dense')(dec_gru_out)\n",
    "\n",
    "        # Compile model\n",
    "        model = Model([enc_input, dec_input], dec_out, name=MODEL_NAME)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-4, clipnorm=5.0),\n",
    "            loss=loss_func,\n",
    "            metrics=[accuracy_func]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Build up model\n",
    "    model = create_base_enc_dec_model(\n",
    "        enc_input_shape=(64, 2048),\n",
    "        output_seq_length=enc_input_prep.shape[-1],\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        units=UNITS)\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_filepath = DIR_MODEL_STORE + 'checkpoint'\n",
    "    cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy_func',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    cb_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=EARLY_STOPPING_PATIENT)\n",
    "    cb_learning_rate = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "    cb_tqdm = tfa.callbacks.TQDMProgressBar()\n",
    "    if LOG_MODEL:\n",
    "        cb_tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir = DIR_TENSORBOARD_LOG + f\"{model.name}_{timestamp_str}\",\n",
    "            histogram_freq=0, update_freq='epoch',\n",
    "            write_graph=True, write_images=False)\n",
    "        callbacks = [cb_early_stopping, cb_tqdm, cb_tensorboard,\n",
    "                     cb_checkpoint, cb_learning_rate]\n",
    "    else:\n",
    "        callbacks = [cb_early_stopping, cb_tqdm, cb_checkpoint, cb_learning_rate]\n",
    "\n",
    "    # Plot model\n",
    "    if SAVE_MODEL:\n",
    "        display(plot_model(model,\n",
    "               show_shapes=True, \n",
    "               to_file=DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".png\"))\n",
    "    else:\n",
    "        display(plot_model(model, show_shapes=True))\n",
    "\n",
    "    # Train model\n",
    "    if RUN_TRAINING:\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=test_gen,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0)\n",
    "\n",
    "        # Save model and history\n",
    "        if SAVE_MODEL:\n",
    "            # Save history\n",
    "            fp_hist = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".pickle\"\n",
    "            save_hist(fp_hist, history)\n",
    "            print(f\"Model history for {model.name} saved to: {fp_hist}\")\n",
    "\n",
    "            # Save model\n",
    "            fp_model = DIR_MODEL_STORE + model.name + \"_\" + timestamp_str + \".h5\"\n",
    "            model.save(fp_model, save_format='h5')\n",
    "            print(f\"Model {model.name} saved to: {fp_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display Tensorboard\n",
    "if TENSORBOARD_SHOW_INLINE:\n",
    "    # Load extension\n",
    "    %load_ext tensorboard\n",
    "\n",
    "    # Start tensorboard\n",
    "    if TENSORBOARD_LAUNCH:\n",
    "        %tensorboard --logdir $DIR_TENSORBOARD_LOG\n",
    "    else:\n",
    "        # Show tensorboard inline für monitoring training performance\n",
    "        tb.notebook.list() # View open TensorBoard instances\n",
    "        tb.notebook.display(port=6006, height=800)  #  Select the hopefully already startet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encoder-Decoder model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../models/InceptV3_GRU_NoAttention_CustEmbedding_768-units-all/InceptV3_GRU_NoAttention_CustEmbedding_20210429-173000.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a0d4b16364ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     model = tf.keras.models.load_model(\n\u001b[1;32m     53\u001b[0m         \u001b[0mfp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss_func\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy_func\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0maccuracy_func\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model {model.name} loaded from file: {fp_model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aida/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aida/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ../models/InceptV3_GRU_NoAttention_CustEmbedding_768-units-all/InceptV3_GRU_NoAttention_CustEmbedding_20210429-173000.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "def creat_prediction_models(model: tf.keras.Model, units: int) -> (tf.keras.Model, tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Create prediction models from trained model\n",
    "\n",
    "    Arguments:\n",
    "        model: Former created training model\n",
    "        units: Number of CNN and RNN units to use\n",
    "    Returns:\n",
    "        Tuple containing the compiled models\n",
    "        Index: 0: endoder\n",
    "               1: decoder\n",
    "    \"\"\"\n",
    "\n",
    "    # ENCODER\n",
    "    enc_input = model.get_layer(name='enc_input').input\n",
    "    enc_output = model.get_layer(name='enc_output').output\n",
    "    encoder_model = Model(enc_input, [enc_output])\n",
    "\n",
    "    # DECODER\n",
    "    # 1. define Inputs (LSTM => (hidden_state, cell_state))\n",
    "    dec_input = model.get_layer(name='dec_input').input\n",
    "    dec_state_input_h = Input(shape=(units,))\n",
    "    dec_states_inputs = [dec_state_input_h]\n",
    "\n",
    "    # 2. get embedding layer\n",
    "    dec_embedding = model.get_layer(name='dec_embed')\n",
    "    dec_outputs = dec_embedding(dec_input)\n",
    "\n",
    "    # 3. get LSTM \n",
    "    # initial_state comes from encoder in first time step after that from last times hidden+cell state\n",
    "    dec_gru = model.get_layer(name='dec_gru')\n",
    "    dec_outputs, state_h_dec = dec_gru(\n",
    "        dec_outputs, initial_state=dec_states_inputs\n",
    "    )\n",
    "\n",
    "    # 4. output FC layer\n",
    "    dec_dense = model.get_layer(name='dec_dense')\n",
    "    dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "    # 5. Create Model\n",
    "    decoder_model = Model(\n",
    "        [dec_input, dec_state_input_h],\n",
    "        [dec_outputs, state_h_dec] # give out hidden state for next iteration\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "# Build up model\n",
    "if not RUN_TRAINING:\n",
    "    # Load model\n",
    "    fp_model = DIR_MODEL_STORE + FN_MODEL2LOAD\n",
    "    model = tf.keras.models.load_model(\n",
    "        fp_model,\n",
    "        custom_objects={\"loss_func\": loss_func, \"accuracy_func\" : accuracy_func}\n",
    "    )\n",
    "    print(f\"Model {model.name} loaded from file: {fp_model}\")\n",
    "\n",
    "    # Load training history\n",
    "    fp_hist = DIR_MODEL_STORE + FN_HIST2LOAD\n",
    "    history = load_hist(fp_hist)\n",
    "    print(f\"Training history for model {model.name} loaded from file: {fp_hist}\")\n",
    "\n",
    "encoder_model, decoder_model = creat_prediction_models(model, units=UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DECODER_SEQ_LEN = enc_input_prep.shape[-1]\n",
    "\n",
    "def make_prediction(input_img_feature_path, verbose: int = 0):\n",
    "    \"\"\"\n",
    "    Initialize \n",
    "    Arguments:\n",
    "        input_img_feature_path: Complete path to image file\n",
    "        verbose: 0: don't show any comment\n",
    "                 1: show working comments\n",
    "    Returns:\n",
    "        predicted caption for the input image\n",
    "    \"\"\"\n",
    "    # input_img_feature_path: path to picture features numpy data\n",
    "    # load numpy features from input_img_feature_path\n",
    "    img_features = np.expand_dims(np.load(input_img_feature_path), axis=0 )\n",
    "\n",
    "    # get encoder states\n",
    "    state_h = encoder_model.predict(img_features)\n",
    "    if verbose >= 1:\n",
    "        print('state_h', state_h.shape)\n",
    "\n",
    "    # create start token\n",
    "    dec_input = tf.constant([[tokenizer.word_index['<start>']]])\n",
    "    if verbose >= 1:\n",
    "        print('dec_input.shape', dec_input.shape)\n",
    "\n",
    "    caption = []\n",
    "    while len(caption) < MAX_DECODER_SEQ_LEN:\n",
    "        dec_output, state_h = decoder_model.predict(\n",
    "            [dec_input, state_h],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # next input must be the current output (this is already a word index)\n",
    "        dec_input = tf.argmax(dec_output, -1)\n",
    "\n",
    "        word_ndx = dec_input.numpy()[0,0] # detach from tf\n",
    "        predicted_word = tokenizer.index_word[word_ndx]\n",
    "\n",
    "        # do not append <end> tag to caption\n",
    "        if predicted_word == '<end>':\n",
    "           break\n",
    "\n",
    "        caption.append(predicted_word)\n",
    "\n",
    "    return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc ROUGE-L and BLEU-1 metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare true captures (references) and predicted captures (candidates)\n",
    "c_true_test = dict()\n",
    "c_pred_test = dict()\n",
    "df = df_cap.loc[df_cap.is_test == True]\n",
    "for img_fname in tqdm.tqdm_notebook(df.image.unique(), total=len(df.image.unique())):\n",
    "    c_true_test[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()\n",
    "    c_pred_test[img_fname] = make_prediction(os.path.join(DIR_IMAGE_FEATURES, img_fname.replace('.jpg','.npy')))\n",
    "\n",
    "# Serialize data into file\n",
    "if timestamp_str == None:\n",
    "    if USE_CONFIG_TIMESTAMP:\n",
    "        timestamp_str = FN_TIMESTAMP\n",
    "    else:\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n",
    "json.dump(c_true_test, open(DIR_RESULT_STORE + f\"c_true_test_{timestamp_str}.json\", 'w'))\n",
    "json.dump(c_pred_test, open(DIR_RESULT_STORE + f\"c_pred_test_{timestamp_str}.json\", 'w'))\n",
    "\n",
    "# Calculate Metrics\n",
    "rouge_l_test, bleu_1_test = met.evaluate(c_true_test, c_pred_test)\n",
    "print(f\"Metrics for Testset:\\nROUGE-L recall:{rouge_l_test:>10.2%}\\nBLEU-1 precision: {bleu_1_test:>7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare true captures (references) and predicted captures (candidates)\n",
    "c_true_train = dict()\n",
    "c_pred_train = dict()\n",
    "df = df_cap.loc[df_cap.is_test == False]\n",
    "for img_fname in tqdm.tqdm_notebook(df.image.unique(), total=len(df.image.unique())):\n",
    "    c_true_train[img_fname] = df_cap.loc[df_cap.image == img_fname].caption.to_list()\n",
    "    c_pred_train[img_fname] = make_prediction(os.path.join(DIR_IMAGE_FEATURES, img_fname.replace('.jpg','.npy')))\n",
    "\n",
    "# Serialize data into file\n",
    "if timestamp_str == None:\n",
    "    if USE_CONFIG_TIMESTAMP:\n",
    "        timestamp_str = FN_TIMESTAMP\n",
    "    else:\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n",
    "json.dump(c_true_train, open(DIR_RESULT_STORE + f\"c_true_train_{timestamp_str}.json\", 'w'))\n",
    "json.dump(c_pred_train, open(DIR_RESULT_STORE + f\"c_pred_train_{timestamp_str}.json\", 'w'))\n",
    "\n",
    "# Calculate Metrics\n",
    "rouge_l_train, bleu_1_train = met.evaluate(c_true_train, c_pred_train)\n",
    "print(f\"Metrics for Trainset:\\nROUGE-L recall:{rouge_l_train:>10.2%}\\nBLEU-1 precision: {bleu_1_train:>7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to csv file\n",
    "if timestamp_str == None:\n",
    "    if USE_CONFIG_TIMESTAMP:\n",
    "        timestamp_str = FN_TIMESTAMP\n",
    "    else:\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # set timestamp for current run\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\"ROUGE-L recallt\", \"BLEU-1 precission\"],\n",
    "    \"Score Test\": [rouge_l_test, bleu_1_test],\n",
    "    \"Score Train\": [rouge_l_train, bleu_1_train]}).reset_index(drop=True)\n",
    "\n",
    "fp = DIR_RESULT_STORE + f\"{model.name}_{timestamp_str}.csv\"\n",
    "df.to_csv(fp)\n",
    "print(f\"Results written to file: {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict something from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_image_from_test(img_name):\n",
    "    img = plt.imread(os.path.join(DIR_IMAGES, img_name))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nReference captions:\\n{\"-\"*80}')\n",
    "    for l in df_cap[df_cap['image']==img_name]['caption'].values:\n",
    "        print(f'- {l}')\n",
    "\n",
    "    prediction = make_prediction(os.path.join(DIR_IMAGE_FEATURES, img_name.replace('.jpg','.npy')))\n",
    "    print(f'\\nPrediction:\\n{\"-\"*80}')\n",
    "    print(f'- {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 10\n",
    "for img_file_name in df_cap[df_cap['is_test']].sample(num_predictions)['image'].values:\n",
    "    eval_image_from_test(img_file_name)\n",
    "    print(f'{\"=\"*80}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
