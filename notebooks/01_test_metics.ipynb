{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAgbCF4OFgv9"
   },
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_2_TENSORBOARD = False  # True: write accuracy and loss statistics to tensorboard\n",
    "RUN_ON_KAGGLE = False  # True: Only Colab ready code will be processed\n",
    "DO_GOOGLE_BACKTRANSLATION = False\n",
    "\n",
    "RUN_GRU_MODEL = True  # True: run prediction, save model during training\n",
    "TRAIN_GRU_MODEL = False  # True: train and save model\n",
    "\n",
    "GRU_MODEL_FNAME = \"GRU_Translator_256_20210427-155004.hd5\"\n",
    "\n",
    "DEBUG = False\n",
    "SEED = 42\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "MEMORY_OF_GPU = 10000  # MBytes\n",
    "MEMORY_OF_GPU = 6000  # MBytes\n",
    "\n",
    "TENSORBOARD_LOGDIR = logdir = \"./tensorboard_logs/scalars/\"\n",
    "FILE_DIR = \"../data/raw/\"\n",
    "MODEL_DIR = \"../data/interim/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMzRkuZbgt3k"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MGrOSRpUxG9L"
   },
   "outputs": [],
   "source": [
    "# You may want to upgrade the tensorflow package to the latest verion (currently version 2.3) if you have the proper infrastructure\n",
    "if RUN_ON_KAGGLE:\n",
    "    !pip install --upgrade tensorflow\n",
    "    !pip install tqdm\n",
    "    !pip install google_trans_new\n",
    "    !pip install pycocoevalcap\n",
    "    !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cD0z5tl4dVOM"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as kaba\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, LSTM\n",
    "from tensorflow.keras.layers import Embedding, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
    "\n",
    "import src.models.metrics as met\n",
    "\n",
    "import collections\n",
    "import tqdm\n",
    "import datetime as dt\n",
    "import time\n",
    "import google_trans_new\n",
    "import sklearn\n",
    "import sklearn.metrics as skme\n",
    "import sklearn.model_selection as skms\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "gtranse = google_trans_new.google_translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7326,
     "status": "ok",
     "timestamp": 1604638019422,
     "user": {
      "displayName": "Wolfgang Reuter",
      "photoUrl": "",
      "userId": "00430053052083581161"
     },
     "user_tz": -60
    },
    "id": "fobJjbQhBuju",
    "outputId": "372f5437-ce3f-44a1-d78e-acaf82741832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "Runtime Context: CPUs\n",
      "Batch Size: 64 datasets\n"
     ]
    }
   ],
   "source": [
    "# To get access to a GPU instance you can use the `change runtime type` and set the option to `GPU` from the `Runtime` tab  in the notebook\n",
    "# Checking the GPU availability for the notebook\n",
    "#tf.test.gpu_device_name()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Create virtual GPUs\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK\n",
    "            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],\n",
    "        )\n",
    "\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK \n",
    "            gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],            \n",
    "        )\n",
    "    except:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(\"Warning: During GPU handling.\")\n",
    "        pass\n",
    "    finally:\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\\n\")\n",
    "\n",
    "# Set runtime context and batch size\n",
    "l_rtc_names = [\n",
    "    \"multi-GPU_MirroredStrategy\",\n",
    "    \"multi-GPU_CentralStorageStrategy\",\n",
    "    \"1-GPU\",\n",
    "    \"CPUs\",\n",
    "    \"multi-GPU_MirroredStrategy_NCCL-All-Reduced\",\n",
    "]\n",
    "l_rtc = [\n",
    "    tf.distribute.MirroredStrategy().scope(),\n",
    "    tf.distribute.experimental.CentralStorageStrategy().scope(),\n",
    "    tf.device(\"/GPU:0\"),\n",
    "    tf.device(\"/CPU:0\"),\n",
    "    tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce()).scope(),\n",
    "]\n",
    "if len(gpus) == 0:\n",
    "    rtc_idx = 3\n",
    "    batch_size = 64\n",
    "\n",
    "elif len(gpus) == 1:\n",
    "    rtc_idx = 2\n",
    "    batch_size = 4*256\n",
    "\n",
    "elif len(gpus) > 1:\n",
    "    rtc_idx = 0\n",
    "    batch_size = 8*256\n",
    "\n",
    "# DELME\n",
    "#rtc_idx = 3\n",
    "\n",
    "runtime_context = l_rtc[rtc_idx]\n",
    "\n",
    "print(f\"\\nRuntime Context: {l_rtc_names[rtc_idx]}\")\n",
    "print(f\"Batch Size: {batch_size} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu1mXD8unuu9"
   },
   "source": [
    "### Loading and Preprocessing of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPqbg_C09G6s"
   },
   "source": [
    "#### Reading CSV data files as a list of strings, where each string represents a single line from the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CHuc68kPoSqo"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import TextIOWrapper\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1604638087780,
     "user": {
      "displayName": "Wolfgang Reuter",
      "photoUrl": "",
      "userId": "00430053052083581161"
     },
     "user_tz": -60
    },
    "id": "oGBtz_Lt39XM",
    "outputId": "a3c11f7e-26da-4e21-bc90-1c788f0f09eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text samples: ['new jersey is sometimes quiet during autumn , and it is snowy in april .', 'the united states is usually chilly during july , and it is usually freezing in november .']\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(FILE_DIR + 'small_vocab_en.csv.zip') as zf:\n",
    "    with zf.open('small_vocab_en.csv', 'r') as infile:\n",
    "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
    "        english_text = [item for sublist in reader for item in sublist]\n",
    "        print(f\"English text samples: {english_text[0:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vVS_nH-4y6J",
    "outputId": "a1e3d080-241d-4f4f-c750-ae7819dc3b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French translation text samples: [\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\", 'les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .']\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(FILE_DIR + 'small_vocab_fr.csv.zip') as zf:\n",
    "    with zf.open('small_vocab_fr.csv', 'r') as infile:\n",
    "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
    "        french_text = [item for sublist in reader for item in sublist]\n",
    "        print(f\"French translation text samples: {french_text[0:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5y0rPhaH66T"
   },
   "source": [
    "### TODO: Data exploration -> calculate and print some statistics on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text\n",
      "============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137860 entries, 0 to 137859\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    137860 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>129174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>he dislikes grapes , lemons , and strawberries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "count                                            137860\n",
       "unique                                           129174\n",
       "top     he dislikes grapes , lemons , and strawberries.\n",
       "freq                                                  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. sentence length: 102\n",
      "Min. sentence length: 13\n",
      "Number of words: 200\n",
      "Number of duplicated sentences: 8686\n"
     ]
    }
   ],
   "source": [
    "# Load text to dataframes\n",
    "df_en = pd.DataFrame(data=english_text, columns=[\"text\"], dtype=str)\n",
    "df_fr = pd.DataFrame(data=french_text, columns=[\"text\"], dtype=str)\n",
    "\n",
    "# Show statistics\n",
    "print(\"English Text\")\n",
    "print(\"============\")\n",
    "display(df_en.info())\n",
    "display(df_en.describe())\n",
    "print(f\"Max. sentence length: {max(df_en.text.str.len())}\")\n",
    "print(f\"Min. sentence length: {min(df_en.text.str.len())}\")\n",
    "results_en = collections.Counter()\n",
    "df_en.text.str.replace(\"[^a-zA-z ]\", \"\").str.split(\" \").apply(results_en.update)\n",
    "print(f\"Number of words: {len(results_en.keys())}\")\n",
    "english_text_unique = set(english_text)\n",
    "print(f\"Number of duplicated sentences: {len(english_text) - len(english_text_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "French Text\n",
      "=============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137860 entries, 0 to 137859\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    137860 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>120806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>elle déteste les citrons et les citrons verts .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "count                                            137860\n",
       "unique                                           120806\n",
       "top     elle déteste les citrons et les citrons verts .\n",
       "freq                                                  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. sentence length: 114\n",
      "Min. sentence length: 13\n",
      "Number of unique words: 340\n",
      "Number of duplicated sentences: 17054\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrench Text\")\n",
    "print(\"=============\")\n",
    "display(df_fr.info())\n",
    "display(df_fr.describe())\n",
    "print(f\"Max. sentence length: {max(df_fr.text.str.len())}\")\n",
    "print(f\"Min. sentence length: {min(df_fr.text.str.len())}\")\n",
    "results_fr = collections.Counter()\n",
    "df_fr.text.str.replace(\"[^a-zA-z ]\", \"\").str.split(\" \").apply(results_fr.update)\n",
    "print(f\"Number of unique words: {len(results_fr.keys())}\")\n",
    "# check doubled english sentences\n",
    "french_text_unique = set(french_text)\n",
    "print(f\"Number of duplicated sentences: {len(french_text) - len(french_text_unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ8zEtacK08P"
   },
   "source": [
    "#### Tokenizing the data\n",
    "\n",
    "Since the input data to neural networks needs to be in numerical format, we first turn each sentence into a sequence of word ids using `Tokenizer` function from Keras. Word ids are numerical presentations for words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6s5iwnMKzT4",
    "outputId": "a9e7c9c1-f60c-4514-fdd7-fda9be0f2490"
   },
   "outputs": [],
   "source": [
    "def tokenize(x: List[str]):\n",
    "  \"\"\"Tokenizes sentences into word ids.\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "\n",
    "  \"\"\"\n",
    "  tokenizer=Tokenizer()\n",
    "  tokenizer.fit_on_texts(x)\n",
    "  t=tokenizer.texts_to_sequences(x)\n",
    "  return t, tokenizer\n",
    "\n",
    "if DEBUG:\n",
    "    # Tokenize Example output\n",
    "    text_sentences = [\n",
    "        'The quick brown fox jumps over the lazy dog .',\n",
    "        'By Jove , my quick study of lexicography won a prize .',\n",
    "        'This is a short sentence .']\n",
    "    text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "    print(text_tokenizer.word_index)\n",
    "    for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "        print('Sequence {} in x'.format(sample_i + 1))\n",
    "        print('  Input:  {}'.format(sent))\n",
    "        print('  Output: {}'.format(token_sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfffhgywsXMN"
   },
   "source": [
    "#### Padding Sequences\n",
    "\n",
    "To be able to batch the sequences of word ids together, all sequences need to be of the same length. Therefore, as a second preprocessing step we make sure that all the English sentences have the same length as their respective French translations using `pad_sequence` function form Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsB7MNttVYVS",
    "outputId": "cc3f9dbd-26fc-4ccb-ab7b-290032404415"
   },
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    padding=pad_sequences(x,padding='post',maxlen=length)\n",
    "    return padding\n",
    "\n",
    "if DEBUG:\n",
    "    # Pad Tokenized output\n",
    "    test_pad = pad(text_tokenized)\n",
    "    for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "        print('Sequence {} in x'.format(sample_i + 1))\n",
    "        print('  Input:  {}'.format(np.array(token_sent)))\n",
    "        print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzZF_s8TuGqw"
   },
   "source": [
    "#### Preprocessing Pipeline\n",
    "\n",
    "As mentioned in the introduction section in this excercise your focus is on building different recurrent neural network architectures. Threfore, in the following cell we provide you with a preprocessing pipeline. You can apply this function to the input data to prepare the data for using in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B158-HbGqMwd",
    "outputId": "8ca482b1-5e3b-48f3-da1d-aa6176301c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npreproc_english_sentences_all, preproc_french_sentences_all, english_tokenizer, french_tokenizer =    preprocess(english_text, french_text)\\n\\nmax_english_sequence_length = preproc_english_sentences_all.shape[1]\\nmax_french_sequence_length = preproc_french_sentences_all.shape[1]\\nenglish_vocab_size = len(english_tokenizer.word_index)\\nfrench_vocab_size = len(french_tokenizer.word_index)\\n\\nprint(\\'Preprocessed Data Info\\')\\nprint(\"Max English sentence length:\", max_english_sequence_length)\\nprint(\"Max French sentence length:\", max_french_sequence_length)\\nprint(\"English vocabulary size:\", english_vocab_size)\\nprint(\"French vocabulary size:\", french_vocab_size)\\n\\n# Show some details\\nif DEBUG:\\n    print(preproc_english_sentences_all.shape, preproc_english_sentences[0])\\n    print(english_tokenizer.sequences_to_texts([[17],[23],[1],[44]]))\\n    print(english_tokenizer.get_config())\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(x, y, x_len=None, y_len=None):\n",
    "    \"\"\"\n",
    "    Preprocessing of input (x) and target (y), words are converted to numbers\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :param x_len: forced length of sentence\n",
    "    :param y_len: forced length of sentence\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x, x_len)\n",
    "    preprocess_y = pad(preprocess_y, y_len)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\"\"\"\n",
    "preproc_english_sentences_all, preproc_french_sentences_all, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_text, french_text)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences_all.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences_all.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Preprocessed Data Info')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n",
    "\n",
    "# Show some details\n",
    "if DEBUG:\n",
    "    print(preproc_english_sentences_all.shape, preproc_english_sentences[0])\n",
    "    print(english_tokenizer.sequences_to_texts([[17],[23],[1],[44]]))\n",
    "    print(english_tokenizer.get_config())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Info\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Remove duplicated english sentences\n",
    "df = pd.DataFrame(data=zip(english_text, french_text, ), columns=[\"english_text\", \"french_text\"])\n",
    "df = df.drop_duplicates(subset=['english_text'])\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df.english_text.to_numpy(), df.french_text.to_numpy())\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Preprocessed Data Info')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate a Test-Set for performance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116256, 15), (116256, 21, 1), (12918, 15), (12918, 21, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split it\n",
    "df_train, df_test = skms.train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# Convert it\n",
    "preproc_english_sentences_train, preproc_french_sentences_train, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df_train.english_text.to_numpy(),\n",
    "               df_train.french_text.to_numpy(),\n",
    "               x_len=max_english_sequence_length,\n",
    "               y_len=max_french_sequence_length)\n",
    "\n",
    "preproc_english_sentences_test, preproc_french_sentences_test, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df_test.english_text.to_numpy(),\n",
    "               df_test.french_text.to_numpy(),\n",
    "               x_len=max_english_sequence_length,\n",
    "               y_len=max_french_sequence_length)\n",
    "\n",
    "preproc_english_sentences_train.shape, preproc_french_sentences_train.shape,\\\n",
    "preproc_english_sentences_test.shape, preproc_french_sentences_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_fCUxliHXwO"
   },
   "source": [
    "### Model\n",
    "\n",
    "In this section we ask you to build and train Three (optionally four) different neural networks and use them to predict the French Translation of the English sentences.\n",
    "\n",
    "Make use of the `logits_to_text()` function to transform the logits from output of your networks to a French translation. This helps you to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2CrRvXm91nhd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_to_text(fl, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn features or target labels of a neural network into text using the tokenizer\n",
    "    :param fl: Feature or label used in a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return' '.join([index_to_words[w[0]] for idx, w in enumerate(fl)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(model, X_train, X_train_print=None):\n",
    "    \"\"\"\n",
    "    Print two sentences checking the ability of the model\n",
    "    for doing translations\n",
    "    \"\"\"\n",
    "    if X_train_print == None:\n",
    "        X_train_print = X_train\n",
    "\n",
    "    pred = lambda x: x[0:1]\n",
    "    y_pred_train = model.predict(pred(X_train))\n",
    "    orig_sentence = fl_to_text(pred(X_train_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en')}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "\n",
    "    pred = lambda x: x[1:2]\n",
    "    y_pred_train = model.predict(pred(X_train))\n",
    "    orig_sentence = fl_to_text(pred(X_train_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en').lower()}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "\n",
    "\n",
    "def print_predictions_ed(model, X_train_en, X_train_fr, X_train_en_print=None):\n",
    "    \"\"\"\n",
    "    Print two sentences checking the ability of the model\n",
    "    for doing translations\n",
    "    \"\"\"\n",
    "    pred = lambda x: x[0:1]\n",
    "    y_pred_train = model.predict([pred(X_train_en), pred(X_train_fr)])\n",
    "    orig_sentence = fl_to_text(pred(X_train_en_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en')}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "\n",
    "    pred = lambda x: x[1:2]\n",
    "    y_pred_train = model.predict([pred(X_train_en), pred(X_train_fr)])\n",
    "    orig_sentence = fl_to_text(pred(X_train_en_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en')}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(model, X_test, y_true_test):\n",
    "    \"\"\"\n",
    "    Print metrics on test set\n",
    "    \"\"\"\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_test = tf.keras.metrics.Accuracy(y_true_test, y_pred_test)\n",
    "\n",
    "    print(f\"accuracy: {acc:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks used within models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(early_stopping_patience=5):\n",
    "    \"\"\"Definition of keras callbacks during model run\n",
    "    \"\"\"\n",
    "    # Tensorboard config\n",
    "    if not RUN_ON_KAGGLE:\n",
    "        # Define log directory\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tb_log_dir = \\\n",
    "            TENSORBOARD_LOGDIR + \\\n",
    "            f\"{model.name}_\" + timestamp_str\n",
    "\n",
    "        # Define writers for custom scalar \"learning rate\"\n",
    "        file_writer = tf.summary.create_file_writer(tb_log_dir + \"/metrics\")\n",
    "        file_writer.set_as_default()\n",
    "\n",
    "    # Init callbacks\n",
    "    cb_early_stopping = kaba.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=early_stopping_patience)\n",
    "    cb_tqdm = tfa.callbacks.TQDMProgressBar()\n",
    "    cb_tensorboard = kaba.TensorBoard(\n",
    "        log_dir = tb_log_dir,\n",
    "        histogram_freq=0, update_freq='epoch',\n",
    "        write_graph=True, write_images=False)\n",
    "\n",
    "    # Set final callbacks\n",
    "    callbacks = [cb_early_stopping, cb_tqdm]\n",
    "    if LOG_2_TENSORBOARD:\n",
    "        callbacks = callbacks.append(cb_tensorboard)\n",
    "\n",
    "    return callbacks, timestamp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model_dir:str, model_fname: str):\n",
    "    \"\"\"Loads a model from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from file {model_fname}...\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(model_dir + model_fname)\n",
    "\n",
    "    # Check and set name of model\n",
    "    if model.name == None:\n",
    "        model_name = \"Noname\"\n",
    "    else:\n",
    "        model_name = model.name\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    secs_all = toc - tic\n",
    "    mins = int(secs_all / 60)\n",
    "    secs = int((secs_all - mins*60))\n",
    "    print(f\"model {model_name} loaded in {mins}m {secs}s!\")\n",
    "\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhYC7lJmxpdd"
   },
   "source": [
    "#### Model 1: Implement a simple RNN network and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [],
   "source": [
    "# Model 1: simple RNN model\n",
    "def gru_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers and compile the model. (Hint: Use a GRU network followed by a TimeDistributed dense layer from tensorflow.keras.layers module)\n",
    "    # What is the input shape of your model? Which activation function do you choose for the dense layer?\n",
    "    # +++ Answers ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # Input shape: time step:  max_sentence_length\n",
    "    #              features:   english_vocab_size\n",
    "    # Activation Function: softmax (mulit-class problem per TimeDistributed dense layer output)\n",
    "    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++   \n",
    "    # Input Layer\n",
    "    inputs = Input(shape=input_shape, name=\"Input\")\n",
    "\n",
    "    # GRU layer\n",
    "    # CuDNNGRU implementation are used for fast GPU processing, right padding is a must!\n",
    "    gru_units = 256\n",
    "    rnn = GRU(\n",
    "        units=gru_units,\n",
    "        return_sequences=True,  # many to many\n",
    "        # cuDNN requirements\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        recurrent_dropout=0,\n",
    "        unroll=False,\n",
    "        use_bias=True,\n",
    "        reset_after=True,\n",
    "        name=\"RNN\"\n",
    "    )(inputs)\n",
    "\n",
    "    # Add a time distributed Dense layer\n",
    "    # That dense layer will be applied to every time step\n",
    "    logits = TimeDistributed(Dense(units=french_vocab_size), name=\"Logits\")(rnn)\n",
    "\n",
    "    # Add activation\n",
    "    output = Activation('softmax', name=\"Softmax_Activation\")(logits)\n",
    "\n",
    "    # Assemble model\n",
    "    model = Model(inputs, output, name=f\"GRU_Translator_{gru_units}\")\n",
    "\n",
    "    # Build up optimizer and loss function\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        metrics=['accuracy', met.categorical_accuracy_with_variable_timestep]\n",
    "    )\n",
    "\n",
    "    # Show it\n",
    "    model.summary()\n",
    "    display(tf.keras.utils.plot_model(model))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "if TRAIN_GRU_MODEL:\n",
    "    with runtime_context:\n",
    "        # Clear GPU Memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # TODO: Preprocess and reshape the input to work with a basic RNN. (Hint: Use preproc_english_sentences, preproc_french_sentences and reshape function from keras)\n",
    "        max_sentence_length = max([max_english_sequence_length, max_french_sequence_length])\n",
    "        X_train = pad(preproc_english_sentences_train, max_sentence_length)\n",
    "        X_train = X_train.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_train = pad(preproc_french_sentences_train, max_sentence_length)\n",
    "        X_test = pad(preproc_english_sentences_test, max_sentence_length)\n",
    "        X_test = X_test.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_test = pad(preproc_french_sentences_test, max_sentence_length)\n",
    "        print(X_train.shape, y_true_train.shape, X_test.shape, y_true_test.shape)\n",
    "\n",
    "        # TODO: Train the network you built in rnn_model. What size do you choose for the epochs, batch_size and validation_split?\n",
    "        timestep = max_sentence_length\n",
    "        features = 1  # observations at time\n",
    "        model = gru_model(\n",
    "            (timestep, features),\n",
    "            timestep,\n",
    "            english_vocab_size,\n",
    "            french_vocab_size\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        callbacks, timestamp_str = get_callbacks()\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_true_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116256, 21, 1) (116256, 21, 1) (12918, 21, 1) (12918, 21, 1)\n",
      "Loading model from file GRU_Translator_256_20210427-155004.hd5...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-705e3fed1b0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mgru_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU_MODEL_FNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgru_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-84bedcb79b26>\u001b[0m in \u001b[0;36mmodel_load\u001b[1;34m(model_dir, model_fname)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Check and set name of model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\anaconda3\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m   raise IOError(\n",
      "\u001b[1;32mC:\\Tools\\anaconda3\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[1;31m# Recreate layers and metrics using the info stored in the metadata.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[0mkeras_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_graph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[0mkeras_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;31m# Generate a dictionary of all loaded nodes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\anaconda3\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_layers\u001b[1;34m(self, compile)\u001b[0m\n\u001b[0;32m    380\u001b[0m         self.loaded_nodes[node_metadata.node_id] = self._load_layer(\n\u001b[0;32m    381\u001b[0m             \u001b[0mnode_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             node_metadata.metadata)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Metrics are only needed when the model is compiled later. We ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\anaconda3\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36m_load_layer\u001b[1;34m(self, node_id, identifier, metadata)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_revive_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m       \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrevive_custom_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;31m# Add an attribute that stores the extra functions/objects saved in the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\anaconda3\\envs\\aida\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mrevive_custom_object\u001b[1;34m(identifier, metadata)\u001b[0m\n\u001b[0;32m    935\u001b[0m                      \u001b[1;34m'and `from_config` when saving. In addition, please use '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m                      \u001b[1;34m'the `custom_objects` arg when calling `load_model()`.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m                      .format(identifier))\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`."
     ]
    }
   ],
   "source": [
    "if RUN_GRU_MODEL:\n",
    "    gru_model = None\n",
    "    if TRAIN_GRU_MODEL:\n",
    "        # Save model\n",
    "        gru_model = model\n",
    "        model.save(MODEL_DIR + model.name + \"_\" + timestamp_str + \".hd5\")\n",
    "    else:\n",
    "        # Load model and set paras\n",
    "        max_sentence_length = max([max_english_sequence_length, max_french_sequence_length])\n",
    "        X_train = pad(preproc_english_sentences_train, max_sentence_length)\n",
    "        X_train = X_train.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_train = pad(preproc_french_sentences_train, max_sentence_length)\n",
    "        X_test = pad(preproc_english_sentences_test, max_sentence_length)\n",
    "        X_test = X_test.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_test = pad(preproc_french_sentences_test, max_sentence_length)\n",
    "        print(X_train.shape, y_true_train.shape, X_test.shape, y_true_test.shape)\n",
    "        try:\n",
    "            gru_model, model_name = model_load(MODEL_DIR, GRU_MODEL_FNAME)\n",
    "            display(tf.keras.utils.plot_model(gru_model))\n",
    "        except:\n",
    "            print(f\"Couldn't load model {GRU_MODEL_FNAME}\")\n",
    "\n",
    "    # TODO: Print prediction(s) for one or more of the English sentences. Hint: Make use of the `logits_to_text` function for transforming output logits to text. \n",
    "    # Check performance\n",
    "    if gru_model != None:\n",
    "        #print_predictions(gru_model, X_train)\n",
    "        #print_performance(gru_model, X_test, y_true_test)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-850f26318780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-b74d38be7004>\u001b[0m in \u001b[0;36mprint_predictions\u001b[1;34m(model, X_train, X_train_print)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0morig_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfl_to_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_print\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menglish_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "print_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, include_prediction=False):\n",
    "    id_to_prediction = {}\n",
    "    id_to_references = {}\n",
    "\n",
    "    id_to_prediction[\"1\"] = y_pred\n",
    "    id_to_references[\"1\"] = [y_true]\n",
    "\n",
    "    metrics = {}\n",
    "    for metric in [met.BLEU(1), met.ROUGE()]:\n",
    "        metric_name_to_value = metric.calculate(id_to_prediction, id_to_references)\n",
    "        metrics.update(metric_name_to_value)\n",
    "    return (metrics, id_to_prediction) if include_prediction else metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to check\n",
    "pred = lambda x: x[0:1]\n",
    "y_pred_train = model.predict(pred(X_train))\n",
    "orig_sentence = fl_to_text(pred(X_train)[0], english_tokenizer)\n",
    "orig_translat = fl_to_text(pred(y_true_train)[0], french_tokenizer)\n",
    "pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "print(f\"Orig. translation:\\t\\t\\t{orig_translat}\")\n",
    "print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "evaluate(\n",
    "    orig_translat,\n",
    "    pred_sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    fl_to_text(pred(y_true_train)[0], french_tokenizer),\n",
    "    logits_to_text(y_pred_train[0:1], french_tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von Kopie von Translation.ipynb",
   "provenance": [
    {
     "file_id": "1J0WAJBVzswzQPrrxyw0i-l1s1kjrSs5q",
     "timestamp": 1618213242247
    },
    {
     "file_id": "1TCG0XYY0Z9TQbpPTFrwJ69T5saN7dInb",
     "timestamp": 1604919010084
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
