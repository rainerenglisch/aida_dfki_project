{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAgbCF4OFgv9"
   },
   "source": [
    "# Test metrics\n",
    "**See chapter \"Own implementaion\" for final result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_2_TENSORBOARD = False  # True: write accuracy and loss statistics to tensorboard\n",
    "RUN_ON_KAGGLE = False  # True: Only Colab ready code will be processed\n",
    "DO_GOOGLE_BACKTRANSLATION = False\n",
    "\n",
    "SAVE_GRU_MODEL = False\n",
    "RUN_GRU_MODEL = True  # True: run prediction, save model during training if SAVE_GRU_MODEL = True\n",
    "TRAIN_GRU_MODEL = True  # True: train and save model\n",
    "\n",
    "GRU_MODEL_FNAME = \"GRU_Translator_256_20210427-162007.hd5\"\n",
    "\n",
    "DEBUG = False\n",
    "SEED = 42\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "MEMORY_OF_GPU = 10000  # MBytes\n",
    "MEMORY_OF_GPU = 6000  # MBytes\n",
    "\n",
    "TENSORBOARD_LOGDIR = logdir = \"./tensorboard_logs/scalars/\"\n",
    "FILE_DIR = \"../data/raw/\"\n",
    "MODEL_DIR = \"../data/interim/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMzRkuZbgt3k"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MGrOSRpUxG9L"
   },
   "outputs": [],
   "source": [
    "# You may want to upgrade the tensorflow package to the latest verion (currently version 2.3) if you have the proper infrastructure\n",
    "if RUN_ON_KAGGLE:\n",
    "    !pip install --upgrade tensorflow\n",
    "    !pip install tqdm\n",
    "    !pip install google_trans_new\n",
    "    !pip install pycocoevalcap\n",
    "    !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cD0z5tl4dVOM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ths/miniconda3/envs/aida/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ths/miniconda3/envs/aida/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as kaba\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, LSTM\n",
    "from tensorflow.keras.layers import Embedding, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
    "\n",
    "import src.models.metrics_experimental as met_exp\n",
    "import src.models.metrics as met\n",
    "\n",
    "import collections\n",
    "import tqdm\n",
    "import datetime as dt\n",
    "import time\n",
    "import google_trans_new\n",
    "import sklearn\n",
    "import sklearn.metrics as skme\n",
    "import sklearn.model_selection as skms\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "gtranse = google_trans_new.google_translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7326,
     "status": "ok",
     "timestamp": 1604638019422,
     "user": {
      "displayName": "Wolfgang Reuter",
      "photoUrl": "",
      "userId": "00430053052083581161"
     },
     "user_tz": -60
    },
    "id": "fobJjbQhBuju",
    "outputId": "372f5437-ce3f-44a1-d78e-acaf82741832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: During GPU handling.\n",
      "1 Physical GPU, 2 Logical GPUs\n",
      "\n",
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1'], variable_device = '/device:CPU:0'\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "\n",
      "Runtime Context: 1-GPU\n",
      "Batch Size: 1024 datasets\n"
     ]
    }
   ],
   "source": [
    "# To get access to a GPU instance you can use the `change runtime type` and set the option to `GPU` from the `Runtime` tab  in the notebook\n",
    "# Checking the GPU availability for the notebook\n",
    "#tf.test.gpu_device_name()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Create virtual GPUs\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK\n",
    "            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],\n",
    "        )\n",
    "\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            #OK, but solwer: \n",
    "            #gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024),\n",
    "            #      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2.5*1024)],\n",
    "            #OK \n",
    "            gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2),\n",
    "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=MEMORY_OF_GPU//2)],\n",
    "            #Error using NCCL automatically on mirrored strategy: gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)],            \n",
    "        )\n",
    "    except:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(\"Warning: During GPU handling.\")\n",
    "        pass\n",
    "    finally:\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\\n\")\n",
    "\n",
    "# Set runtime context and batch size\n",
    "l_rtc_names = [\n",
    "    \"multi-GPU_MirroredStrategy\",\n",
    "    \"multi-GPU_CentralStorageStrategy\",\n",
    "    \"1-GPU\",\n",
    "    \"CPUs\",\n",
    "    \"multi-GPU_MirroredStrategy_NCCL-All-Reduced\",\n",
    "]\n",
    "l_rtc = [\n",
    "    tf.distribute.MirroredStrategy().scope(),\n",
    "    tf.distribute.experimental.CentralStorageStrategy().scope(),\n",
    "    tf.device(\"/GPU:0\"),\n",
    "    tf.device(\"/CPU:0\"),\n",
    "    tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce()).scope(),\n",
    "]\n",
    "if len(gpus) == 0:\n",
    "    rtc_idx = 3\n",
    "    batch_size = 64\n",
    "\n",
    "elif len(gpus) == 1:\n",
    "    rtc_idx = 2\n",
    "    batch_size = 4*256\n",
    "\n",
    "elif len(gpus) > 1:\n",
    "    rtc_idx = 0\n",
    "    batch_size = 8*256\n",
    "\n",
    "# DELME\n",
    "#rtc_idx = 3\n",
    "\n",
    "runtime_context = l_rtc[rtc_idx]\n",
    "\n",
    "print(f\"\\nRuntime Context: {l_rtc_names[rtc_idx]}\")\n",
    "print(f\"Batch Size: {batch_size} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu1mXD8unuu9"
   },
   "source": [
    "### Loading and Preprocessing of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPqbg_C09G6s"
   },
   "source": [
    "#### Reading CSV data files as a list of strings, where each string represents a single line from the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CHuc68kPoSqo"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import TextIOWrapper\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1604638087780,
     "user": {
      "displayName": "Wolfgang Reuter",
      "photoUrl": "",
      "userId": "00430053052083581161"
     },
     "user_tz": -60
    },
    "id": "oGBtz_Lt39XM",
    "outputId": "a3c11f7e-26da-4e21-bc90-1c788f0f09eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text samples: ['new jersey is sometimes quiet during autumn , and it is snowy in april .', 'the united states is usually chilly during july , and it is usually freezing in november .']\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(FILE_DIR + 'small_vocab_en.csv.zip') as zf:\n",
    "    with zf.open('small_vocab_en.csv', 'r') as infile:\n",
    "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
    "        english_text = [item for sublist in reader for item in sublist]\n",
    "        print(f\"English text samples: {english_text[0:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vVS_nH-4y6J",
    "outputId": "a1e3d080-241d-4f4f-c750-ae7819dc3b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French translation text samples: [\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\", 'les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .']\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(FILE_DIR + 'small_vocab_fr.csv.zip') as zf:\n",
    "    with zf.open('small_vocab_fr.csv', 'r') as infile:\n",
    "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
    "        french_text = [item for sublist in reader for item in sublist]\n",
    "        print(f\"French translation text samples: {french_text[0:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5y0rPhaH66T"
   },
   "source": [
    "### TODO: Data exploration -> calculate and print some statistics on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text\n",
      "============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137860 entries, 0 to 137859\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    137860 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>129174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>he dislikes bananas and oranges .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text\n",
       "count                              137860\n",
       "unique                             129174\n",
       "top     he dislikes bananas and oranges .\n",
       "freq                                    3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. sentence length: 102\n",
      "Min. sentence length: 13\n",
      "Number of words: 200\n",
      "Number of duplicated sentences: 8686\n"
     ]
    }
   ],
   "source": [
    "# Load text to dataframes\n",
    "df_en = pd.DataFrame(data=english_text, columns=[\"text\"], dtype=str)\n",
    "df_fr = pd.DataFrame(data=french_text, columns=[\"text\"], dtype=str)\n",
    "\n",
    "# Show statistics\n",
    "print(\"English Text\")\n",
    "print(\"============\")\n",
    "display(df_en.info())\n",
    "display(df_en.describe())\n",
    "print(f\"Max. sentence length: {max(df_en.text.str.len())}\")\n",
    "print(f\"Min. sentence length: {min(df_en.text.str.len())}\")\n",
    "results_en = collections.Counter()\n",
    "df_en.text.str.replace(\"[^a-zA-z ]\", \"\").str.split(\" \").apply(results_en.update)\n",
    "print(f\"Number of words: {len(results_en.keys())}\")\n",
    "english_text_unique = set(english_text)\n",
    "print(f\"Number of duplicated sentences: {len(english_text) - len(english_text_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "French Text\n",
      "=============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137860 entries, 0 to 137859\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    137860 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>120806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>elle aime les bananes , les mangues et citrons verts .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          text\n",
       "count                                                   137860\n",
       "unique                                                  120806\n",
       "top     elle aime les bananes , les mangues et citrons verts .\n",
       "freq                                                         6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. sentence length: 114\n",
      "Min. sentence length: 13\n",
      "Number of unique words: 340\n",
      "Number of duplicated sentences: 17054\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrench Text\")\n",
    "print(\"=============\")\n",
    "display(df_fr.info())\n",
    "display(df_fr.describe())\n",
    "print(f\"Max. sentence length: {max(df_fr.text.str.len())}\")\n",
    "print(f\"Min. sentence length: {min(df_fr.text.str.len())}\")\n",
    "results_fr = collections.Counter()\n",
    "df_fr.text.str.replace(\"[^a-zA-z ]\", \"\").str.split(\" \").apply(results_fr.update)\n",
    "print(f\"Number of unique words: {len(results_fr.keys())}\")\n",
    "# check doubled english sentences\n",
    "french_text_unique = set(french_text)\n",
    "print(f\"Number of duplicated sentences: {len(french_text) - len(french_text_unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ8zEtacK08P"
   },
   "source": [
    "#### Tokenizing the data\n",
    "\n",
    "Since the input data to neural networks needs to be in numerical format, we first turn each sentence into a sequence of word ids using `Tokenizer` function from Keras. Word ids are numerical presentations for words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6s5iwnMKzT4",
    "outputId": "a9e7c9c1-f60c-4514-fdd7-fda9be0f2490"
   },
   "outputs": [],
   "source": [
    "def tokenize(x: List[str]):\n",
    "  \"\"\"Tokenizes sentences into word ids.\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "\n",
    "  \"\"\"\n",
    "  tokenizer=Tokenizer()\n",
    "  tokenizer.fit_on_texts(x)\n",
    "  t=tokenizer.texts_to_sequences(x)\n",
    "  return t, tokenizer\n",
    "\n",
    "if DEBUG:\n",
    "    # Tokenize Example output\n",
    "    text_sentences = [\n",
    "        'The quick brown fox jumps over the lazy dog .',\n",
    "        'By Jove , my quick study of lexicography won a prize .',\n",
    "        'This is a short sentence .']\n",
    "    text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "    print(text_tokenizer.word_index)\n",
    "    for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "        print('Sequence {} in x'.format(sample_i + 1))\n",
    "        print('  Input:  {}'.format(sent))\n",
    "        print('  Output: {}'.format(token_sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfffhgywsXMN"
   },
   "source": [
    "#### Padding Sequences\n",
    "\n",
    "To be able to batch the sequences of word ids together, all sequences need to be of the same length. Therefore, as a second preprocessing step we make sure that all the English sentences have the same length as their respective French translations using `pad_sequence` function form Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsB7MNttVYVS",
    "outputId": "cc3f9dbd-26fc-4ccb-ab7b-290032404415"
   },
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    padding=pad_sequences(x,padding='post',maxlen=length)\n",
    "    return padding\n",
    "\n",
    "if DEBUG:\n",
    "    # Pad Tokenized output\n",
    "    test_pad = pad(text_tokenized)\n",
    "    for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "        print('Sequence {} in x'.format(sample_i + 1))\n",
    "        print('  Input:  {}'.format(np.array(token_sent)))\n",
    "        print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzZF_s8TuGqw"
   },
   "source": [
    "#### Preprocessing Pipeline\n",
    "\n",
    "As mentioned in the introduction section in this excercise your focus is on building different recurrent neural network architectures. Threfore, in the following cell we provide you with a preprocessing pipeline. You can apply this function to the input data to prepare the data for using in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B158-HbGqMwd",
    "outputId": "8ca482b1-5e3b-48f3-da1d-aa6176301c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npreproc_english_sentences_all, preproc_french_sentences_all, english_tokenizer, french_tokenizer =    preprocess(english_text, french_text)\\n\\nmax_english_sequence_length = preproc_english_sentences_all.shape[1]\\nmax_french_sequence_length = preproc_french_sentences_all.shape[1]\\nenglish_vocab_size = len(english_tokenizer.word_index)\\nfrench_vocab_size = len(french_tokenizer.word_index)\\n\\nprint(\\'Preprocessed Data Info\\')\\nprint(\"Max English sentence length:\", max_english_sequence_length)\\nprint(\"Max French sentence length:\", max_french_sequence_length)\\nprint(\"English vocabulary size:\", english_vocab_size)\\nprint(\"French vocabulary size:\", french_vocab_size)\\n\\n# Show some details\\nif DEBUG:\\n    print(preproc_english_sentences_all.shape, preproc_english_sentences[0])\\n    print(english_tokenizer.sequences_to_texts([[17],[23],[1],[44]]))\\n    print(english_tokenizer.get_config())\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(x, y, x_len=None, y_len=None):\n",
    "    \"\"\"\n",
    "    Preprocessing of input (x) and target (y), words are converted to numbers\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :param x_len: forced length of sentence\n",
    "    :param y_len: forced length of sentence\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x, x_len)\n",
    "    preprocess_y = pad(preprocess_y, y_len)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\"\"\"\n",
    "preproc_english_sentences_all, preproc_french_sentences_all, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_text, french_text)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences_all.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences_all.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Preprocessed Data Info')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n",
    "\n",
    "# Show some details\n",
    "if DEBUG:\n",
    "    print(preproc_english_sentences_all.shape, preproc_english_sentences[0])\n",
    "    print(english_tokenizer.sequences_to_texts([[17],[23],[1],[44]]))\n",
    "    print(english_tokenizer.get_config())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Info\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Remove duplicated english sentences\n",
    "df = pd.DataFrame(data=zip(english_text, french_text, ), columns=[\"english_text\", \"french_text\"])\n",
    "df = df.drop_duplicates(subset=['english_text'])\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df.english_text.to_numpy(), df.french_text.to_numpy())\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Preprocessed Data Info')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate a Test-Set for performance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116256, 15), (116256, 21, 1), (12918, 15), (12918, 21, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split it\n",
    "df_train, df_test = skms.train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# Convert it\n",
    "preproc_english_sentences_train, preproc_french_sentences_train, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df_train.english_text.to_numpy(),\n",
    "               df_train.french_text.to_numpy(),\n",
    "               x_len=max_english_sequence_length,\n",
    "               y_len=max_french_sequence_length)\n",
    "\n",
    "preproc_english_sentences_test, preproc_french_sentences_test, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(df_test.english_text.to_numpy(),\n",
    "               df_test.french_text.to_numpy(),\n",
    "               x_len=max_english_sequence_length,\n",
    "               y_len=max_french_sequence_length)\n",
    "\n",
    "preproc_english_sentences_train.shape, preproc_french_sentences_train.shape,\\\n",
    "preproc_english_sentences_test.shape, preproc_french_sentences_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_fCUxliHXwO"
   },
   "source": [
    "### Model\n",
    "\n",
    "In this section we ask you to build and train Three (optionally four) different neural networks and use them to predict the French Translation of the English sentences.\n",
    "\n",
    "Make use of the `logits_to_text()` function to transform the logits from output of your networks to a French translation. This helps you to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2CrRvXm91nhd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_to_text(fl, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn features or target labels of a neural network into text using the tokenizer\n",
    "    :param fl: Feature or label used in a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return' '.join([index_to_words[w[0]] for idx, w in enumerate(fl)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(model, X_train, X_train_print=None):\n",
    "    \"\"\"\n",
    "    Print two sentences checking the ability of the model\n",
    "    for doing translations\n",
    "    \"\"\"\n",
    "    if X_train_print == None:\n",
    "        X_train_print = X_train\n",
    "\n",
    "    pred = lambda x: x[0:1]\n",
    "    y_pred_train = model.predict(pred(X_train))\n",
    "    orig_sentence = fl_to_text(pred(X_train_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en')}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "\n",
    "    pred = lambda x: x[1:2]\n",
    "    y_pred_train = model.predict(pred(X_train))\n",
    "    orig_sentence = fl_to_text(pred(X_train_print)[0], english_tokenizer)\n",
    "    print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "    print(f\"Orig. translation:\\t\\t\\t{fl_to_text(pred(y_true_train)[0], french_tokenizer)}\")\n",
    "    pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "    print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")\n",
    "    if DO_GOOGLE_BACKTRANSLATION:\n",
    "        print(f\"Back translation of prediction:\\t\\t{gtranse.translate(pred_sentence, lang_src='fr', lang_tgt='en').lower()}\")\n",
    "        print(f\"Orig. sentence:\\t\\t\\t\\t{orig_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(model, X_test, y_true_test):\n",
    "    \"\"\"\n",
    "    Print metrics on test set\n",
    "    \"\"\"\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_test = tf.keras.metrics.Accuracy(y_true_test, y_pred_test)\n",
    "\n",
    "    print(f\"accuracy: {acc:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks used within models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(early_stopping_patience=5):\n",
    "    \"\"\"Definition of keras callbacks during model run\n",
    "    \"\"\"\n",
    "    # Tensorboard config\n",
    "    if not RUN_ON_KAGGLE:\n",
    "        # Define log directory\n",
    "        timestamp_str = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tb_log_dir = \\\n",
    "            TENSORBOARD_LOGDIR + \\\n",
    "            f\"{model.name}_\" + timestamp_str\n",
    "\n",
    "        # Define writers for custom scalar \"learning rate\"\n",
    "        file_writer = tf.summary.create_file_writer(tb_log_dir + \"/metrics\")\n",
    "        file_writer.set_as_default()\n",
    "\n",
    "    # Init callbacks\n",
    "    cb_early_stopping = kaba.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=early_stopping_patience)\n",
    "    cb_tqdm = tfa.callbacks.TQDMProgressBar()\n",
    "    cb_tensorboard = kaba.TensorBoard(\n",
    "        log_dir = tb_log_dir,\n",
    "        histogram_freq=0, update_freq='epoch',\n",
    "        write_graph=True, write_images=False)\n",
    "\n",
    "    # Set final callbacks\n",
    "    callbacks = [cb_early_stopping, cb_tqdm]\n",
    "    if LOG_2_TENSORBOARD:\n",
    "        callbacks = callbacks.append(cb_tensorboard)\n",
    "\n",
    "    return callbacks, timestamp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model_dir:str, model_fname: str):\n",
    "    \"\"\"Loads a model from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from file {model_fname}...\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(model_dir + model_fname)\n",
    "\n",
    "    # Check and set name of model\n",
    "    if model.name == None:\n",
    "        model_name = \"Noname\"\n",
    "    else:\n",
    "        model_name = model.name\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    secs_all = toc - tic\n",
    "    mins = int(secs_all / 60)\n",
    "    secs = int((secs_all - mins*60))\n",
    "    print(f\"model {model_name} loaded in {mins}m {secs}s!\")\n",
    "\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhYC7lJmxpdd"
   },
   "source": [
    "#### Implement a simple RNN network and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [],
   "source": [
    "# Model 1: simple RNN model\n",
    "def gru_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers and compile the model. (Hint: Use a GRU network followed by a TimeDistributed dense layer from tensorflow.keras.layers module)\n",
    "    # What is the input shape of your model? Which activation function do you choose for the dense layer?\n",
    "    # +++ Answers ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # Input shape: time step:  max_sentence_length\n",
    "    #              features:   english_vocab_size\n",
    "    # Activation Function: softmax (mulit-class problem per TimeDistributed dense layer output)\n",
    "    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++   \n",
    "    # Input Layer\n",
    "    inputs = Input(shape=input_shape, name=\"Input\")\n",
    "\n",
    "    # GRU layer\n",
    "    # CuDNNGRU implementation are used for fast GPU processing, right padding is a must!\n",
    "    gru_units = 256\n",
    "    rnn = GRU(\n",
    "        units=gru_units,\n",
    "        return_sequences=True,  # many to many\n",
    "        # cuDNN requirements\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        recurrent_dropout=0,\n",
    "        unroll=False,\n",
    "        use_bias=True,\n",
    "        reset_after=True,\n",
    "        name=\"RNN\"\n",
    "    )(inputs)\n",
    "\n",
    "    # Add a time distributed Dense layer\n",
    "    # That dense layer will be applied to every time step\n",
    "    logits = TimeDistributed(Dense(units=french_vocab_size), name=\"Logits\")(rnn)\n",
    "\n",
    "    # Add activation\n",
    "    output = Activation('softmax', name=\"Softmax_Activation\")(logits)\n",
    "\n",
    "    # Assemble model\n",
    "    model = Model(inputs, output, name=f\"GRU_Translator_{gru_units}\")\n",
    "\n",
    "    # Build up optimizer and loss function\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        metrics=['accuracy', met_exp.categorical_accuracy_with_variable_timestep]\n",
    "    )\n",
    "\n",
    "    # Show it\n",
    "    model.summary()\n",
    "    display(tf.keras.utils.plot_model(model))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116256, 21, 1) (116256, 21, 1) (12918, 21, 1) (12918, 21, 1)\n",
      "Model: \"GRU_Translator_256\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 21, 1)]           0         \n",
      "_________________________________________________________________\n",
      "RNN (GRU)                    (None, 21, 256)           198912    \n",
      "_________________________________________________________________\n",
      "Logits (TimeDistributed)     (None, 21, 344)           88408     \n",
      "_________________________________________________________________\n",
      "Softmax_Activation (Activati (None, 21, 344)           0         \n",
      "=================================================================\n",
      "Total params: 287,320\n",
      "Trainable params: 287,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e22f3a7e404fc39c6bdc9e30073a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|           0/1 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d209a1eb5acd4671a00d667f45de3b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/91           ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = None\n",
    "if TRAIN_GRU_MODEL:\n",
    "    with runtime_context:\n",
    "        # Clear GPU Memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # TODO: Preprocess and reshape the input to work with a basic RNN. (Hint: Use preproc_english_sentences, preproc_french_sentences and reshape function from keras)\n",
    "        max_sentence_length = max([max_english_sequence_length, max_french_sequence_length])\n",
    "        X_train = pad(preproc_english_sentences_train, max_sentence_length)\n",
    "        X_train = X_train.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_train = pad(preproc_french_sentences_train, max_sentence_length)\n",
    "        X_test = pad(preproc_english_sentences_test, max_sentence_length)\n",
    "        X_test = X_test.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_test = pad(preproc_french_sentences_test, max_sentence_length)\n",
    "        print(X_train.shape, y_true_train.shape, X_test.shape, y_true_test.shape)\n",
    "\n",
    "        # TODO: Train the network you built in rnn_model. What size do you choose for the epochs, batch_size and validation_split?\n",
    "        timestep = max_sentence_length\n",
    "        features = 1  # observations at time\n",
    "        model = gru_model(\n",
    "            (timestep, features),\n",
    "            timestep,\n",
    "            english_vocab_size,\n",
    "            french_vocab_size\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        callbacks, timestamp_str = get_callbacks()\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_true_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_VcO7xkUlJyU"
   },
   "outputs": [],
   "source": [
    "if RUN_GRU_MODEL:\n",
    "    gru_model = None\n",
    "    if TRAIN_GRU_MODEL:\n",
    "        # Save model\n",
    "        if SAVE_GRU_MODEL:\n",
    "            gru_model = model\n",
    "            model.save(MODEL_DIR + model.name + \"_\" + timestamp_str + \".hd5\")\n",
    "    else:\n",
    "        # Load model and set paras\n",
    "        max_sentence_length = max([max_english_sequence_length, max_french_sequence_length])\n",
    "        X_train = pad(preproc_english_sentences_train, max_sentence_length)\n",
    "        X_train = X_train.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_train = pad(preproc_french_sentences_train, max_sentence_length)\n",
    "        X_test = pad(preproc_english_sentences_test, max_sentence_length)\n",
    "        X_test = X_test.reshape(-1, max_sentence_length, 1)\n",
    "        y_true_test = pad(preproc_french_sentences_test, max_sentence_length)\n",
    "        print(X_train.shape, y_true_train.shape, X_test.shape, y_true_test.shape)\n",
    "        try:\n",
    "            gru_model, model_name = model_load(MODEL_DIR, GRU_MODEL_FNAME)\n",
    "            display(tf.keras.utils.plot_model(gru_model))\n",
    "        except:\n",
    "            print(f\"Couldn't load model {GRU_MODEL_FNAME}\")\n",
    "\n",
    "    # TODO: Print prediction(s) for one or more of the English sentences. Hint: Make use of the `logits_to_text` function for transforming output logits to text. \n",
    "    # Check performance\n",
    "    if gru_model != None:\n",
    "        #print_predictions(gru_model, X_train)\n",
    "        #print_performance(gru_model, X_test, y_true_test)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orig. sentence:\t\t\t\tparis is usually rainy during september but it is usually nice in january <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Orig. translation:\t\t\tpluvieux enneigée les favori l' pamplemousse et les gèle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model translation (prediction):\t\tunis est est jamais en en mais est il il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Orig. sentence:\t\t\t\tmy favorite fruit is the pear but his favorite is the grapefruit <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Orig. translation:\t\t\tles paris inde est jamais relaxant en avril et il est généralement juillet en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model translation (prediction):\t\tunis préféré préféré est est la en mais mais est est parfois <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported via metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_external(y_true, y_pred, include_prediction=False):\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    id_to_prediction = {}\n",
    "    id_to_references = {}\n",
    "\n",
    "    id_to_prediction[\"1\"] = y_pred\n",
    "    id_to_references[\"1\"] = [y_true]\n",
    "\n",
    "    metrics = {}\n",
    "    for metric in [met.BLEU(1), met.ROUGE()]:\n",
    "        metric_name_to_value = metric.calculate(id_to_prediction, id_to_references)\n",
    "        metrics.update(metric_name_to_value)\n",
    "    return (metrics, id_to_prediction) if include_prediction else metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orig. sentence:\t\t\t\the likes limes busy and grapes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Orig. translation:\t\t\tpluvieux enneigée les favori l' pamplemousse et les gèle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model translation (prediction):\t\tpluvieux les les les les les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data to check\n",
    "pred = lambda x: x[0:1]\n",
    "y_pred_train = model.predict(pred(X_train))\n",
    "orig_sentence = fl_to_text(pred(X_train)[0], english_tokenizer)\n",
    "orig_translat = fl_to_text(pred(y_true_train)[0], french_tokenizer)\n",
    "pred_sentence = logits_to_text(y_pred_train[0], french_tokenizer)\n",
    "print(f\"\\nOrig. sentence:\\t\\t\\t\\t{orig_sentence}\")\n",
    "print(f\"Orig. translation:\\t\\t\\t{orig_translat}\")\n",
    "print(f\"Model translation (prediction):\\t\\t{pred_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he likes limes busy and grapes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "pluvieux les les les les les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'src.models.metrics' has no attribute 'BLEU'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e5820be7545f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluate_external(\n\u001b[1;32m      2\u001b[0m     \u001b[0morig_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-25-ace109dd3dbc>\u001b[0m in \u001b[0;36mevaluate_external\u001b[0;34m(y_true, y_pred, include_prediction)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBLEU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROUGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmetric_name_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_references\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name_to_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'src.models.metrics' has no attribute 'BLEU'"
     ]
    }
   ],
   "source": [
    "evaluate_external(\n",
    "    orig_sentence,\n",
    "    pred_sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own implementation (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as ntbs\n",
    "import rouge_score.rouge_scorer as rouge_scorer\n",
    "\n",
    "def evaluate(c_true: dict, c_pred: dict, verbose:int = 0) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the performance of the model\n",
    "    Metrics are: ROUGE-L and BLEU-1\n",
    "    Arguments:\n",
    "        c_true:  Dictionary containing all given captions per picture\n",
    "                 key of the dictionary is the picture name\n",
    "        c_pred:  Dictionary with one predicted caption per picture\n",
    "                 key of the dictionary is the picture name\n",
    "        verbose: Show insides regarding the verbose level\n",
    "                 Levels:\n",
    "                     0: show no insides\n",
    "                     1: show ROUGE_L-Recall and BLEU-1-Precision lists\n",
    "                     2: show internal data structures during run\n",
    "                     3: show data inside loops\n",
    "    Returns:\n",
    "        ROUGE-L recall and BLEU-1 precision as float values within a tuple:\n",
    "        Index:\n",
    "            0: ROUGE-L recall\n",
    "            1: BLEU-1 precision\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    ROUGE_L_INDI = \"rougeL\"\n",
    "    RECALL_INDEX = 1  # ROUGE-L score index\n",
    "    WEIGTHS_1_GRAM = (1, 0, 0, 0)  # BLEU-1 config\n",
    "    rouge_l_rec = []\n",
    "    bleu_1_prec = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "    crlf2 = lambda x: '\\n' if x > 2 else ''\n",
    "\n",
    "    # Calc scores\n",
    "    for k in c_true.keys():\n",
    "        references = c_true[k]\n",
    "        if len(references) > 0:\n",
    "            candidate = candidates[k]  # one per picture\n",
    "\n",
    "            # Calc ROUGE-L recall and BLEU-1 references\n",
    "            rrec = []  # ROUGE-L recalls temp\n",
    "            bref = []  # BLEU-1 references temp\n",
    "            for reference in references:\n",
    "                # Calc ROUGE-L recall\n",
    "                score = scorer.score(reference, candidate)[ROUGE_L_INDI][RECALL_INDEX]\n",
    "                rrec.append(score)\n",
    "                if verbose >= 3:\n",
    "                    print(f\"Rouge-L-Recall: pred. caption '{candidate}'\\ttrue caption '{reference:<30}':\\t{score}\")\n",
    "\n",
    "                # BLEU-1 reference transformation\n",
    "                bref.append(reference.split(\" \"))\n",
    "\n",
    "            # ROUGE-L max value selection\n",
    "            r_score = max(rrec)\n",
    "            rouge_l_rec.append(r_score)\n",
    "            if verbose >= 2:\n",
    "                print(f\"Rouge-L-Recalls pred. caption '{candidate}': {rrec} -> {r_score}{crlf2(verbose)}\")\n",
    "\n",
    "            # Calc BlEU-1 precision\n",
    "            b_score = ntbs.sentence_bleu(bref, candidate.split(\" \"), weights=WEIGTHS_1_GRAM)\n",
    "            bleu_1_prec.append(b_score)\n",
    "            if verbose >= 3:\n",
    "                print(f\"List of BLEU-1 true captions: {bref}\")\n",
    "            if verbose >= 2:\n",
    "                print(f\"BLEU-1-Precision pred. caption: '{candidate}': {b_score}\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"Reference caption for image {k} wrong\")\n",
    "\n",
    "    # Prepare results\n",
    "    rouge_l_rec_score = sum(rouge_l_rec)/len(rouge_l_rec)\n",
    "    bleu_1_prec_score = sum(bleu_1_prec)/len(bleu_1_prec)\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(f\"ROUGE-L-Recalls:\\n{rouge_l_rec} -> {rouge_l_rec_score}\")\n",
    "        print(f\"\\nBLEU-1-Precisions:\\n{bleu_1_prec} -> {bleu_1_prec_score}\")\n",
    "\n",
    "    return rouge_l_rec_score, bleu_1_prec_score\n",
    "\n",
    "\n",
    "# Test it\n",
    "references = dict({\n",
    "    \"picture1.jpg\": [\"police killed the gunman\", \"police kills the gunman\", \"police has killed the gunman\"], \n",
    "    \"picture2.jpg\": [\"girl is playing with a doll\", \"little girl holds a doll\", \"blond girl is playing\"],\n",
    "    \"picture3.jpg\": [\"car is standing on a lot\", \"car before a building \", \"car is parking near a house\"]\n",
    "})\n",
    "candidates = dict({\n",
    "    \"picture1.jpg\": \"police kill the gunman\",\n",
    "    \"picture2.jpg\": \"girl is playing\",\n",
    "    \"picture3.jpg\": \"near a building car is waiting\"\n",
    "})\n",
    "assert evaluate(references, candidates, verbose=0) == (0.6666666666666666, 0.7666215479690409)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'picture0.jpg': [\"pluvieux enneigée les favori l' pamplemousse et les gèle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\"],\n",
       " 'picture1.jpg': ['les paris inde est jamais relaxant en avril et il est généralement juillet en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture2.jpg': ['new est merveilleux au mois de hiver et il est jamais calme en printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture3.jpg': ['unis états est jamais agréable en mars et il est jamais elle en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture4.jpg': [\"la chaux est leur préféré fruit moins mais l' pamplemousse est chaud moins fruit <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'picture0.jpg': 'pluvieux les les les les les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture1.jpg': 'paris paris inde est est jamais en en mais il il il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture2.jpg': 'unis est en en en et est est il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture3.jpg': 'unis états est est jamais est en en est <PAD> il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture4.jpg': 'paris pendant est en préféré moins moins est est la en en mais il <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE-L-Recalls:\n",
      "[0.6521739130434783, 0.5652173913043478, 0.5238095238095238, 0.6363636363636364, 0.5652173913043478] -> 0.5885563711650668\n",
      "\n",
      "BLEU-1-Precisions:\n",
      "[0.7142857142857143, 0.6666666666666666, 0.5714285714285714, 0.7142857142857143, 0.6666666666666666] -> 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5885563711650668, 0.6666666666666666)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "n_ds = 5\n",
    "y_pred_train = model.predict(X_train[0:n_ds])\n",
    "references = dict()\n",
    "candidates = dict()\n",
    "for i in range(n_ds):\n",
    "    references[f\"picture{i}.jpg\"] = [fl_to_text(y_true_train[i], french_tokenizer)]\n",
    "    candidates[f\"picture{i}.jpg\"] = logits_to_text(y_pred_train[i], french_tokenizer)\n",
    "display(references, candidates)\n",
    "print()\n",
    "\n",
    "# Run evaluation\n",
    "evaluate(references, candidates, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own implementation (metrics.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'picture0.jpg': [\"pluvieux enneigée les favori l' pamplemousse et les gèle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\"],\n",
       " 'picture1.jpg': ['les paris inde est jamais relaxant en avril et il est généralement juillet en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture2.jpg': ['new est merveilleux au mois de hiver et il est jamais calme en printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture3.jpg': ['unis états est jamais agréable en mars et il est jamais elle en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'],\n",
       " 'picture4.jpg': [\"la chaux est leur préféré fruit moins mais l' pamplemousse est chaud moins fruit <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'picture0.jpg': 'pluvieux les les les les les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture1.jpg': 'paris paris inde est est jamais en en mais il il il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture2.jpg': 'unis est en en en et est est il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture3.jpg': 'unis états est est jamais est en en est <PAD> il parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'picture4.jpg': 'paris pendant est en préféré moins moins est est la en en mais il <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE-L-Recalls:\n",
      "[0.6521739130434783, 0.5652173913043478, 0.5238095238095238, 0.6363636363636364, 0.5652173913043478] -> 0.5885563711650668\n",
      "\n",
      "BLEU-1-Precisions:\n",
      "[0.7142857142857143, 0.6666666666666666, 0.5714285714285714, 0.7142857142857143, 0.6666666666666666] -> 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5885563711650668, 0.6666666666666666)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "n_ds = 5\n",
    "y_pred_train = model.predict(X_train[0:n_ds])\n",
    "references = dict()\n",
    "candidates = dict()\n",
    "for i in range(n_ds):\n",
    "    references[f\"picture{i}.jpg\"] = [fl_to_text(y_true_train[i], french_tokenizer)]\n",
    "    candidates[f\"picture{i}.jpg\"] = logits_to_text(y_pred_train[i], french_tokenizer)\n",
    "display(references, candidates)\n",
    "print()\n",
    "\n",
    "# Run evaluation\n",
    "met.evaluate(references, candidates, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von Kopie von Translation.ipynb",
   "provenance": [
    {
     "file_id": "1J0WAJBVzswzQPrrxyw0i-l1s1kjrSs5q",
     "timestamp": 1618213242247
    },
    {
     "file_id": "1TCG0XYY0Z9TQbpPTFrwJ69T5saN7dInb",
     "timestamp": 1604919010084
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
