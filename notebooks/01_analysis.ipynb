{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: nltk in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (3.5)\n",
      "Requirement already satisfied: absl-py in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: numpy in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from rouge-score) (1.19.2)\n",
      "Requirement already satisfied: regex in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (4.59.0)\n",
      "Requirement already satisfied: click in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from nltk->rouge-score) (0.15.1)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.0.4\n",
      "Requirement already satisfied: pycocoevalcap in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from pycocoevalcap) (2.0.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (0.29.22)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (49.2.1.post20200807)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (7.2.0)\n",
      "Requirement already satisfied: six in /home/ths/miniconda3/envs/aida/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install rouge-score\n",
    "!pip install rouge-score\n",
    "!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "#import tf_tf_rouge_l #rouge_l_tensorflow\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU-1 (see 1-gram below)\n",
    "### Python implementation wihtin NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence BLEU Score: Perfect match\n",
      "1.0\n",
      "\n",
      "Sentence BLEU Score: Individual N-GRAM Score\n",
      "Individual 1-gram - BLEU-1: 1.000000\n",
      "Individual 2-gram - BLEU-2: 1.000000\n",
      "Individual 3-gram - BLEU-3: 1.000000\n",
      "Individual 4-gram - BLEU-4: 1.000000\n",
      "\n",
      "Sentence BLEU Score: Cumulative N-GRAM Score\n",
      "1.0547686614863434e-154\n",
      "\n",
      "Corpus (more than one reference sentence) BLEU Score\n",
      "9.280167055464391e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ths/miniconda3/envs/aida/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ths/miniconda3/envs/aida/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "import nltk.translate as ntrans\n",
    "import nltk.translate.bleu_score as ntbs\n",
    "\n",
    "print(\"Sentence BLEU Score: Perfect match\")\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = ntbs.sentence_bleu(reference, candidate)\n",
    "print(score)\n",
    "\n",
    "print(\"\\nSentence BLEU Score: Individual N-GRAM Score\")\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual 1-gram - BLEU-1: %f' % ntbs.sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram - BLEU-2: %f' % ntbs.sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram - BLEU-3: %f' % ntbs.sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram - BLEU-4: %f' % ntbs.sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n",
    "\n",
    "print(\"\\nSentence BLEU Score: Cumulative N-GRAM Score\")\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = ntbs.sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)\n",
    "\n",
    "print(\"\\nCorpus (more than one reference sentence) BLEU Score\")\n",
    "references = [[['this', 'is', 'a', 'small', 'test'], ['this', 'is', 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = ntbs.corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([['police', 'killed', 'the', 'gunman'], ['police', 'kill', 'the', 'gunman']],\n",
       "  ['police', 'kill', 'the', 'gunman'],\n",
       "  1.0),\n",
       " ([['police', 'killed', 'the', 'gunman'], ['police', 'kill', 'the', 'gunman']],\n",
       "  ['the', 'gunman', 'kill', 'police', ''],\n",
       "  0.8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = [\"police killed the gunman\".split(\" \"), \"police kill the gunman\".split(\" \")]\n",
    "candidate1 = \"police kill the gunman\".split(\" \")\n",
    "candidate2 = \"the gunman kill police \".split(\" \")\n",
    "score1 = ntbs.sentence_bleu(reference, candidate1, weights=(1, 0, 0, 0))\n",
    "score2 = ntbs.sentence_bleu(reference, candidate2, weights=(1, 0, 0, 0))\n",
    "\n",
    "(reference, candidate1, score1), (reference, candidate2, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 0, 'reflen': 0, 'guess': [0, 0], 'correct': [0, 0]}\n",
      "ratio: 1e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0], [[], []])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycocoevalcap.bleu.bleu as pyco_bleu\n",
    "reference = [\"police killed the gunman\"]\n",
    "candidate1 = \"police kill the gunman\"\n",
    "candidate2 = \"the gunman kill police\"\n",
    "\n",
    "bleu_scorer = pyco_bleu.BleuScorer(n=2)\n",
    "bleu_scorer.compute_score(option=\"closest\", verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflen': [4], 'testlen': 4, 'guess': [4, 3, 2, 1], 'correct': [3, 1, 0, 0]} 4\n",
      "{'reflen': [4], 'testlen': 4, 'guess': [4, 3, 2, 1], 'correct': [3, 1, 0, 0]} 4\n",
      "{'testlen': 8, 'reflen': 8, 'guess': [8, 6], 'correct': [6, 2]}\n",
      "ratio: 0.9999999998750002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7499999998125003, 0.49999999986458354]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer += (candidate1, reference)\n",
    "bleu_scorer += (candidate2, reference)\n",
    "\n",
    "score, scores = bleu_scorer.compute_score(option=\"closest\", verbose=2)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.models.metrics.categorical_accuracy_with_variable_timestep(y_true, y_pred)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Usage of metrics\n",
    "model.compile(optimizer=Adam(lr=self._learning_rate, clipnorm=5.0),\n",
    "                      loss=categorical_crossentropy_from_logits,\n",
    "                      metrics=[categorical_accuracy_with_variable_timestep])\n",
    "\"\"\"\n",
    "import src.models.metrics as met\n",
    "met.categorical_accuracy_with_variable_timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rouge1': Score(precision=0.75, recall=0.75, fmeasure=0.75),\n",
       "  'rougeL': Score(precision=0.75, recall=0.75, fmeasure=0.75)},\n",
       " {'rouge1': Score(precision=0.75, recall=0.75, fmeasure=0.75),\n",
       "  'rougeL': Score(precision=0.5, recall=0.5, fmeasure=0.5)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rouge_score.rouge_scorer as rouge_scorer\n",
    "\n",
    "reference = \"police killed the gunman\"\n",
    "candidate1 = \"police kill the gunman\"\n",
    "candidate2 = \"the gunman kill police\"\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
    "scores1 = scorer.score(reference, candidate1)\n",
    "scores2 = scorer.score(reference, candidate2)\n",
    "\n",
    "scores1, scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.75, 1.0], [0.5, 0.5], 1.0, 0.5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = [\"police killed the gunman\", \"police kill the gunman\"]\n",
    "candidate1 = \"police kill the gunman\"\n",
    "candidate2 = \"the gunman kill police\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "for r in reference:\n",
    "    scores1.append(scorer.score(r, candidate1)[\"rougeL\"][0])\n",
    "\n",
    "for r in reference:\n",
    "    scores2.append(scorer.score(r, candidate2)[\"rougeL\"][0])\n",
    "scores1, scores2, max(scores1), max(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1[0][\"rougeL\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
