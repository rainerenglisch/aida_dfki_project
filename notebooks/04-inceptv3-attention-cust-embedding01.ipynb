{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"execution_count":319,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.7/site-packages (0.0.4)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (0.12.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.19.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport datetime as dt\nimport sys\nimport nltk.translate.bleu_score as ntbs  # BLEU scoring basics\nimport rouge_score.rouge_scorer as rouge_scorer  # ROUGE scoring basics\nsys.path.insert(0, '../input/aida-metrics-functions')  # custom metrics implementation\nimport metrics as met  # custom metrics BLUE-1 pecsision and ROUGE-L recall\nimport json","metadata":{"trusted":true},"execution_count":320,"outputs":[]},{"cell_type":"code","source":"#Import all the required libraries\n\n# System Libraries \nimport os, glob\nfrom glob import glob\nimport pickle\nfrom sys import getsizeof\n\n# Date and Time \nimport datetime,time\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\nimport collections, random, re\nfrom collections import Counter\n\n\n# Model building \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n\n#Read/Display  images\nfrom skimage import io\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# tensorflow Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import plot_model\n\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":321,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#Import the dataset and read the image into a seperate variable\nINPUT_PATH = \"../input/flickr8k/\"\nIMAGE_PATH = INPUT_PATH+'Images/'\nCAPTIONS_FILE = INPUT_PATH+'captions.txt'\nOUTPUT_IMAGE_PATH = \"../working/Image/\"","metadata":{"trusted":true},"execution_count":322,"outputs":[]},{"cell_type":"code","source":"all_imgs = glob(IMAGE_PATH + '*.jpg')","metadata":{"trusted":true},"execution_count":323,"outputs":[]},{"cell_type":"markdown","source":"1. Image are of different sizes, will resize them to (299, 299)","metadata":{}},{"cell_type":"code","source":"# Create a dataframe which summarizes the image, path & captions as a dataframe\n# Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples.\n\ncaptions_df = pd.read_csv(CAPTIONS_FILE)\n\npd.set_option('display.max_colwidth',-1) # Set the max column width to see the complete caption\nprint(captions_df.shape)\ncaptions_df.head()","metadata":{"trusted":true},"execution_count":324,"outputs":[{"name":"stdout","text":"(40455, 2)\n","output_type":"stream"},{"execution_count":324,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                                                    caption  \n0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n1  A girl going into a wooden building .                                     \n2  A little girl climbing into a wooden playhouse .                          \n3  A little girl climbing the stairs to her playhouse .                      \n4  A little girl in a pink dress going into a wooden cabin .                 ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playhouse .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a wooden cabin .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"captions_df.nunique()","metadata":{"trusted":true},"execution_count":325,"outputs":[{"execution_count":325,"output_type":"execute_result","data":{"text/plain":"image      8091 \ncaption    40201\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Reading captions file\nfile = open(CAPTIONS_FILE,'rb')\ncaptions_txt = file.read().decode('utf-8')\nfile.close()\nimg_cap_corpus=captions_txt.split('\\n')\nimg_cap_corpus.pop(0)## poping first line names of columns (image,caption)","metadata":{"trusted":true},"execution_count":326,"outputs":[{"execution_count":326,"output_type":"execute_result","data":{"text/plain":"'image,caption'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Create a dataframe which summarizes the image, path & captions as a dataframe","metadata":{}},{"cell_type":"code","source":"datatxt = []\nfor line in img_cap_corpus:\n    col = line.split(',')# Seperates columns image and caption\n   \n    if len(col)==1:\n        continue\n    w = col[0].split(\"_\") # seperating image filename to extract the id \n   \n    w[1] = IMAGE_PATH + col[0] # saving complete path of image file for building model\n    datatxt.append(w + [col[1].lower()])\n\ndf= pd.DataFrame(datatxt,columns=['ID','Path','Captions'])\n#df = df.reindex(columns =['ID','Path','Captions'])\nuni_filenames= np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df.ID.values).values())","metadata":{"trusted":true},"execution_count":327,"outputs":[{"name":"stdout","text":"The number of unique file names : 8091\nThe distribution of the number of captions for each image:\n","output_type":"stream"},{"execution_count":327,"output_type":"execute_result","data":{"text/plain":"Counter({5: 8091})"},"metadata":{}}]},{"cell_type":"code","source":"all_img_id= df.ID#store all the image id \nall_img_vector= df.Path#store all the image path here\nannotations= df.Captions#store all the captions here\n\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(set(all_img_vector))))\nprint(annotations[:10])","metadata":{"trusted":true},"execution_count":328,"outputs":[{"name":"stdout","text":"Total captions present in the dataset: 40455\nTotal images present in the dataset: 8091\n0    a child in a pink dress is climbing up a set of stairs in an entry way .              \n1    a girl going into a wooden building .                                                 \n2    a little girl climbing into a wooden playhouse .                                      \n3    a little girl climbing the stairs to her playhouse .                                  \n4    a little girl in a pink dress going into a wooden cabin .                             \n5    a black dog and a spotted dog are fighting                                            \n6    a black dog and a tri-colored dog playing with each other on the road .               \n7    a black dog and a white dog with brown spots are staring at each other in the street .\n8    two dogs of different breeds looking at each other on the road .                      \n9    two dogs on pavement moving toward each other .                                       \nName: Captions, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Create the vocabulary & the counter for the captions","metadata":{}},{"cell_type":"code","source":"uni_filenames = np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df.ID.values).values())","metadata":{"trusted":true},"execution_count":329,"outputs":[{"name":"stdout","text":"The number of unique file names : 8091\nThe distribution of the number of captions for each image:\n","output_type":"stream"},{"execution_count":329,"output_type":"execute_result","data":{"text/plain":"Counter({5: 8091})"},"metadata":{}}]},{"cell_type":"code","source":"def split_sentence(sentence):\n    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n\ndef generate_vocabulary(captions):\n      \n    words = []\n\n    for sentence in captions:\n        sent_words = split_sentence(sentence)\n        for word in sent_words: \n            words.append(word)\n    return sorted(words)\n    \nvocab = generate_vocabulary(df.Captions)","metadata":{"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"markdown","source":"#### Creating captions Counter","metadata":{}},{"cell_type":"code","source":"## Creating vocabulary of all words present in captions\n\nvocabulary =  Counter(vocab)\n\ndf_word = pd.DataFrame.from_dict(vocabulary, orient='index')\n#display(df_word.head(2))\n\ndf_word = df_word.sort_values(by=[0],ascending=False).reset_index()\n#display(df_word.head(2))\ndf_word =df_word.rename(columns={'index':'word', 0:'count'})\n#display(df_word.head(2))\n","metadata":{"trusted":true},"execution_count":331,"outputs":[]},{"cell_type":"code","source":"df_word.head()","metadata":{"trusted":true},"execution_count":332,"outputs":[{"execution_count":332,"output_type":"execute_result","data":{"text/plain":"  word  count\n0  a    61436\n1  in   18193\n2  the  17677\n3  on   10369\n4  is   9069 ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a</td>\n      <td>61436</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>in</td>\n      <td>18193</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the</td>\n      <td>17677</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>on</td>\n      <td>10369</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>is</td>\n      <td>9069</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(f'Vocab size = {len(vocabulary)}')","metadata":{"trusted":true},"execution_count":333,"outputs":[{"name":"stdout","text":"Vocab size = 8315\n","output_type":"stream"}]},{"cell_type":"code","source":"#Visualise the top 30 occuring words in the captions\n\ndef plthist(index,words,count, title=\"The top 30 most frequently appearing words\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(words,count,color='maroon', width =0.4)\n    plt.xlabel(\"Words\",  fontsize=20) \n    plt.ylabel(\"Word Count\",rotation=90,fontsize=20) \n   # plt.yticks(fontsize=20)\n    plt.xticks(index,words,rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n    \nwords = list(df_word[:30].word)\n\ncount =list(df_word['count'][:30])\nplthist(list(range(0,30)),words,count)","metadata":{"trusted":true},"execution_count":334,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### 5.Create a list which contains all the captions & path","metadata":{}},{"cell_type":"code","source":"#Create a list which contains all the captions\n\n#adding  the <start> & <end> token to all the captions \ndf['Captions']=df.Captions.apply(lambda x : f\"<start> {x} <end>\")\nannotations = df.Captions\n# Find max length of sequence\nmax_length = max(df.Captions.apply(lambda x : len(x.split())))\n\n#Create a list which contains all the path to the images\nunique_img_path= sorted(set(all_img_vector))#write your code here\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_vector)))\nprint(\"Unique images present in the dataset: \" + str(len(unique_img_path)))","metadata":{"trusted":true},"execution_count":335,"outputs":[{"name":"stdout","text":"Total captions present in the dataset: 40455\nTotal images present in the dataset: 40455\nUnique images present in the dataset: 8091\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Pre-Processing the captions\n2.1 Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.2 Replace all other words with the unknown token \"UNK\" .\n\n2.3 Create word-to-index and index-to-word mappings.\n\n2.4 Pad all sequences to be the same length as the longest one.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Create the tokenizer\n Creating captions Counter, and dataframe of top 5000 frequently used words, for observation.","metadata":{}},{"cell_type":"code","source":"vocabulary =  Counter(vocab)\nlen(vocabulary)","metadata":{"trusted":true},"execution_count":336,"outputs":[{"execution_count":336,"output_type":"execute_result","data":{"text/plain":"8315"},"metadata":{}}]},{"cell_type":"code","source":"len(annotations), len(all_img_vector)","metadata":{"trusted":true},"execution_count":337,"outputs":[{"execution_count":337,"output_type":"execute_result","data":{"text/plain":"(40455, 40455)"},"metadata":{}}]},{"cell_type":"code","source":"def data_limiter(all_captions,all_img_vector):\n    img_captions, img_name_vector = shuffle(all_captions,all_img_vector,random_state=42)\n   # img_captions = img_captions[:num]\n   # img_name_vector = img_name_vector[:num]\n    return img_captions,img_name_vector\n\nimg_captions,all_img_vector = data_limiter(annotations,all_img_vector)","metadata":{"trusted":true},"execution_count":338,"outputs":[]},{"cell_type":"code","source":"len(img_captions), len(all_img_vector)","metadata":{"trusted":true},"execution_count":339,"outputs":[{"execution_count":339,"output_type":"execute_result","data":{"text/plain":"(40455, 40455)"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_captions(top_cap,captions):\n    special_chars = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_freq_words,\n                                                  oov_token=\"UNK\",\n                                                  filters=special_chars)\n    tokenizer.fit_on_texts(captions)\n    \n    # Adding PAD to tokenizer list\n    tokenizer.word_index['PAD'] = 0\n    tokenizer.index_word[0] = 'PAD'   \n   \n    return tokenizer  \n    ","metadata":{"trusted":true},"execution_count":340,"outputs":[]},{"cell_type":"code","source":"top_freq_words = 5000\ntokenizer = tokenize_captions(top_freq_words,img_captions)\n# Pad each vector to the max_length of the captions ^ store it to a vairable\n\n# Create the tokenized vectors\ncap_seqs = tokenizer.texts_to_sequences(img_captions)\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))\nprint(cap_vector[:5])","metadata":{"trusted":true},"execution_count":341,"outputs":[{"name":"stdout","text":"The shape of Caption vector is :(40455, 35)\n[[   3    2   55 1591  593    8 3971    2  224  123    2 1455    4    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0]\n [   3   14   28   32  499    7    6 1667  172    4    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0]\n [   3    2   12    5  166    8   40    7    2   83   91   88   24    6\n   458   75    6 3972    4    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0]\n [   3    2  882   15   10    8   33    7    6   42    4    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0]\n [   3    2   43  378  728    4    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Maximum length of sequence \nmax_length = max([len(cap) for cap in cap_seqs])\nprint(\"Shape of caption vector :\", cap_vector.shape, )\nprint(\"Maximium length of sequence = \", max_length)","metadata":{"trusted":true},"execution_count":342,"outputs":[{"name":"stdout","text":"Shape of caption vector : (40455, 35)\nMaximium length of sequence =  35\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create word-to-index and index-to-word mappings.\ndef print_word_2_index(word):\n    print(\"Word = {}, index = {}\".format(word, tokenizer.word_index[word]))\n\n          \nprint( \"Word 2 index mapping\")\nprint_word_2_index(\"<start>\")\nprint_word_2_index(\"PAD\")","metadata":{"trusted":true},"execution_count":343,"outputs":[{"name":"stdout","text":"Word 2 index mapping\nWord = <start>, index = 3\nWord = PAD, index = 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create word-to-index and index-to-word mappings.\ndef print_index_2_word(index):\n    print(\"Index = {}, Word = {}\".format(index, tokenizer.index_word[index]))\n\n          \nprint( \"Index 2 word mapping\")\nprint_index_2_word(5)\nprint_index_2_word(4999)","metadata":{"trusted":true},"execution_count":344,"outputs":[{"name":"stdout","text":"Index 2 word mapping\nIndex = 5, Word = in\nIndex = 4999, Word = salvar\n","output_type":"stream"}]},{"cell_type":"code","source":"# Word count of your tokenizer to see the Top 30 occuring words after text processing\nword_count = tokenizer.word_counts\nimport operator\nword_count = sorted(word_count.items(), key=operator.itemgetter(1),reverse=True)\nfor k,v in word_count[:30]:\n    print (k,v)\n    ","metadata":{"trusted":true},"execution_count":345,"outputs":[{"name":"stdout","text":"a 61436\n<start> 40455\n<end> 40455\nin 18193\nthe 17677\non 10369\nis 9069\nand 8066\ndog 7958\nwith 7308\nman 7172\nof 6504\ntwo 5550\nwhite 3728\nblack 3641\nboy 3534\nare 3365\nwoman 3320\ngirl 3284\nto 3008\nwearing 2917\npeople 2837\nat 2820\nwater 2683\nyoung 2610\nred 2571\nbrown 2484\nan 2358\nhis 2255\nblue 2136\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Pre-processing the images","metadata":{}},{"cell_type":"markdown","source":"## Create the train & test data ","metadata":{}},{"cell_type":"code","source":"image_train, image_test, captions_train, captions_test = train_test_split(all_img_vector,cap_vector, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":346,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\n\nfor img,cap in zip(all_img_vector,cap_vector):\n    #print(img,cap)\n    img_to_cap_vector[img].append(cap)","metadata":{"trusted":true},"execution_count":347,"outputs":[]},{"cell_type":"code","source":"list(img_to_cap_vector.items())[2], len(img_to_cap_vector)*5, tokenizer.index_word[3], tokenizer.index_word[4], tokenizer.index_word[0]\n#, print_index_2_word[4], print_index_2_word[0]","metadata":{"trusted":true},"execution_count":348,"outputs":[{"execution_count":348,"output_type":"execute_result","data":{"text/plain":"(('../input/flickr8k/Images/2053006423_6adf69ca67.jpg',\n  [array([   3,    2,   12,    5,  166,    8,   40,    7,    2,   83,   91,\n            88,   24,    6,  458,   75,    6, 3972,    4,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0], dtype=int32),\n   array([  3,   2,  12,  91,  24,   6, 458,  13,   2, 128, 108,  40,   7,\n            2,  83,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n   array([  3,  12,  11, 497,  36,   7,  22, 166,   9, 578, 639,  40,  11,\n          176,   7,  30, 162,  91,  88, 244,   2, 108,   4,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n   array([  3,   2, 414,  12,  40,   7,   2,  83, 416,   2, 953,   4,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n   array([  3, 414,  12,   7, 194, 256,  11, 158,  21,  94,   4,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)]),\n 40455,\n '<start>',\n '<end>',\n 'PAD')"},"metadata":{}}]},{"cell_type":"code","source":"def vector_to_sentence(caplist):\n    captions_word =[]\n    #print(caplist)\n    for captions in caplist:\n        #print(captions)\n        list_caption = list(captions)\n\n        captions_word.append(' '.join(tokenizer.index_word[i] for i in list_caption))\n    return captions_word\n        ","metadata":{"trusted":true},"execution_count":349,"outputs":[]},{"cell_type":"markdown","source":"## 4. Create the train & test data ","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\n# Create training and validation sets using an 80-20 split randomly.\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\nprint(len(img_keys))\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\nprint(len(img_name_train_keys), len(img_name_val_keys))","metadata":{"trusted":true},"execution_count":350,"outputs":[{"name":"stdout","text":"8091\n6472 1619\n","output_type":"stream"}]},{"cell_type":"code","source":"image_train = []\ncaptions_train = []\nfor imgt in img_name_train_keys:\n    captions_len = len(img_to_cap_vector[imgt])\n    #print(captions_len)\n    image_train.extend([imgt] * captions_len)\n    captions_train.extend(img_to_cap_vector[imgt])\n    break","metadata":{"trusted":true},"execution_count":351,"outputs":[]},{"cell_type":"code","source":"#4.1 Combine both images & captions to create the train & test dataset using tf.data.Dataset API. \n#Create the train-test spliit using 80-20 ratio & random state = 42\ntf.random.set_seed(42)\n# Create training and validation sets using an 80-20 split randomly.\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n#print(img_name_train_keys, img_name_val_keys)\nimage_train = []\ncaptions_train = []\nfor imgt in img_name_train_keys:\n    captions_len = len(img_to_cap_vector[imgt])\n    #print(captions_len)\n    image_train.extend([imgt] * captions_len)\n    captions_train.extend(img_to_cap_vector[imgt])\n\nimage_test = []\ncaptions_test = []\nfor imgtest in img_name_val_keys:\n    captions_len = len(img_to_cap_vector[imgtest])\n    image_test.extend([imgtest] * captions_len)\n    captions_test.extend(img_to_cap_vector[imgtest])","metadata":{"trusted":true},"execution_count":352,"outputs":[]},{"cell_type":"code","source":"img_name_train_keys[:5], img_to_cap_vector[img_name_train_keys[0]]","metadata":{"trusted":true},"execution_count":353,"outputs":[{"execution_count":353,"output_type":"execute_result","data":{"text/plain":"(['../input/flickr8k/Images/3373870185_f79163fa51.jpg',\n  '../input/flickr8k/Images/2439154641_bbf985aa57.jpg',\n  '../input/flickr8k/Images/319870744_0e0138d205.jpg',\n  '../input/flickr8k/Images/2085557551_7a88d01d4e.jpg',\n  '../input/flickr8k/Images/2902486045_6298eb22ef.jpg'],\n [array([   3,    2,   15,  263, 3047,   76,  732,    9,   68,   64,    6,\n           25,  489, 1363,    4,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0], dtype=int32),\n  array([   3,    2,  263, 3678,   47,   25,    4,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0], dtype=int32),\n  array([  3,   2,  16,   9,  15, 263,   8, 284,  47,  25,   4,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n  array([  3,   2, 263,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n  array([   3,    2,   15,  263,   11,    2,   16, 2097, 1084,   21, 1086,\n           67,    6,   25,    4,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0], dtype=int32)])"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Image_train = {}, Captions_train = {}\".format(len(image_train), len(captions_train)))\n\nprint(\"Image_test = {}, Captions_test = {}\".format(len(image_test), len(captions_test)))","metadata":{"trusted":true},"execution_count":354,"outputs":[{"name":"stdout","text":"Image_train = 32360, Captions_train = 32360\nImage_test = 8095, Captions_test = 8095\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_file_name(filename):\n    return (IMAGE_PATH + filename)\n\ndef vector_to_sentence(caplist):\n    captions_word =[]\n    #print(caplist)\n    for captions in caplist:\n        #print(captions)\n        list_caption = list(captions)\n\n        captions_word.append(' '.join(tokenizer.index_word[i] for i in list_caption))\n    return captions_word","metadata":{"trusted":true},"execution_count":355,"outputs":[]},{"cell_type":"code","source":"caplist_train = []\n#for i in range(1,10):\ncaptions_word = vector_to_sentence(list(captions_train[0:30]))\nprint(captions_word[28:])\n","metadata":{"trusted":true},"execution_count":356,"outputs":[{"name":"stdout","text":"['<start> a man jumping a dirt ramp on a bmx bike <end> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD', '<start> man riding a mountain bike doing a jump in the air <end> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD']\n","output_type":"stream"}]},{"cell_type":"code","source":"DIR_IMAGE_FEATURES = '../input/aida-image-captioning/Images/'\n# Load the numpy files\ndef map_func(img_name, cap):\n    filename = DIR_IMAGE_FEATURES + img_name.decode('utf-8').split('/')[-1].replace('.jpg', '.npy')\n    img_tensor = np.load(filename)\n    return img_tensor, cap","metadata":{"trusted":true},"execution_count":357,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\ntrain_dataset = tf.data.Dataset.from_tensor_slices((image_train, captions_train))\n# Use map to load the numpy files in parallel\ntrain_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":358,"outputs":[]},{"cell_type":"code","source":"\ntest_dataset = tf.data.Dataset.from_tensor_slices((image_test, captions_test))\n# Use map to load the numpy files in parallel\ntest_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"train_img_batch, train_cap_batch = next(iter(train_dataset))\nprint(train_img_batch.shape) #(batch_size, 8*8,2048)\nprint(train_cap_batch.shape) #(batch_size, max_len)","metadata":{"trusted":true},"execution_count":360,"outputs":[{"name":"stdout","text":"(64, 64, 2048)\n(64, 35)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_img_batch, test_cap_batch = next(iter(test_dataset))\nprint(test_img_batch.shape) #(batch_size, 8*8,2048)\nprint(test_cap_batch.shape) #(batch_size, max_len)","metadata":{"trusted":true},"execution_count":361,"outputs":[{"name":"stdout","text":"(64, 64, 2048)\n(64, 35)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_img_batch[0][0], train_cap_batch[0]","metadata":{"trusted":true},"execution_count":362,"outputs":[{"execution_count":362,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(2048,), dtype=float32, numpy=\n array([0.5298742 , 0.        , 1.2619808 , ..., 0.        , 0.        ,\n        0.03653452], dtype=float32)>,\n <tf.Tensor: shape=(35,), dtype=int32, numpy=\n array([   3,   81,   17,    8, 1037,   73,   17,   21,  463,    2,   58,\n         305,    5,    6,   70,    4,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0], dtype=int32)>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\n\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64\n\nembedding_dim = 256 \nunits = 512\nvocab_size = top_freq_words + 1 #top 5,000 words +1\ntrain_num_steps = len(image_train) // BATCH_SIZE\ntest_num_steps = len(image_test) // BATCH_SIZE","metadata":{"trusted":true},"execution_count":363,"outputs":[]},{"cell_type":"markdown","source":"## Encoder","metadata":{}},{"cell_type":"markdown","source":"## Attention Model","metadata":{}},{"cell_type":"code","source":"def padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(W1, W2, v, mask):\n    matmul_W1W2 = tf.matmul(W1, W2, transpose_b=True)  # (..., seq_len_W1, seq_len_W2)\n    dimW2 = tf.cast(tf.shape(W2)[-1], tf.float32)\n    scaled_attention_logits = matmul_W1W2/ tf.math.sqrt(dimW2)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9) \n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n    output = tf.matmul(attention_weights, v)  # (..., seq_len_W1, depth_v)\n\n    return output, attention_weights","metadata":{"trusted":true},"execution_count":364,"outputs":[]},{"cell_type":"code","source":"# Subclassing tensor layer to create multilayer Attention layer to be later  used in Attention Layer\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n        self.W1 = tf.keras.layers.Dense(d_model)\n        self.W2 = tf.keras.layers.Dense(d_model)\n        self.Wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, W1, W2, mask=None):\n        batch_size = tf.shape(q)[0]\n        W1 = self.W1(W1)  # (batch_size, seq_len, d_model)\n        W2 = self.W2(W2)  # (batch_size, seq_len, d_model)\n        Wv = self.Wv(v)  # (batch_size, seq_len, d_model)\n\n        W1 = self.split_heads(W1, batch_size)  # (batch_size, num_heads, seq_len_W1, depth)\n        W2 = self.split_heads(W2, batch_size)  # (batch_size, num_heads, seq_len_W2, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(W1, W2, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                 (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n\n    def point_wise_feed_forward_network(d_model, dff):\n        return tf.keras.Sequential([\n                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n                tf.keras.layers.Dense(d_model)])  # (batch_size, seq_len, d_model)\n","metadata":{"trusted":true},"execution_count":365,"outputs":[]},{"cell_type":"markdown","source":"# Encoder/Decoder Layers","metadata":{}},{"cell_type":"code","source":"#### Creating Encoder layer subclass keras.layers\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask=None):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n        return out2","metadata":{"trusted":true},"execution_count":366,"outputs":[]},{"cell_type":"code","source":"\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"trusted":true},"execution_count":367,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding,   rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                         for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                            look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n\n        return x, attention_weights","metadata":{"trusted":true},"execution_count":368,"outputs":[]},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self,embedding_dim):\n        super(Encoder, self).__init__()\n        #Dense layer with relu activation\n        self.dense = tf.keras.layers.Dense(embedding_dim)\n        # shape after fc == (batch_size, 64, embedding_dim)\n        #self.fc = tf.keras.layers.Dense(embedding_dim)\n        #self.dropout = tf.keras.layers.Dropout(0.5,noise_shape=None,seed=None)\n        \n    def call(self, features):\n        features = self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features = tf.nn.relu(features)\n        return features","metadata":{"trusted":true},"execution_count":369,"outputs":[]},{"cell_type":"markdown","source":"## Decoder","metadata":{}},{"cell_type":"code","source":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)#build your Dense layer\n        self.W2 = tf.keras.layers.Dense(units)#build your Dense layer\n        self.V = tf.keras.layers.Dense(1)#build your final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden,1) # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n        score = self.V(attention_hidden_layer)# build your score funciton to shape: (batch_size, 8*8, units)\n        attention_weights =  tf.nn.softmax(score,axis=1)# extract your attention weights with shape: (batch_size, 8*8, 1)\n        context_vector = attention_weights * features  #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = tf.reduce_sum(context_vector,axis=1)# reduce the shape to (batch_size, embedding_dim)\n        \n\n        return context_vector, attention_weights","metadata":{"trusted":true},"execution_count":370,"outputs":[]},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units)#iniitalise your Attention model with units\n        self.embed = tf.keras.layers.Embedding(vocab_size,embed_dim) #build your Embedding layer\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units)#build your Dense layer\n        self.d2 = tf.keras.layers.Dense(vocab_size)#build your Dense layer\n        self.attention = Attention_model(self.units)\n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features,hidden)#create your context vector & attention weights from attention model\n        # embed your input to shape: (batch_size, 1, embedding_dim)\n        embed =  self.embed(x)\n        embed =  tf.concat([tf.expand_dims(context_vector,1),embed],axis=-1)# Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed)# Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        x = self.d1(output)\n        x = tf.reshape(x, (-1, x.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        x = self.d2(x) # shape : (batch_size * max_length, vocab_size)\n        \n        return x,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","metadata":{"trusted":true},"execution_count":371,"outputs":[]},{"cell_type":"code","source":"encoder=Encoder(embedding_dim)\ndecoder=Decoder(embedding_dim, units, vocab_size)","metadata":{"trusted":true},"execution_count":372,"outputs":[]},{"cell_type":"code","source":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n\nfeatures=encoder(train_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * train_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","metadata":{"trusted":true},"execution_count":373,"outputs":[{"name":"stdout","text":"Feature shape from Encoder: (64, 64, 256)\nPredcitions shape from Decoder: (64, 5001)\nAttention weights shape from Decoder: (64, 64, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()#define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True, reduction='none')#define your loss object\nmetric = tf.keras.metrics.Mean('train_loss',dtype=tf.float32)","metadata":{"trusted":true},"execution_count":374,"outputs":[]},{"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","metadata":{"trusted":true},"execution_count":375,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor,training =True)\n\n        for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n    total_loss = (loss / int(target.shape[1]))\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n    metric(loss)    \n    return loss, total_loss","metadata":{"trusted":true},"execution_count":376,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    features = encoder(img_tensor)\n    for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n            predicted_id = tf.argmax(predictions[0])\n            dec_input = tf.expand_dims([predicted_id] * target.shape[0] , 1)\n   \n    avg_loss = (loss / int(target.shape[1]))\n        \n    return loss, avg_loss","metadata":{"trusted":true},"execution_count":377,"outputs":[]},{"cell_type":"code","source":"def test_loss_cal(dataset):\n    total_loss = 0\n\n    for (batch,(img_tensor,target)) in enumerate(dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n   \n    \n    return total_loss/int(target.shape[1])","metadata":{"trusted":true},"execution_count":378,"outputs":[]},{"cell_type":"code","source":"train_loss = tf.keras.metrics.Mean('train_loss',dtype=tf.float32)","metadata":{"trusted":true},"execution_count":379,"outputs":[]},{"cell_type":"code","source":"#checkpoint_path_ckpt = \"./checkpoints/train\"\ncheckpoint_path_ckpt = '../input/brokendown-ckpt/checkpoints/train'\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer,\n                           metrics=metric)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)","metadata":{"trusted":true},"execution_count":380,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    print('Restoring from latest checkpoint')\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n    \n    ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"trusted":true},"execution_count":381,"outputs":[{"name":"stdout","text":"Restoring from latest checkpoint\n","output_type":"stream"}]},{"cell_type":"code","source":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n    avg_train_loss=total_loss / train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()\n        \n# save at the end\nckpt_manager.save()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"def init_features(image):\n    filename = DIR_IMAGE_FEATURES + image.split('/')[-1].replace('.jpg', '.npy')\n    img_tensor_val = np.load(filename)\n    \n    features = encoder(img_tensor_val)# extract the features by passing the input to encoder\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    return features,dec_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prediction(image):\n    hidden = decoder.init_state(batch_size=1)\n    features,dec_input = init_features(image)\n    \n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,features,hidden) # get the output from decoder\n\n        #extract the predicted id(embedded value) which carries the max value\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        # print(predicted_id)\n        result.append(tokenizer.index_word[predicted_id])#map the id to the word from tokenizer and append the value to the result list\n\n        if (tokenizer.index_word[predicted_id] == \"<end>\"):\n            return result\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# captions on the validation set\nrid = np.random.randint(0, len(image_test))\nimage = image_test[rid]\nreal_caption = ' '.join([tokenizer.index_word[i] for i in captions_test[rid] if i not in [0]])\nresult = make_prediction(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BLEU-1 and ROUGE-L score","metadata":{}},{"cell_type":"markdown","source":"### Test set","metadata":{}},{"cell_type":"code","source":"actual = []\nfor i in range(0,len(image_test)-5,5):\n    temp = []\n    for j in range(i,i+5):\n        real_caption = [tokenizer.index_word[k]\n                        for k in captions_test[j] if k not in [0]]\n        temp.append(real_caption)\n    actual.append(temp)","metadata":{"trusted":true},"execution_count":382,"outputs":[]},{"cell_type":"code","source":"pred = []\nfor i in tqdm(range(0,len(image_test)-5,5)):\n    image = image_test[i]\n    result = make_prediction(image)\n    result.insert(0,'<start>')\n    pred.append(result)","metadata":{"trusted":true},"execution_count":383,"outputs":[{"name":"stderr","text":"100%|| 1618/1618 [02:17<00:00, 11.78it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"real_dict, pred_dict = {}, {}\nfor i, (reals, preds) in enumerate(zip(actual, pred)):    \n    real_dict[i] = [' '.join(x) for x in reals]\n    pred_dict[i] = ' '.join(preds) ","metadata":{"trusted":true},"execution_count":384,"outputs":[]},{"cell_type":"code","source":"rouge_l_test, bleu_1_test = met.evaluate(real_dict, pred_dict)\nprint(f\"Metrics for Testset:\\nROUGE-L recall:{rouge_l_test:>10.2%}\\nBLEU-1 precision: {bleu_1_test:>7.2%}\")","metadata":{"trusted":true},"execution_count":385,"outputs":[{"name":"stdout","text":"Metrics for Testset:\nROUGE-L recall:    45.37%\nBLEU-1 precision:  44.64%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train set","metadata":{}},{"cell_type":"code","source":"actual = []\nfor i in range(0,len(image_train)-5,5):\n    temp = []\n    for j in range(i,i+5):\n        real_caption = [tokenizer.index_word[k]\n                        for k in captions_train[j] if k not in [0]]\n        temp.append(real_caption)\n    actual.append(temp)\n    \npred = []\nfor i in tqdm(range(0,len(image_train)-5,5)):\n    image = image_train[i]\n    result = make_prediction(image)\n    result.insert(0,'<start>')\n    pred.append(result)\n    \nreal_dict, pred_dict = {}, {}\nfor i, (reals, preds) in enumerate(zip(actual, pred)):    \n    real_dict[i] = [' '.join(x) for x in reals]\n    pred_dict[i] = ' '.join(preds) ","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 19%|        | 1229/6471 [01:45<07:49, 11.17it/s]","output_type":"stream"}]},{"cell_type":"code","source":"rouge_l_train, bleu_1_train = met.evaluate(real_dict, pred_dict)\nprint(f\"Metrics for Trainset:\\nROUGE-L recall:{rouge_l_test:>10.2%}\\nBLEU-1 precision: {bleu_1_test:>7.2%}\")","metadata":{},"execution_count":null,"outputs":[]}]}